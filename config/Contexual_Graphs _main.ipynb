{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3240dd4-4bab-424d-93f2-ef1b22ddb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start########################################################################################################################\n",
    "##############################################################################################################################\n",
    "################COCO dataset images into the camera views of nuscenes #######################################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3d7792fe-bc25-449a-94a0-bf312d96a9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations for val2017…\n",
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "val2017: extracting 22 categories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val2017: categories: 100%|█████████████████████████████████████████████████████| 22/22 [07:54<00:00, 21.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations for train2017…\n",
      "loading annotations into memory...\n",
      "Done (t=9.06s)\n",
      "creating index...\n",
      "index created!\n",
      "train2017: extracting 22 categories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train2017: categories: 100%|████████████████████████████████████████████████| 22/22 [1:18:43<00:00, 214.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved/updated manifest → /data/Asad/ASSETS/manifest.csv\n",
      "\n",
      "Per-category totals (including pre-existing files):\n",
      "bear                 4\n",
      "bed                  4\n",
      "bench                4\n",
      "bird                 4\n",
      "boat                 4\n",
      "cat                  4\n",
      "chair                4\n",
      "cow                  4\n",
      "dog                  4\n",
      "elephant             4\n",
      "frisbee              4\n",
      "giraffe              4\n",
      "horse                4\n",
      "kite                 4\n",
      "sheep                4\n",
      "skateboard           4\n",
      "skis                 1\n",
      "snowboard            4\n",
      "sports_ball          4\n",
      "surfboard            4\n",
      "teddy_bear           4\n",
      "zebra                4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, io, json, zipfile, shutil, time, math, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# ========= OOD categories (non-road) =========\n",
    "CATEGORIES_FILTER = [\n",
    "    # Animals not typical in city streets\n",
    "    \"zebra\", \"giraffe\", \"elephant\", \"bear\", \"sheep\", \"cow\", \"horse\", \"bird\",\n",
    "    # Sports / leisure\n",
    "    \"surfboard\", \"frisbee\", \"snowboard\", \"kite\", \"skateboard\",\n",
    "    # Household / indoor furniture & appliances\n",
    "    \"bed\",\n",
    "    \"teddy bear\"\n",
    "]\n",
    "\n",
    "# Added realistic on-road OODs (deduped + order-preserving)\n",
    "MORE_ROAD_OOD = [\n",
    "    # Animals (very plausible)\n",
    "    \"dog\", \"cat\",\n",
    "\n",
    "\n",
    "    # Sports / leisure (extras)\n",
    "    \"sports ball\", \"skis\",\n",
    "\n",
    "    # Furniture / clutter often seen curbside\n",
    "    \"chair\", \"bench\", \"potted plant\",\n",
    "\n",
    "    # Large non-road vehicles/objects\n",
    "    \"boat\",\n",
    "]\n",
    "\n",
    "# Merge with deduplication, preserving the original order where possible\n",
    "CATEGORIES_FILTER = list(dict.fromkeys(CATEGORIES_FILTER + MORE_ROAD_OOD))\n",
    "\n",
    "\n",
    "# ========= Limits / filters =========\n",
    "MAX_PER_CATEGORY        = 4     # hard cap per category (set 3–4 as you like)\n",
    "MIN_MASK_AREA           = 1000  # pixels; skip tiny bits\n",
    "FULL_BODY_MIN_H_RATIO   = 0.40  # bbox_h / image_h >= this → keep\n",
    "FULL_BODY_MIN_W_RATIO   = 0.15  # bbox_w / image_w >= this → keep\n",
    "SPLITS = [\"val2017\", \"train2017\"]\n",
    "\n",
    "# ========= Behavior =========\n",
    "USE_LOCAL_COCO = False\n",
    "LOCAL_INST_JSON = {  # only used if USE_LOCAL_COCO=True\n",
    "    \"train2017\": \"/path/to/annotations/instances_train2017.json\",\n",
    "    \"val2017\":   \"/path/to/annotations/instances_val2017.json\",\n",
    "}\n",
    "\n",
    "IMG_CACHE_DIR   = Path(\"/tmp/coco_img_cache\")  # image download cache\n",
    "REQUEST_TIMEOUT = 15\n",
    "RETRY_LIMIT     = 3\n",
    "RANDOM_SEED     = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "ANN_ZIP_URL = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "\n",
    "IMG_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9bb4a5-bd60-4231-934e-570594f48012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End########################################################################################################################\n",
    "##############################################################################################################################\n",
    "################COCO dataset images into the 6 camera views of nuscenes #######################################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "23b44003-e481-4284-bdfe-c3c2c5ea797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Copying nuScenes-mini tree …\n",
      "  Copied to /data/Asad/NuScenesMiniNovel\n",
      " 85 assets found in /data/Asad/ASSETS\n",
      "Indexing scenes across 6 cameras …\n",
      "Pasting novel objects across multiple cameras …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes: 100%|██████████████████████████████████████████████████████████████████| 10/10 [00:38<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✔ Finished OOD injection: 1367 novel objects added.\n",
      "  ✔ Detection JSON → /data/Asad/NuScenesMiniNovel/v1.0-mini/detection_novel.json\n",
      " Projecting GT 3D boxes to 2D for all 6 cameras …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ID boxes: 100%|█████████████████████████████████████████████████████████| 31206/31206 [01:44<00:00, 299.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ID Detection JSON → /data/Asad/NuScenesMiniNovel/v1.0-mini/detection_id.json\n",
      "\n",
      "  Clone ready at: /data/Asad/NuScenesMiniNovel\n",
      "    - OOD boxes:  /data/Asad/NuScenesMiniNovel/v1.0-mini/detection_novel.json\n",
      "    - ID boxes :  /data/Asad/NuScenesMiniNovel/v1.0-mini/detection_id.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# nuScenes-mini → cloned novel set with OOD objects in all 6 cams\n",
    "# Saves detection_novel.json (OOD) and detection_id.json (ID) in DST\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "import random, uuid, json, shutil, os\n",
    "import numpy as np\n",
    "import cv2, albumentations as A\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import view_points, BoxVisibility\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "# ╭──CONFIG ─────────────────────────────────────────────╮\n",
    "SRC        = Path(\"/data/Asad/NuScenesMini\")        # original dataset root\n",
    "DST        = Path(\"/data/Asad/NuScenesMiniNovel\")   # clone will be written here\n",
    "ASSETS     = Path(\"/data/Asad/ASSETS\")              # flat folder of RGBA PNGs\n",
    "\n",
    "N_PASTES_PER_SCENE = 50\n",
    "NOVEL_RATE = 0.5\n",
    "MIN_CAMS_PER_FRAME = 3\n",
    "MAX_CAMS_PER_FRAME = 6\n",
    "\n",
    "SEED = 42\n",
    "# ╰───────────────────────────────────────────────────────────╯\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "assert SRC.exists(),                 f\"SRC not found: {SRC}\"\n",
    "assert ASSETS.exists() and any(ASSETS.glob(\"*.png\")), \"ASSETS folder empty\"\n",
    "if DST.exists():\n",
    "    raise FileExistsError(f\"{DST} already exists — remove it or choose a new path\")\n",
    "\n",
    "# copy whole dataset -------------------------------------------------\n",
    "print(\" Copying nuScenes-mini tree …\")\n",
    "shutil.copytree(SRC, DST)\n",
    "print(\"  Copied to\", DST)\n",
    "\n",
    "# assets + augment pipeline -----------------------------------------\n",
    "asset_paths = sorted(ASSETS.glob(\"*.png\"))\n",
    "print(f\" {len(asset_paths)} assets found in {ASSETS}\")\n",
    "\n",
    "augment = A.Compose(\n",
    "    [\n",
    "        A.RandomScale(scale_limit=(0.3, 0.6), p=1.0),\n",
    "        A.Rotate(limit=8, border_mode=cv2.BORDER_CONSTANT, p=0.6),\n",
    "        A.RandomBrightnessContrast(p=0.45),\n",
    "        A.HorizontalFlip(p=0.25),\n",
    "    ],\n",
    "    additional_targets={\"mask\": \"mask\"},\n",
    ")\n",
    "\n",
    "def paste_object(img_bgr: np.ndarray, obj_rgba: np.ndarray):\n",
    "\n",
    "    h, w = img_bgr.shape[:2]\n",
    "\n",
    "    # ensure 4-channel RGBA\n",
    "    if obj_rgba.ndim == 2:\n",
    "        obj_rgba = np.dstack([\n",
    "            obj_rgba, obj_rgba, obj_rgba,\n",
    "            255 * np.ones_like(obj_rgba)\n",
    "        ])\n",
    "    if obj_rgba.shape[2] == 3:\n",
    "        alpha = 255 * np.ones(obj_rgba.shape[:2], obj_rgba.dtype)\n",
    "        obj_rgba = np.dstack([obj_rgba, alpha])\n",
    "    if obj_rgba.shape[2] != 4:\n",
    "        return img_bgr, None\n",
    "\n",
    "    # Split color + alpha\n",
    "    obj_rgb  = obj_rgba[:, :, :3]   # RGB from PIL\n",
    "    alpha    = obj_rgba[:, :, 3]\n",
    "\n",
    "    aug = augment(image=obj_rgb, mask=alpha)\n",
    "    obj_rgb_aug, alpha_mask = aug[\"image\"], aug[\"mask\"]\n",
    "\n",
    "\n",
    "    obj_bgr = cv2.cvtColor(obj_rgb_aug, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    oh, ow = obj_bgr.shape[:2]\n",
    "\n",
    "    # keep inside lower half, resize if needed\n",
    "    max_w, max_h = w - 10, int(h * 0.5) - 10\n",
    "    if ow >= max_w or oh >= max_h:\n",
    "        scale = 0.9 * min(max_w / max(ow, 1), max_h / max(oh, 1))\n",
    "        if scale <= 0:\n",
    "            return img_bgr, None\n",
    "        obj_bgr    = cv2.resize(obj_bgr,    None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        alpha_mask = cv2.resize(alpha_mask, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        oh, ow     = obj_bgr.shape[:2]\n",
    "\n",
    "    # place somewhere in lower half\n",
    "    x0 = random.randint(0, max(0, w - ow))\n",
    "    y0 = random.randint(int(h * 0.5), max(int(h * 0.5), h - oh))\n",
    "\n",
    "    roi     = img_bgr[y0:y0 + oh, x0:x0 + ow]\n",
    "    alpha_f = (alpha_mask[:, :, None] / 255.0).astype(np.float32)\n",
    "\n",
    "    img_bgr[y0:y0 + oh, x0:x0 + ow] = (1 - alpha_f) * roi + alpha_f * obj_bgr\n",
    "\n",
    "    return img_bgr, (x0, y0, x0 + ow, y0 + oh)\n",
    "\n",
    "\n",
    "#  build scene → frames → {channel: (path, sd_token)} -----------------\n",
    "print(\"Indexing scenes across 6 cameras …\")\n",
    "ALL_CAM_CHANNELS = [\n",
    "    \"CAM_FRONT\",\n",
    "    \"CAM_FRONT_LEFT\",\n",
    "    \"CAM_FRONT_RIGHT\",\n",
    "    \"CAM_BACK\",\n",
    "    \"CAM_BACK_LEFT\",\n",
    "    \"CAM_BACK_RIGHT\",\n",
    "]\n",
    "\n",
    "nusc = NuScenes(version=\"v1.0-mini\", dataroot=str(DST), verbose=False)\n",
    "\n",
    "# scene_to_frames: {scene_token: {sample_token: {channel: (img_path, sd_token)}}}\n",
    "scene_to_frames = {}\n",
    "for sd in nusc.sample_data:\n",
    "    ch = sd[\"channel\"]\n",
    "    if ch not in ALL_CAM_CHANNELS:\n",
    "        continue\n",
    "    sample_token = sd[\"sample_token\"]\n",
    "    scene_token  = nusc.get(\"sample\", sample_token)[\"scene_token\"]\n",
    "    img_path     = DST / sd[\"filename\"]\n",
    "    scene_to_frames.setdefault(scene_token, {}).setdefault(sample_token, {})[ch] = (img_path, sd[\"token\"])\n",
    "\n",
    "#  multi-camera main loop ---------------------------------------------\n",
    "print(\"Pasting novel objects across multiple cameras …\")\n",
    "new_boxes = []  # entries for detection_novel.json\n",
    "\n",
    "scenes = list(scene_to_frames.items())\n",
    "random.shuffle(scenes)\n",
    "\n",
    "for scene_token, frames in tqdm(scenes, desc=\"Scenes\"):\n",
    "    sample_tokens = sorted(frames.keys())\n",
    "\n",
    "    # --- 5a: guarantee some pastes across this scene ---\n",
    "    guaranteed_remaining = N_PASTES_PER_SCENE\n",
    "\n",
    "    for sample_token in sample_tokens:\n",
    "        if guaranteed_remaining <= 0:\n",
    "            break\n",
    "\n",
    "        cams = list(frames[sample_token].keys())\n",
    "        random.shuffle(cams)\n",
    "        k = random.randint(MIN_CAMS_PER_FRAME, min(MAX_CAMS_PER_FRAME, len(cams)))\n",
    "        k = min(k, guaranteed_remaining)\n",
    "\n",
    "        for ch in cams[:k]:\n",
    "            img_path, sd_token = frames[sample_token][ch]\n",
    "            try:\n",
    "                img_rgb = np.array(Image.open(img_path))\n",
    "            except Exception:\n",
    "                print(\"Can't open\", img_path)\n",
    "                continue\n",
    "\n",
    "            img_bgr  = img_rgb[..., ::-1]\n",
    "            obj_path = random.choice(asset_paths)\n",
    "            try:\n",
    "                obj_rgba = np.array(Image.open(obj_path).convert(\"RGBA\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            img_bgr, bbox = paste_object(img_bgr, obj_rgba)\n",
    "            if bbox:\n",
    "                x0, y0, x1, y1 = map(int, bbox)\n",
    "                new_boxes.append(\n",
    "                    {\n",
    "                        \"sample_data_token\": sd_token,\n",
    "                        \"translation\": [0, 0, 0],\n",
    "                        \"size\": [0, 0, 0],\n",
    "                        \"rotation\": [0, 0, 0, 1],\n",
    "                        \"velocity\": None,\n",
    "                        \"detection_name\": f\"novel.{obj_path.stem}\",\n",
    "                        \"detection_score\": 1.0,\n",
    "                        \"attribute_name\": \"\",\n",
    "                        \"bbox_2d\": [x0, y0, x1, y1],\n",
    "                        \"token\": str(uuid.uuid4()),\n",
    "                    }\n",
    "                )\n",
    "                Image.fromarray(img_bgr[..., ::-1]).save(img_path, quality=95)\n",
    "                guaranteed_remaining -= 1\n",
    "                if guaranteed_remaining <= 0:\n",
    "                    break\n",
    "\n",
    "    for sample_token in sample_tokens:\n",
    "        if random.random() >= NOVEL_RATE:\n",
    "            continue\n",
    "        cams = list(frames[sample_token].keys())\n",
    "        random.shuffle(cams)\n",
    "        k = random.randint(MIN_CAMS_PER_FRAME, min(MAX_CAMS_PER_FRAME, len(cams)))\n",
    "        for ch in cams[:k]:\n",
    "            img_path, sd_token = frames[sample_token][ch]\n",
    "            try:\n",
    "                img_rgb = np.array(Image.open(img_path))\n",
    "            except Exception:\n",
    "                continue\n",
    "            img_bgr  = img_rgb[..., ::-1]\n",
    "            obj_path = random.choice(asset_paths)\n",
    "            try:\n",
    "                obj_rgba = np.array(Image.open(obj_path).convert(\"RGBA\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "            img_bgr, bbox = paste_object(img_bgr, obj_rgba)\n",
    "            if bbox:\n",
    "                x0, y0, x1, y1 = map(int, bbox)\n",
    "                new_boxes.append(\n",
    "                    {\n",
    "                        \"sample_data_token\": sd_token,\n",
    "                        \"translation\": [0, 0, 0],\n",
    "                        \"size\": [0, 0, 0],\n",
    "                        \"rotation\": [0, 0, 0, 1],\n",
    "                        \"velocity\": None,\n",
    "                        \"detection_name\": f\"novel.{obj_path.stem}\",\n",
    "                        \"detection_score\": 1.0,\n",
    "                        \"attribute_name\": \"\",\n",
    "                        \"bbox_2d\": [x0, y0, x1, y1],\n",
    "                        \"token\": str(uuid.uuid4()),\n",
    "                    }\n",
    "                )\n",
    "                Image.fromarray(img_bgr[..., ::-1]).save(img_path, quality=95)\n",
    "\n",
    "print(f\"  ✔ Finished OOD injection: {len(new_boxes)} novel objects added.\")\n",
    "\n",
    "det_src = SRC / \"v1.0-mini\" / \"detection.json\"\n",
    "if det_src.exists():\n",
    "    det_data = json.loads(det_src.read_text())\n",
    "else:\n",
    "    det_data = {\"results\": {}, \"meta\": {\"version\": \"v1.0-mini\"}}\n",
    "\n",
    "for rec in new_boxes:\n",
    "    det_data[\"results\"].setdefault(rec[\"sample_data_token\"], []).append(rec)\n",
    "\n",
    "out_novel = DST / \"v1.0-mini\" / \"detection_novel.json\"\n",
    "out_novel.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_novel.write_text(json.dumps(det_data))\n",
    "print(\"  ✔ Detection JSON →\", out_novel)\n",
    "\n",
    "print(\" Projecting GT 3D boxes to 2D for all 6 cameras …\")\n",
    "nusc_src = NuScenes(version=\"v1.0-mini\", dataroot=str(SRC), verbose=False)\n",
    "id_results = {}\n",
    "\n",
    "for sd in tqdm(nusc_src.sample_data, desc=\"ID boxes\"):\n",
    "    if sd[\"channel\"] not in ALL_CAM_CHANNELS:\n",
    "        continue\n",
    "\n",
    "    sd_token = sd[\"token\"]\n",
    "    cs = nusc_src.get(\"calibrated_sensor\", sd[\"calibrated_sensor_token\"])\n",
    "    cam_intrinsic = np.array(cs[\"camera_intrinsic\"])\n",
    "    _, boxes, _ = nusc_src.get_sample_data(sd_token, box_vis_level=BoxVisibility.ANY)\n",
    "\n",
    "    entries = []\n",
    "    for box in boxes:\n",
    "        # 3D corners in camera frame → project to 2D\n",
    "        corners_3d = box.corners()  # (3,8)\n",
    "        pts_2d = view_points(corners_3d, cam_intrinsic, normalize=True)  # (3,8)\n",
    "        xs, ys = pts_2d[0], pts_2d[1]\n",
    "        x0, y0 = float(xs.min()), float(ys.min())\n",
    "        x1, y1 = float(xs.max()), float(ys.max())\n",
    "\n",
    "        entries.append({\n",
    "            \"sample_data_token\": sd_token,\n",
    "            \"translation\": list(box.center),                 # [x, y, z]\n",
    "            \"size\":       list(box.wlh[::-1]),               # [l, w, h] (kept as before)\n",
    "            \"rotation\":   list(box.orientation.elements),    # [x,y,z,w]\n",
    "            \"velocity\":   [box.velocity[0], box.velocity[1], 0.0] if box.velocity is not None else None,\n",
    "            \"detection_name\": box.name,                      # e.g., \"car\"\n",
    "            \"detection_score\": 1.0,\n",
    "            \"attribute_name\": \"\",\n",
    "            \"bbox_2d\":    [x0, y0, x1, y1],\n",
    "            \"token\":      str(uuid.uuid4()),\n",
    "        })\n",
    "\n",
    "    if entries:\n",
    "        id_results[sd_token] = entries\n",
    "\n",
    "out_id = DST / \"v1.0-mini\" / \"detection_id.json\"\n",
    "out_id.write_text(json.dumps({\"results\": id_results, \"meta\": {\"version\": \"v1.0-mini\"}}, indent=2))\n",
    "print(\"  ID Detection JSON →\", out_id)\n",
    "\n",
    "print(\"\\n  Clone ready at:\", DST)\n",
    "print(\"    - OOD boxes: \", out_novel)\n",
    "print(\"    - ID boxes : \", out_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb858c7-9d9f-4fe7-b426-6a4c4dc6c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files: 0\n",
      "Mixed sample tokens in a single frame: 0\n",
      "Mixed scene tokens in a single frame: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "DST = Path(\"/data/Asad/NuScenesMiniNovel\")\n",
    "JSONDIR = DST / \"v1.0-mini\"\n",
    "ALL_CAM_CHANNELS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "nusc = NuScenes(version=\"v1.0-mini\", dataroot=str(DST), verbose=False)\n",
    "\n",
    "bad = []\n",
    "missing_files = []\n",
    "mixed_scenes = []\n",
    "\n",
    "for scene in nusc.scene:\n",
    "    scene_tok = scene[\"token\"]\n",
    "    st = scene[\"first_sample_token\"]\n",
    "    while st:\n",
    "        sample = nusc.get('sample', st)\n",
    "        ch2sd = sample[\"data\"]  # channel -> sample_data_token\n",
    "\n",
    "        # Require all 6 cams present\n",
    "        if not all(ch in ch2sd for ch in ALL_CAM_CHANNELS):\n",
    "            st = sample[\"next\"]\n",
    "            continue\n",
    "\n",
    "        # Check each sd belongs to same sample & scene\n",
    "        scene_toks = set()\n",
    "        sample_toks = set()\n",
    "        files = []\n",
    "\n",
    "        for ch in ALL_CAM_CHANNELS:\n",
    "            sd_tok = ch2sd[ch]\n",
    "            sd = nusc.get('sample_data', sd_tok)\n",
    "            files.append((ch, sd[\"filename\"]))\n",
    "            sample_toks.add(sd[\"sample_token\"])\n",
    "            # back out the scene via sample token\n",
    "            s_tmp = nusc.get('sample', sd[\"sample_token\"])\n",
    "            scene_toks.add(s_tmp[\"scene_token\"])\n",
    "\n",
    "            # file exists?\n",
    "            fpath = DST / sd[\"filename\"]\n",
    "            if not fpath.exists():\n",
    "                missing_files.append((sd_tok, f\"{fpath}\"))\n",
    "\n",
    "        if len(sample_toks) != 1:\n",
    "            bad.append((\"mixed_sample_tokens\", scene[\"name\"], st, list(sample_toks), files))\n",
    "        if len(scene_toks) != 1:\n",
    "            mixed_scenes.append((\"mixed_scene_tokens\", scene[\"name\"], st, list(scene_toks), files))\n",
    "\n",
    "        st = sample[\"next\"]\n",
    "\n",
    "print(f\"Missing files: {len(missing_files)}\")\n",
    "print(f\"Mixed sample tokens in a single frame: {len(bad)}\")\n",
    "print(f\"Mixed scene tokens in a single frame: {len(mixed_scenes)}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\nExample missing file:\", missing_files[0])\n",
    "if bad:\n",
    "    print(\"\\nExample mixed-sample frame:\", bad[0])\n",
    "if mixed_scenes:\n",
    "    print(\"\\nExample mixed-scene frame:\", mixed_scenes[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b18a3c3-3c41-4f9f-b4b0-1b37d963c4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  10%|██████                                                       | 1/10 [00:06<00:55,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-0061.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  20%|████████████▏                                                | 2/10 [00:12<00:48,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-0103.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  30%|██████████████████▎                                          | 3/10 [00:17<00:40,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-0553.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  40%|████████████████████████▍                                    | 4/10 [00:23<00:35,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-0655.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  50%|██████████████████████████████▌                              | 5/10 [00:29<00:29,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-0757.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  60%|████████████████████████████████████▌                        | 6/10 [00:36<00:24,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-0796.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  70%|██████████████████████████████████████████▋                  | 7/10 [00:42<00:18,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-0916.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  80%|████████████████████████████████████████████████▊            | 8/10 [00:50<00:13,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-1077.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video:  90%|██████████████████████████████████████████████████████▉      | 9/10 [00:57<00:06,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-1094.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenes→Video: 100%|████████████████████████████████████████████████████████████| 10/10 [01:03<00:00,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote /data/Asad/NuScenesMiniNovel/videos_sync/scene-1100.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "DST = Path(\"/data/Asad/NuScenesMiniNovel\")\n",
    "JSONDIR = DST / \"v1.0-mini\"\n",
    "OUTDIR = DST / \"videos_sync\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FPS = 6\n",
    "GRID = (3,2)\n",
    "ALL_CAM_CHANNELS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "iden  = json.loads((JSONDIR / \"detection_id.json\").read_text())[\"results\"]\n",
    "novel = json.loads((JSONDIR / \"detection_novel.json\").read_text())[\"results\"]\n",
    "\n",
    "nusc = NuScenes(version=\"v1.0-mini\", dataroot=str(DST), verbose=False)\n",
    "\n",
    "def draw_overlay_with_label(img_rgb, boxes_id, boxes_ood, label_text):\n",
    "    img = Image.fromarray(img_rgb.copy())\n",
    "    drw = ImageDraw.Draw(img)\n",
    "    bb = drw.textbbox((6,6), label_text)\n",
    "    drw.rectangle([bb[0]+2, bb[1]+2, bb[2]+10, bb[3]+10], fill=(0,0,0))\n",
    "    drw.text((8,8), label_text, fill=(255,255,255))\n",
    "    # ID solid\n",
    "    for b in boxes_id:\n",
    "        x0,y0,x1,y1 = b[\"bbox_2d\"]\n",
    "        drw.rectangle([x0,y0,x1,y1], outline=(0,255,0), width=3)\n",
    "    # OOD dashed\n",
    "    dash = 12\n",
    "    for b in boxes_ood:\n",
    "        x0,y0,x1,y1 = b[\"bbox_2d\"]\n",
    "        for x in range(int(x0), int(x1), dash*2):\n",
    "            drw.line([(x,y0),(min(x+dash,x1),y0)], fill=(255,0,0), width=3)\n",
    "            drw.line([(x,y1),(min(x+dash,x1),y1)], fill=(255,0,0), width=3)\n",
    "        for y in range(int(y0), int(y1), dash*2):\n",
    "            drw.line([(x0,y),(x0,min(y+dash,y1))], fill=(255,0,0), width=3)\n",
    "            drw.line([(x1,y),(x1,min(y+dash,y1))], fill=(255,0,0), width=3)\n",
    "    return np.asarray(img)\n",
    "\n",
    "def tile_grid(imgs, grid=(3,2), pad=6, bg=0):\n",
    "    rows, cols = grid\n",
    "    assert len(imgs) == rows*cols\n",
    "    target_h = min(im.shape[0] for im in imgs)\n",
    "    resized = [cv2.resize(im, (int(im.shape[1]*target_h/im.shape[0]), target_h), interpolation=cv2.INTER_AREA) for im in imgs]\n",
    "    row_imgs = []\n",
    "    for r in range(rows):\n",
    "        row = resized[r*cols:(r+1)*cols]\n",
    "        maxw = max(im.shape[1] for im in row)\n",
    "        row = [np.pad(im, ((0,0),(0,maxw-im.shape[1]),(0,0)), constant_values=bg) for im in row]\n",
    "        row_imgs.append(np.concatenate(row, axis=1))\n",
    "    maxw = max(im.shape[1] for im in row_imgs)\n",
    "    row_imgs = [np.pad(im, ((0,0),(0,maxw-im.shape[1]),(0,0)), constant_values=bg) for im in row_imgs]\n",
    "    pad_arr = np.full((pad, maxw, 3), bg, dtype=np.uint8)\n",
    "    out = row_imgs[0]\n",
    "    for r in row_imgs[1:]:\n",
    "        out = np.concatenate([out, pad_arr, r], axis=0)\n",
    "    return out\n",
    "\n",
    "for scene in tqdm(nusc.scene, desc=\"Scenes→Video\"):\n",
    "    name = scene[\"name\"].replace(\"/\", \"_\")\n",
    "    out_path = str((OUTDIR / f\"{name}.mp4\").resolve())\n",
    "    writer = None\n",
    "\n",
    "    sample_tok = scene[\"first_sample_token\"]\n",
    "    while sample_tok:\n",
    "        sample = nusc.get('sample', sample_tok)\n",
    "        ch2sd = sample[\"data\"]\n",
    "\n",
    "        # must have all 6 cameras\n",
    "        if not all(ch in ch2sd for ch in ALL_CAM_CHANNELS):\n",
    "            sample_tok = sample[\"next\"]\n",
    "            continue\n",
    "\n",
    "        # Safety assertions: same sample & scene\n",
    "        sample_tokens = set()\n",
    "        scene_tokens = set()\n",
    "        frame_imgs = []\n",
    "\n",
    "        for ch in ALL_CAM_CHANNELS:\n",
    "            sd_tok = ch2sd[ch]\n",
    "            sd = nusc.get('sample_data', sd_tok)\n",
    "            sample_tokens.add(sd[\"sample_token\"])\n",
    "            scene_tokens.add(nusc.get('sample', sd[\"sample_token\"])[\"scene_token\"])\n",
    "\n",
    "            f = DST / sd[\"filename\"]\n",
    "            if not f.exists():\n",
    "                frame_imgs = []\n",
    "                break\n",
    "\n",
    "            img = np.array(Image.open(f).convert(\"RGB\"))\n",
    "            img = draw_overlay_with_label(img, iden.get(sd_tok, []), novel.get(sd_tok, []), ch)\n",
    "            frame_imgs.append(img)\n",
    "\n",
    "        if len(frame_imgs) != 6:\n",
    "            sample_tok = sample[\"next\"]\n",
    "            continue\n",
    "\n",
    "        assert len(sample_tokens) == 1, f\"Mixed sample tokens in one frame: {sample_tokens}\"\n",
    "        assert len(scene_tokens) == 1, f\"Mixed scene tokens in one frame: {scene_tokens}\"\n",
    "\n",
    "        grid_img = tile_grid(frame_imgs, grid=GRID, pad=6, bg=0)\n",
    "\n",
    "        if writer is None:\n",
    "            h,w = grid_img.shape[:2]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "            writer = cv2.VideoWriter(out_path, fourcc, FPS, (w,h))\n",
    "\n",
    "        writer.write(cv2.cvtColor(grid_img, cv2.COLOR_RGB2BGR))\n",
    "        sample_tok = sample[\"next\"]\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "        print(f\"  Wrote {out_path}\")\n",
    "    else:\n",
    "        print(f\"  (No frames for scene {name})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5971728c-2139-4180-a9c6-4bcc993ea5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "################Detection approach  #########################################################################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98479449-d605-4d64-9bdd-7b6b1b24cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.13.0+cu117 CUDA: True\n",
      "Device: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch, os, platform\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True  # speed up conv backends\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # reduces OOM fragmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe672cd4-c04d-4ea7-8fdc-ff3b348875d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install timm==0.9.12 scikit-learn==1.3.2\n",
    "# PyG wheels compatible with torch 1.13.* + cu117:\n",
    "!pip -q install torch-geometric==2.3.1 \\\n",
    "  torch-scatter==2.1.1 torch-sparse==0.6.17 torch-cluster==1.6.1 torch-spline-conv==1.2.2 \\\n",
    "  -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "807578b6-0c9d-453f-bdc7-5e548cf444fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.13.0+cu117 CUDA: True\n",
      "Device: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import os, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATAROOT = Path(\"/data/Asad/NuScenesMiniNovel\")\n",
    "JSONDIR  = DATAROOT / \"v1.0-mini\"\n",
    "ALL_CAMS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "# Load nuScenes tables used\n",
    "sd_rows   = {d[\"token\"]: d for d in json.loads((JSONDIR / \"sample_data.json\").read_text())}\n",
    "samples   = {s[\"token\"]: s for s in json.loads((JSONDIR / \"sample.json\").read_text())}\n",
    "scenes    = json.loads((JSONDIR / \"scene.json\").read_text())\n",
    "sensor_by = {s[\"token\"]: s for s in json.loads((JSONDIR / \"sensor.json\").read_text())}\n",
    "calib_by  = {c[\"token\"]: c for c in json.loads((JSONDIR / \"calibrated_sensor.json\").read_text())}\n",
    "\n",
    "# Your GT (ID = in-dist, OOD = novel)\n",
    "gt_id    = json.loads((JSONDIR / \"detection_id.json\").read_text())[\"results\"]\n",
    "gt_ood   = json.loads((JSONDIR / \"detection_novel.json\").read_text())[\"results\"]\n",
    "\n",
    "# Utility: channel for a sample_data row\n",
    "def channel_of_sd_row(sd_row):\n",
    "    calib = calib_by[sd_row[\"calibrated_sensor_token\"]]\n",
    "    sensor = sensor_by[calib[\"sensor_token\"]]\n",
    "    return sensor[\"channel\"]\n",
    "\n",
    "sample_to_ch2sd = {}\n",
    "for sd in sd_rows.values():\n",
    "    ch = channel_of_sd_row(sd)\n",
    "    if not ch.startswith(\"CAM_\"):\n",
    "        continue\n",
    "    st = sd[\"sample_token\"]\n",
    "    sample_to_ch2sd.setdefault(st, {})[ch] = sd[\"token\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b5efc7-0a88-470a-9fd3-af96dc173a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Cache\n",
    "CACHE_DIR = DATAROOT / \".cache\" / \"emb_v1\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BATCH_SIZE = 64  # tune for your GPU\n",
    "\n",
    "try:\n",
    "    backbone = timm.create_model('dinov2_small', pretrained=True, num_classes=0).to(device).eval()\n",
    "except Exception:\n",
    "    backbone = timm.create_model('vit_base_patch16_224.dino', pretrained=True, num_classes=0).to(device).eval()\n",
    "\n",
    "FEAT_DIM = backbone.num_features\n",
    "preproc = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
    "])\n",
    "\n",
    "def _sanitize_box(box, W, H, min_size=2):\n",
    "    # clamp + fix ordering + remove NaNs; returns integer box or None\n",
    "    x0, y0, x1, y1 = [float(v) for v in box]\n",
    "    if x1 < x0: x0, x1 = x1, x0\n",
    "    if y1 < y0: y0, y1 = y1, y0\n",
    "    if not np.all(np.isfinite([x0,y0,x1,y1])): return None\n",
    "    x0 = max(0.0, min(x0, W - 1)); y0 = max(0.0, min(y0, H - 1))\n",
    "    x1 = max(x0 + 1.0, min(x1, W)); y1 = max(y0 + 1.0, min(y1, H))\n",
    "    w, h = (x1-x0), (y1-y0)\n",
    "    if w < min_size or h < min_size: return None\n",
    "    # avoid near-full-image crops that can trigger PIL bomb checks\n",
    "    if (w*h) > 0.98*(W*H):\n",
    "        pad = 1.0\n",
    "        x0 = max(0.0, x0 + pad); y0 = max(0.0, y0 + pad)\n",
    "        x1 = min(W,   x1 - pad); y1 = min(H,   y1 - pad)\n",
    "    return (int(round(x0)), int(round(y0)), int(round(x1)), int(round(y1)))\n",
    "\n",
    "def _box_key(sd_token, box_tuple):\n",
    "    s = f\"{sd_token}|{box_tuple[0]},{box_tuple[1]},{box_tuple[2]},{box_tuple[3]}|{backbone.default_cfg.get('architecture','dino')}|224\"\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "def _cache_path(key): \n",
    "    return CACHE_DIR / f\"{key}.npy\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_crops_batched(pil_img: Image.Image, boxes, sd_token):\n",
    "    \"\"\"Return (N, FEAT_DIM) embeddings; invalid boxes -> zero vectors. Uses GPU batching + cache.\"\"\"\n",
    "    W, H = pil_img.size\n",
    "    embs = [None] * len(boxes)\n",
    "    to_run, run_meta = [], []  # (tensor list), list[(i, cache_key)]\n",
    "\n",
    "    # prepare\n",
    "    for i, box in enumerate(boxes):\n",
    "        sb = _sanitize_box(box, W, H)\n",
    "        if sb is None:\n",
    "            embs[i] = np.zeros((FEAT_DIM,), dtype=np.float32)\n",
    "            continue\n",
    "        key = _box_key(sd_token, sb)\n",
    "        cp = _cache_path(key)\n",
    "        if cp.exists():\n",
    "            embs[i] = np.load(cp)\n",
    "        else:\n",
    "            crop = pil_img.crop(sb)\n",
    "            to_run.append(preproc(crop))\n",
    "            run_meta.append((i, key))\n",
    "\n",
    "    # run in batches\n",
    "    for start in range(0, len(to_run), BATCH_SIZE):\n",
    "        batch = torch.stack(to_run[start:start+BATCH_SIZE], dim=0).to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            Z = backbone(batch)  # (B, D)\n",
    "        Z = Z.float().cpu().numpy()\n",
    "        for (i, key), z in zip(run_meta[start:start+BATCH_SIZE], Z):\n",
    "            np.save(_cache_path(key), z)\n",
    "            embs[i] = z\n",
    "\n",
    "    # fill any remaining\n",
    "    for i in range(len(embs)):\n",
    "        if embs[i] is None:\n",
    "            embs[i] = np.zeros((FEAT_DIM,), dtype=np.float32)\n",
    "\n",
    "    return np.stack(embs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1210a2af-5432-4124-9dc0-7cd7b8d4ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs built: 2171 train | 2274 val\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "def build_graphs(split=\"train\", knn_k=6, include_id_from_ood_frames=True):\n",
    "\n",
    "    graphs = []\n",
    "    for sc in scenes:\n",
    "        s_tok = sc[\"first_sample_token\"]\n",
    "        while s_tok:\n",
    "            sample = samples[s_tok]                 # <-- capture once\n",
    "            ch2sd  = sample_to_ch2sd.get(s_tok, {}) # sync cams for this timestamp\n",
    "\n",
    "            for ch in ALL_CAMS:\n",
    "                sd_tok = ch2sd.get(ch)\n",
    "                if not sd_tok:\n",
    "                    continue\n",
    "\n",
    "                fn = sd_rows[sd_tok][\"filename\"]\n",
    "                img_path = DATAROOT / fn\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                W, H = img.width, img.height\n",
    "\n",
    "                # gather boxes + labels\n",
    "                id_boxes   = [b[\"bbox_2d\"] for b in gt_id.get(sd_tok, [])]\n",
    "                ood_boxes  = [b[\"bbox_2d\"] for b in gt_ood.get(sd_tok, [])]\n",
    "                has_ood    = len(ood_boxes) > 0\n",
    "\n",
    "                boxes, labels = [], []\n",
    "                if include_id_from_ood_frames or not has_ood:\n",
    "                    boxes += id_boxes; labels += [0]*len(id_boxes)\n",
    "                if split != \"train\":\n",
    "                    boxes += ood_boxes; labels += [1]*len(ood_boxes)\n",
    "\n",
    "                if not boxes:\n",
    "                    continue\n",
    "\n",
    "                # embeddings (batched + cached)\n",
    "                emb = embed_crops_batched(img, boxes, sd_tok)  # (N, D)\n",
    "\n",
    "                # geometry + pos\n",
    "                geo, pos = [], []\n",
    "                for x0,y0,x1,y1 in boxes:\n",
    "                    cx, cy = 0.5*(x0+x1), 0.5*(y0+y1)\n",
    "                    w, h   = max(1.0, x1-x0), max(1.0, y1-y0)\n",
    "                    asp, area = w/h, (w*h)/(W*H)\n",
    "                    geo.append([cx/W, cy/H, w/W, h/H, asp, area])\n",
    "                    pos.append([cx/W, cy/H])\n",
    "\n",
    "                X  = torch.from_numpy(np.concatenate([emb, np.array(geo, dtype=np.float32)], axis=1))\n",
    "                P  = torch.from_numpy(np.array(pos, dtype=np.float32))\n",
    "                Y  = torch.from_numpy(np.array(labels, dtype=np.int64))\n",
    "\n",
    "                k = min(knn_k, max(1, len(P)-1))\n",
    "                ei = knn_graph(P, k=k)\n",
    "\n",
    "                g = GeoData(x=X, pos=P, y=Y, edge_index=ei)\n",
    "                g.meta = {\"sd_token\": sd_tok, \"channel\": ch, \"scene\": sc[\"name\"]}\n",
    "                graphs.append(g)\n",
    "\n",
    "            # advance safely to next sample (may be \"\")\n",
    "            s_tok = sample[\"next\"]\n",
    "    return graphs\n",
    "\n",
    "\n",
    "train_graphs = build_graphs(\"train\", knn_k=6, include_id_from_ood_frames=True)\n",
    "val_graphs   = build_graphs(\"val\",   knn_k=6, include_id_from_ood_frames=True)\n",
    "print(f\"Graphs built: {len(train_graphs)} train | {len(val_graphs)} val\")\n",
    "\n",
    "def todev(g: GeoData):\n",
    "    return GeoData(\n",
    "        x=g.x.to(device), pos=g.pos.to(device), y=g.y.to(device),\n",
    "        edge_index=g.edge_index.to(device)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2a784a01-ca94-4eaf-8303-767f97abc4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 1984 train | 2087 val\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "def is_valid_graph(g):\n",
    "    return (g.x is not None) and (g.x.numel() > 0) and (g.x.size(0) >= 2) and (g.edge_index.numel() > 0)\n",
    "\n",
    "def nan_to_num_(t):\n",
    "    return torch.nan_to_num(t, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "\n",
    "# 1) Filter degenerate graphs\n",
    "train_graphs = [g for g in train_graphs if is_valid_graph(g)]\n",
    "val_graphs   = [g for g in val_graphs   if is_valid_graph(g)]\n",
    "\n",
    "if len(train_graphs) == 0:\n",
    "    raise RuntimeError(\"No valid training graphs after filtering. Check detection boxes / graph build.\")\n",
    "\n",
    "# 2) Compute feature normalization on train set\n",
    "with torch.no_grad():\n",
    "    Xs = []\n",
    "    for g in train_graphs[:min(200, len(train_graphs))]:\n",
    "        Xs.append(g.x.float())\n",
    "    Xs = torch.cat(Xs, dim=0)\n",
    "    mu = torch.mean(Xs, dim=0)\n",
    "    sd = torch.std(Xs, dim=0)\n",
    "    sd = torch.clamp(sd, min=1e-3) \n",
    "\n",
    "def normalize_graphs(graphs):\n",
    "    out = []\n",
    "    for g in graphs:\n",
    "        gx = g.x.float()\n",
    "        gx = (gx - mu) / sd\n",
    "        gx = nan_to_num_(gx)\n",
    "        g.x = gx\n",
    "        # also sanitize positions just in case\n",
    "        g.pos = nan_to_num_(g.pos.float())\n",
    "        out.append(g)\n",
    "    return out\n",
    "\n",
    "train_graphs = normalize_graphs(train_graphs)\n",
    "val_graphs   = normalize_graphs(val_graphs)\n",
    "\n",
    "print(f\"After filtering: {len(train_graphs)} train | {len(val_graphs)} val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbdbd4fc-acab-4032-a642-c27ccfaa2f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VGAE] epoch 001  train=inf  val=9.327306\n",
      "[VGAE] epoch 002  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 003  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 004  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 005  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 006  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 007  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 008  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 009  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 010  train=10.396014  val=9.327306\n",
      "[VGAE] epoch 011  train=10.396014  val=9.327306\n",
      "Early stopping at epoch 11\n",
      "Loaded best model with val_loss=9.327306\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "if len(train_graphs) == 0 or len(val_graphs) == 0:\n",
    "    raise RuntimeError(\"No graphs built. Check paths/JSONs and that frames contain ID/OOD boxes.\")\n",
    "\n",
    "in_dim = train_graphs[0].x.size(1)\n",
    "\n",
    "\n",
    "class GEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hid=256, z=128):\n",
    "        super().__init__()\n",
    "        self.g1 = GCNConv(in_dim, hid)\n",
    "        self.g2 = GCNConv(hid, z)\n",
    "\n",
    "    def forward(self, x, ei):\n",
    "        h = F.relu(self.g1(x, ei))\n",
    "        z = self.g2(h, ei)\n",
    "        return z\n",
    "\n",
    "\n",
    "class VGAE(nn.Module):\n",
    "    def __init__(self, in_dim, hid=256, z=128):\n",
    "        super().__init__()\n",
    "        self.mu = GEncoder(in_dim, hid, z)\n",
    "        self.lv = GEncoder(in_dim, hid, z)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(z, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, in_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, ei):\n",
    "        mu = self.mu(x, ei)\n",
    "        logv = self.lv(x, ei)\n",
    "        std = torch.exp(0.5 * logv)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        xrec = self.dec(z)\n",
    "        return z, xrec, mu, logv\n",
    "\n",
    "\n",
    "def vgae_loss(x, xrec, mu, logv, ei, z, return_components=False):\n",
    "    xrec = torch.nan_to_num(xrec)\n",
    "    mu = torch.nan_to_num(mu)\n",
    "    logv = torch.clamp(torch.nan_to_num(logv), min=-10.0, max=10.0)\n",
    "\n",
    "    feat = F.mse_loss(xrec, x)\n",
    "\n",
    "    if ei.numel() > 0:\n",
    "        pos = ei\n",
    "        num_pos = pos.size(1)\n",
    "        if num_pos > 0:\n",
    "            neg = negative_sampling(pos, num_nodes=z.size(0), num_neg_samples=num_pos)\n",
    "            pos_log = (z[pos[0]] * z[pos[1]]).sum(dim=1)\n",
    "            neg_log = (z[neg[0]] * z[neg[1]]).sum(dim=1)\n",
    "            pos_tgt = torch.ones_like(pos_log)\n",
    "            neg_tgt = torch.zeros_like(neg_log)\n",
    "            logits = torch.cat([pos_log, neg_log], dim=0)\n",
    "            target = torch.cat([pos_tgt, neg_tgt], dim=0)\n",
    "            edge = F.binary_cross_entropy_with_logits(torch.nan_to_num(logits), target)\n",
    "        else:\n",
    "            edge = torch.tensor(0.0, device=logv.device)\n",
    "    else:\n",
    "        edge = torch.tensor(0.0, device=logv.device)\n",
    "\n",
    "    kl = -0.5 * torch.mean(1 + logv - mu.pow(2) - torch.exp(logv))\n",
    "\n",
    "    total = feat + 0.1 * edge + 1e-3 * kl\n",
    "    if torch.isnan(total):\n",
    "        total = feat + 1e-3 * kl\n",
    "\n",
    "    if return_components:\n",
    "        return total, feat.detach().item(), edge.detach().item(), kl.detach().item()\n",
    "    return total\n",
    "\n",
    "\n",
    "\n",
    "def eval_vgae(model, graphs, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for g in graphs:\n",
    "            gd = todev(g)\n",
    "            z, xrec, mu, logv = model(gd.x, gd.edge_index)\n",
    "            loss = vgae_loss(gd.x, xrec, mu, logv, gd.edge_index, z)\n",
    "            if not torch.isnan(loss):\n",
    "                losses.append(loss.item())\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "\n",
    "model_vgae = VGAE(in_dim).to(device)\n",
    "opt = torch.optim.Adam(model_vgae.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "\n",
    "max_epochs = 100\n",
    "patience   = 10\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    np.random.shuffle(train_graphs)\n",
    "    model_vgae.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for g in train_graphs:\n",
    "        gd = todev(g)\n",
    "        z, xrec, mu, logv = model_vgae(gd.x, gd.edge_index)\n",
    "        loss = vgae_loss(gd.x, xrec, mu, logv, gd.edge_index, z)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_vgae.parameters(), max_norm=1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if not torch.isnan(loss):\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "    mean_train = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "    val_loss = eval_vgae(model_vgae, val_graphs, device)\n",
    "\n",
    "    print(f\"[VGAE] epoch {epoch:03d}  train={mean_train:.6f}  val={val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model_vgae.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "if best_state is not None:\n",
    "    model_vgae.load_state_dict(best_state)\n",
    "    model_vgae.to(device)\n",
    "    print(f\"Loaded best model with val_loss={best_val:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4cafdf9-58f9-4a00-a50f-30f75c7f9ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ctx-Mahalanobis] AUROC=0.8636  FPR@95=0.4091 (TPR≈0.950)\n",
      "\n",
      "[Ctx-Mahalanobis] per-camera:\n",
      "CAM_BACK         AUROC=0.8771  FPR@95=0.3839  n=6575\n",
      "CAM_BACK_LEFT    AUROC=0.7829  FPR@95=0.5963  n=1189\n",
      "CAM_BACK_RIGHT   AUROC=0.8331  FPR@95=0.5048  n=3450\n",
      "CAM_FRONT        AUROC=0.8975  FPR@95=0.2755  n=5708\n",
      "CAM_FRONT_LEFT   AUROC=0.8459  FPR@95=0.4265  n=2176\n",
      "CAM_FRONT_RIGHT  AUROC=0.8085  FPR@95=0.4987  n=4339\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "def neighbor_mean_features(g):\n",
    "    X = g.x.float().cpu()                   # already normalized from earlier stabilize step\n",
    "    ei = g.edge_index\n",
    "    N, D = X.shape\n",
    "    if ei.numel() == 0:\n",
    "        return X.numpy(), X.numpy(), g.y.cpu().numpy()\n",
    "    src, dst = ei\n",
    "    ones = torch.ones_like(src, dtype=torch.float32)\n",
    "    deg  = torch.zeros(N, dtype=torch.float32).scatter_add_(0, src.cpu(), ones)\n",
    "    sumN = torch.zeros(N, D, dtype=torch.float32).index_add_(0, src.cpu(), X[dst.cpu()])\n",
    "    C = torch.where(deg.view(-1,1) > 0, sumN/deg.clamp_min(1.0).view(-1,1), X)  # neighbor mean; fallback to self\n",
    "    return X.numpy(), C.numpy(), g.y.cpu().numpy()\n",
    "\n",
    "# === Fit on TRAIN (ID nodes only) ===\n",
    "X_tr, C_tr = [], []\n",
    "for g in train_graphs:\n",
    "    x, c, y = neighbor_mean_features(g)\n",
    "    m = (y == 0)\n",
    "    if m.any():\n",
    "        X_tr.append(x[m]); C_tr.append(c[m])\n",
    "Z_tr = np.concatenate([np.concatenate(X_tr,0), np.concatenate(C_tr,0)], axis=1)  # (N_id, 2D)\n",
    "\n",
    "mu = Z_tr.mean(0, keepdims=True)\n",
    "sd = Z_tr.std(0, keepdims=True).clip(1e-3)\n",
    "Zs = (Z_tr - mu)/sd\n",
    "\n",
    "cov = LedoitWolf().fit(Zs)\n",
    "mu_loc  = cov.location_           # (2D,)\n",
    "prec    = cov.precision_          # (2D,2D)\n",
    "\n",
    "def maha2(Z):\n",
    "    Zs = (Z - mu)/sd\n",
    "    d  = Zs - mu_loc\n",
    "    return np.einsum(\"nd,dd,nd->n\", d, prec, d)     # higher = more OOD\n",
    "\n",
    "def fpr_at_tpr(y, s, target=0.95):\n",
    "    fpr, tpr, thr = roc_curve(y, s)\n",
    "    i = np.argmin(np.abs(tpr - target))\n",
    "    return float(fpr[i]), float(tpr[i]), float(thr[i])\n",
    "\n",
    "# === Score VAL ===\n",
    "scores_all, labels_all = [], []\n",
    "per_cam_scores, per_cam_labels = defaultdict(list), defaultdict(list)\n",
    "\n",
    "for g in val_graphs:\n",
    "    x, c, y = neighbor_mean_features(g)\n",
    "    Z = np.concatenate([x, c], axis=1)\n",
    "    s = maha2(Z)\n",
    "    scores_all.append(s); labels_all.append(y)\n",
    "    cam = g.meta[\"channel\"]\n",
    "    per_cam_scores[cam].append(s); per_cam_labels[cam].append(y)\n",
    "\n",
    "scores_all = np.concatenate(scores_all)\n",
    "labels_all = np.concatenate(labels_all)\n",
    "\n",
    "auroc = roc_auc_score(labels_all, scores_all)\n",
    "fpr95, tpr95, _ = fpr_at_tpr(labels_all, scores_all, 0.95)\n",
    "print(f\"[Ctx-Mahalanobis] AUROC={auroc:.4f}  FPR@95={fpr95:.4f} (TPR≈{tpr95:.3f})\")\n",
    "\n",
    "print(\"\\n[Ctx-Mahalanobis] per-camera:\")\n",
    "for cam in sorted(per_cam_scores.keys()):\n",
    "    s = np.concatenate(per_cam_scores[cam]); y = np.concatenate(per_cam_labels[cam])\n",
    "    if len(set(y)) < 2:\n",
    "        print(f\"{cam:16s} — not enough pos/neg\"); continue\n",
    "    auc = roc_auc_score(y, s); f95,_,_ = fpr_at_tpr(y, s, 0.95)\n",
    "    print(f\"{cam:16s} AUROC={auc:.4f}  FPR@95={f95:.4f}  n={len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6aff5a-0d62-4d6c-bf08-97996681002f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgae_node_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m v_out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m val_graphs:\n\u001b[0;32m----> 4\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mvgae_node_scores\u001b[49m(g)          \u001b[38;5;66;03m# shape (N_nodes,)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     labels \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()            \u001b[38;5;66;03m# 0 = ID, 1 = OOD\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     v_out\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: scores, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vgae_node_scores' is not defined"
     ]
    }
   ],
   "source": [
    "# Build VGAE outputs per graph for later ensembling/plots\n",
    "v_out = []\n",
    "for g in val_graphs:\n",
    "    scores = vgae_node_scores(g)          # shape (N_nodes,)\n",
    "    labels = g.y.cpu().numpy()            # 0 = ID, 1 = OOD\n",
    "    v_out.append({\"scores\": scores, \"labels\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c0004f6-8593-4fd9-b35c-9049154ac4c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv_out\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# VGAE OOD score\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sr \u001b[38;5;241m=\u001b[39m scores_all                                     \u001b[38;5;66;03m# Ctx-Maha OOD score\u001b[39;00m\n\u001b[1;32m      3\u001b[0m y  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([o[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m v_out])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "sv = np.concatenate([o[\"scores\"] for o in v_out])   # VGAE OOD score\n",
    "sr = scores_all                                     # Ctx-Maha OOD score\n",
    "y  = np.concatenate([o[\"labels\"] for o in v_out])\n",
    "\n",
    "best_auc, best_fpr95, best_a = -1, None, None\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "def fpr_at_tpr(y, s, target=0.95):\n",
    "    fpr, tpr, thr = roc_curve(y, s)\n",
    "    i = np.argmin(np.abs(tpr - target))\n",
    "    return float(fpr[i]), float(tpr[i]), float(thr[i])\n",
    "\n",
    "for a in np.linspace(0,1,21):\n",
    "    s = a*sv + (1-a)*sr\n",
    "    auc = roc_auc_score(y, s)\n",
    "    f95,_,_ = fpr_at_tpr(y, s, 0.95)\n",
    "    if auc > best_auc:\n",
    "        best_auc, best_fpr95, best_a = auc, f95, a\n",
    "print(f\"[VGAE + CtxMaha] best α={best_a:.2f}  AUROC={best_auc:.4f}  FPR@95={best_fpr95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81b12088-4cd7-41de-a836-2ae30e6f74b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 262\u001b[0m\n\u001b[1;32m    254\u001b[0m tsne_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nodes) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m    256\u001b[0m     Z2 \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[0;32m--> 262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m    264\u001b[0m     m_id \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \n\u001b[1;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:841\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarnes_hut\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 841\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    848\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    849\u001b[0m         X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64]\n\u001b[1;32m    850\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# ========================= VISUALIZATION PACK =========================\n",
    "# Requirements: assumes in memory\n",
    "# - DATAROOT, JSONDIR, ALL_CAMS\n",
    "# - sd_rows, samples, scenes, sensor_by, calib_by, sample_to_ch2sd\n",
    "# - gt_id, gt_ood\n",
    "# - val_graphs (built earlier)\n",
    "# - model_vgae, device\n",
    "#\n",
    "# Optional but nice-to-have:\n",
    "# - v_out from your VGAE scoring cell (not required here)\n",
    "\n",
    "import os, math, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "RANDOM_SEED_FOR_OOD_SAMPLES = 42\n",
    "random.seed(RANDOM_SEED_FOR_OOD_SAMPLES)\n",
    "\n",
    "OUTDIR = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "val_graph_by_sd = {g.meta[\"sd_token\"]: g for g in val_graphs}\n",
    "\n",
    "\n",
    "def get_boxes_and_labels_for_sd(sd_token: str) -> Tuple[List[List[float]], List[int]]:\n",
    "    boxes, labels = [], []\n",
    "    for b in gt_id.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"])\n",
    "        labels.append(0)\n",
    "    for b in gt_ood.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"])\n",
    "        labels.append(1)\n",
    "    return boxes, labels\n",
    "\n",
    "\n",
    "def get_boxes_labels_classes_for_sd(sd_token: str):\n",
    "    boxes, labels, classes = [], [], []\n",
    "\n",
    "    for b in gt_id.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"])\n",
    "        labels.append(0)\n",
    "        cls = b.get(\"detection_name\", \"ID\")\n",
    "        classes.append(str(cls))\n",
    "\n",
    "    for b in gt_ood.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"])\n",
    "        labels.append(1)\n",
    "        cls = b.get(\"detection_name\", \"OOD\")\n",
    "        classes.append(str(cls))\n",
    "\n",
    "    return boxes, labels, classes\n",
    "\n",
    "\n",
    "def img_path_for_sd(sd_token: str) -> Path:\n",
    "    return DATAROOT / sd_rows[sd_token][\"filename\"]\n",
    "\n",
    "\n",
    "def channel_for_sd(sd_token: str) -> str:\n",
    "    try:\n",
    "        calib = calib_by[sd_rows[sd_token][\"calibrated_sensor_token\"]]\n",
    "        return sensor_by[calib[\"sensor_token\"]][\"channel\"]\n",
    "    except Exception:\n",
    "        return \"?\"\n",
    "\n",
    "\n",
    "# Colors in BGR for OpenCV\n",
    "CLR_ID  = (0, 200, 0)\n",
    "CLR_OOD = (0, 0, 200)\n",
    "CLR_EDGE = (255, 255, 255)\n",
    "CLR_TXT = (255, 255, 255)\n",
    "\n",
    "\n",
    "def draw_frame_with_graph(sd_token: str, show_edges: bool = True) -> np.ndarray:\n",
    "    ip = img_path_for_sd(sd_token)\n",
    "    if not ip.exists():\n",
    "        return np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "\n",
    "    rgb = np.array(Image.open(ip).convert(\"RGB\"))\n",
    "    img = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    boxes, labels = get_boxes_and_labels_for_sd(sd_token)\n",
    "    for (x0, y0, x1, y1), lab in zip(boxes, labels):\n",
    "        p0 = (max(0, int(x0)), max(0, int(y0)))\n",
    "        p1 = (min(W - 1, int(x1)), min(H - 1, int(y1)))\n",
    "        cv2.rectangle(img, p0, p1, CLR_OOD if lab == 1 else CLR_ID, 2)\n",
    "\n",
    "    g = val_graph_by_sd.get(sd_token, None)\n",
    "    if show_edges and g is not None and g.pos is not None and g.pos.shape[0] == len(boxes):\n",
    "        pos = g.pos.cpu().numpy()\n",
    "        cx = (pos[:, 0] * W).astype(int)\n",
    "        cy = (pos[:, 1] * H).astype(int)\n",
    "        ei = g.edge_index.cpu().numpy()\n",
    "        for s, t in zip(ei[0], ei[1]):\n",
    "            p_start = (int(cx[s]), int(cy[s]))\n",
    "            p_end   = (int(cx[t]), int(cy[t]))\n",
    "            cv2.arrowedLine(\n",
    "                img,\n",
    "                p_start,\n",
    "                p_end,\n",
    "                CLR_EDGE,\n",
    "                1,\n",
    "                tipLength=0.2,\n",
    "                line_type=cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "    tag = f\"{channel_for_sd(sd_token)} | {sd_token[:8]}\"\n",
    "    cv2.rectangle(img, (5, 5), (5 + len(tag) * 9, 28), (0, 0, 0), -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        tag,\n",
    "        (10, 25),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.6,\n",
    "        CLR_TXT,\n",
    "        1,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_scene_multiview_video(scene_name: str, fps=6, resize=(640, 360),\n",
    "                               max_frames=None, show_edges=True):\n",
    "    scene_row = next((s for s in scenes if s[\"name\"] == scene_name), None)\n",
    "    assert scene_row is not None, f\"Scene {scene_name} not found\"\n",
    "    s_tok = scene_row[\"first_sample_token\"]\n",
    "    w, h = resize\n",
    "    grid_w, grid_h = 3 * w, 2 * h\n",
    "    out_path = OUTDIR / f\"{scene_name}__multiview.mp4\"\n",
    "    vw = cv2.VideoWriter(\n",
    "        str(out_path),\n",
    "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        fps,\n",
    "        (grid_w, grid_h)\n",
    "    )\n",
    "    n = 0\n",
    "    while s_tok:\n",
    "        ch2sd = sample_to_ch2sd.get(s_tok, {})\n",
    "        tiles = []\n",
    "        for ch in ALL_CAMS:\n",
    "            sd_tok = ch2sd.get(ch)\n",
    "            if sd_tok is None:\n",
    "                tile = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                frm = draw_frame_with_graph(sd_tok, show_edges=show_edges)\n",
    "                tile = cv2.resize(frm, (w, h), interpolation=cv2.INTER_AREA)\n",
    "            tiles.append(tile)\n",
    "        grid = np.vstack([np.hstack(tiles[:3]), np.hstack(tiles[3:6])])\n",
    "        cv2.rectangle(\n",
    "            grid,\n",
    "            (10, grid_h - 40),\n",
    "            (10 + 450, grid_h - 10),\n",
    "            (0, 0, 0),\n",
    "            -1\n",
    "        )\n",
    "        cv2.putText(\n",
    "            grid,\n",
    "            f\"{scene_name}  |  frame {n}\",\n",
    "            (15, grid_h - 18),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        vw.write(grid)\n",
    "        n += 1\n",
    "        if max_frames and n >= max_frames:\n",
    "            break\n",
    "        s_tok = samples[s_tok][\"next\"]\n",
    "    vw.release()\n",
    "    return str(out_path), n\n",
    "\n",
    "\n",
    "scenes_to_render = [s[\"name\"] for s in scenes][:2]\n",
    "video_paths = []\n",
    "for scn in scenes_to_render:\n",
    "    p, n = make_scene_multiview_video(\n",
    "        scn, fps=6, resize=(640, 360), max_frames=80, show_edges=True\n",
    "    )\n",
    "    video_paths.append((p, n))\n",
    "\n",
    "snap_paths = []\n",
    "for scn in scenes_to_render:\n",
    "    s_tok = next(s for s in scenes if s[\"name\"] == scn)[\"first_sample_token\"]\n",
    "    ch2sd = sample_to_ch2sd.get(s_tok, {})\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "    for i, ch in enumerate(ALL_CAMS):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(ch)\n",
    "        sd_tok = ch2sd.get(ch)\n",
    "        if sd_tok is None or sd_tok not in val_graph_by_sd:\n",
    "            ax.text(0.5, 0.5, \"(no data)\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        g = val_graph_by_sd[sd_tok]\n",
    "        pos = g.pos.cpu().numpy()\n",
    "        y = g.y.cpu().numpy()\n",
    "        ei = g.edge_index.cpu().numpy()\n",
    "\n",
    "        for s, t in zip(ei[0], ei[1]):\n",
    "            x_start, y_start = pos[s, 0], pos[s, 1]\n",
    "            x_end, y_end = pos[t, 0], pos[t, 1]\n",
    "            ax.annotate(\n",
    "                \"\",\n",
    "                xy=(x_end, y_end),\n",
    "                xytext=(x_start, y_start),\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=\"->\",\n",
    "                    linewidth=0.5,\n",
    "                    color=\"0.7\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        m_id = (y == 0)\n",
    "        m_ood = (y == 1)\n",
    "        ax.scatter(pos[m_id, 0], pos[m_id, 1], s=15, label=\"ID\")\n",
    "        ax.scatter(pos[m_ood, 0], pos[m_ood, 1], s=20, marker=\"x\", label=\"OOD\")\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(1, 0)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.legend(loc=\"lower right\", fontsize=8)\n",
    "    fig.suptitle(f\"{scn} — Directed Image-plane Graphs (per camera)\")\n",
    "    outp = OUTDIR / f\"{scn}__graphs_snapshot.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outp, dpi=150)\n",
    "    plt.close(fig)\n",
    "    snap_paths.append(str(outp))\n",
    "\n",
    "nodes, labels = [], []\n",
    "max_nodes = 4000\n",
    "count = 0\n",
    "for g in val_graphs:\n",
    "    if count >= max_nodes:\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        mu = model_vgae.mu(g.x.to(device), g.edge_index.to(device))\n",
    "        z = mu.detach().cpu().numpy()\n",
    "    y = g.y.cpu().numpy()\n",
    "    nodes.append(z)\n",
    "    labels.append(y)\n",
    "    count += len(y)\n",
    "\n",
    "nodes = np.concatenate(nodes, 0)\n",
    "labels = np.concatenate(labels, 0)\n",
    "tsne_path = None\n",
    "if len(nodes) >= 10:\n",
    "    Z2 = TSNE(\n",
    "        n_components=2,\n",
    "        init=\"pca\",\n",
    "        perplexity=30,\n",
    "        learning_rate=\"auto\",\n",
    "        n_iter=1000\n",
    "    ).fit_transform(nodes)\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    m_id = (labels == 0)\n",
    "    m_ood = (labels == 1)\n",
    "    plt.scatter(Z2[m_id, 0], Z2[m_id, 1], s=8, label=\"ID\")\n",
    "    plt.scatter(Z2[m_ood, 0], Z2[m_ood, 1], s=8, label=\"OOD\", marker=\"x\")\n",
    "    plt.legend()\n",
    "    plt.title(\"t-SNE of VGAE Latents (val subset)\")\n",
    "    tsne_path = OUTDIR / \"tsne_vgae_val.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(tsne_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def vgae_node_scores(g):\n",
    "    x = g.x.to(device)\n",
    "    ei = g.edge_index.to(device)\n",
    "\n",
    "    z, xrec, _, _ = model_vgae(x, ei)\n",
    "    err = ((x - xrec) ** 2).sum(dim=1)\n",
    "\n",
    "    src, dst = ei\n",
    "    sim = torch.sigmoid((z[src] * z[dst]).sum(dim=1))\n",
    "\n",
    "    deg = torch.zeros(z.size(0), device=z.device).scatter_add_(\n",
    "        0, src, torch.ones_like(src, dtype=torch.float32)\n",
    "    )\n",
    "    agg = torch.zeros(z.size(0), device=z.device).scatter_add_(0, src, sim)\n",
    "    mean_sim = torch.where(deg > 0, agg / deg, torch.zeros_like(deg))\n",
    "\n",
    "    return (err + (1.0 - mean_sim)).cpu().numpy()\n",
    "\n",
    "\n",
    "# Build pool with class names\n",
    "pool = []\n",
    "for g in val_graphs:\n",
    "    sd = g.meta[\"sd_token\"]\n",
    "    boxes, labs, clss = get_boxes_labels_classes_for_sd(sd)\n",
    "    if len(boxes) != g.x.size(0):\n",
    "        continue\n",
    "    s = vgae_node_scores(g)\n",
    "    for bb, lb, cls, ss in zip(boxes, labs, clss, s):\n",
    "        pool.append((float(ss), sd, bb, int(lb), cls))\n",
    "\n",
    "# Filter OOD by class name: horse, dog, elephant, forklift, lawn mower\n",
    "\n",
    "\n",
    "OOD_KEEP_SUBSTRINGS = [\n",
    "    \"horse\",\"zebra\", \"giraffe\", \"elephant\", \"bear\", \"sheep\", \"cow\", \"horse\", \"bird\",\n",
    "    \"dog\", \"cat\",\"sports ball\",\"chair\", \"bench\",\"boat\",\"elephant\",\n",
    "    \"forklift\", \"fork lift\",\n",
    "    \"lawnmower\", \"lawn mower\",\n",
    "]\n",
    "OOD_KEEP_SUBSTRINGS = [s.lower() for s in OOD_KEEP_SUBSTRINGS]\n",
    "\n",
    "def is_kept_ood_class(cls_name: str) -> bool:\n",
    "    c = cls_name.lower()\n",
    "    return any(sub in c for sub in OOD_KEEP_SUBSTRINGS)\n",
    "\n",
    "# Split and filter\n",
    "pool_ood_scored, pool_id_scored = [], []\n",
    "for score, sd, bb, lb, cls in pool:\n",
    "    if lb == 1:\n",
    "        if is_kept_ood_class(cls):\n",
    "            pool_ood_scored.append((score, sd, bb))\n",
    "    else:\n",
    "        pool_id_scored.append((score, sd, bb))\n",
    "\n",
    "\n",
    "# ---- NEW: Deduplicate OOD crops → keep best OOD per frame (sd_token) ----\n",
    "# This ensures at most one OOD crop per image in the contact sheet.\n",
    "best_ood_per_frame = {}  # sd_token -> (score, sd, bb)\n",
    "for score, sd, bb in pool_ood_scored:\n",
    "    if (sd not in best_ood_per_frame) or (score > best_ood_per_frame[sd][0]):\n",
    "        best_ood_per_frame[sd] = (score, sd, bb)\n",
    "\n",
    "unique_ood = list(best_ood_per_frame.values())\n",
    "\n",
    "# ---- Randomly choose up to 50 OOD crops from unique frames ----\n",
    "NUM_OOD_PICS = 100\n",
    "if len(unique_ood) > NUM_OOD_PICS:\n",
    "    top_ood = random.sample(unique_ood, NUM_OOD_PICS)\n",
    "else:\n",
    "    top_ood = unique_ood[:]  # all of them if fewer than 50\n",
    "\n",
    "# ---- For ID, keep highest scoring ones (e.g., 20) ----\n",
    "K_ID = 20\n",
    "pool_id_scored.sort(key=lambda x: -x[0])\n",
    "top_id = pool_id_scored[:K_ID]\n",
    "\n",
    "def make_contact_sheet(items, title, out_path, tile=128, cols=10):\n",
    "    rows = math.ceil(len(items) / cols)\n",
    "    W, H = cols * tile, rows * tile + 40\n",
    "\n",
    "    sheet = Image.new(\"RGB\", (W, H), (20, 20, 20))\n",
    "    draw  = ImageDraw.Draw(sheet)\n",
    "    draw.text((10, 10), title, fill=(255, 255, 255))\n",
    "    yoff = 40\n",
    "\n",
    "    for i, (score, sd, bb) in enumerate(items):\n",
    "        im = Image.open(img_path_for_sd(sd)).convert(\"RGB\")\n",
    "        x0, y0, x1, y1 = [int(v) for v in bb]\n",
    "        x0 = max(0,     min(x0, im.width  - 1))\n",
    "        y0 = max(0,     min(y0, im.height - 1))\n",
    "        x1 = max(x0+1,  min(x1, im.width))\n",
    "        y1 = max(y0+1,  min(y1, im.height))\n",
    "\n",
    "        crop = im.crop((x0, y0, x1, y1)).resize((tile, tile), Image.BILINEAR)\n",
    "        r, c = divmod(i, cols)\n",
    "\n",
    "        sheet.paste(crop, (c * tile, yoff + r * tile))\n",
    "        draw.text((c * tile + 4, yoff + r * tile + 4),\n",
    "                  f\"{score:.2f}\", fill=(255, 255, 0))\n",
    "\n",
    "    sheet = sheet.convert(\"RGB\")\n",
    "    sheet.save(out_path, format=\"PNG\")\n",
    "    return out_path\n",
    "\n",
    "sheet_ood = OUTDIR / \"topK_vgae_OOD_50rand.png\"\n",
    "sheet_id  = OUTDIR / \"topK_vgae_ID.png\"\n",
    "\n",
    "if len(top_ood):\n",
    "    make_contact_sheet(top_ood, \"Random 50 VGAE OOD (unique frames, filtered classes)\", sheet_ood)\n",
    "if len(top_id):\n",
    "    make_contact_sheet(top_id, \"Top-K VGAE ID (highest score among ID)\", sheet_id)\n",
    "\n",
    "print(\"Saved:\")\n",
    "for p, n in video_paths:\n",
    "    print(f\"  - Multiview video: {p}  (frames: {n})\")\n",
    "for p in snap_paths:\n",
    "    print(f\"  - Graph snapshot: {p}\")\n",
    "if tsne_path:\n",
    "    print(f\"  - t-SNE: {tsne_path}\")\n",
    "if sheet_ood.exists():\n",
    "    print(f\"  - Contact sheet (OOD, unique frames): {sheet_ood}\")\n",
    "if sheet_id.exists():\n",
    "    print(f\"  - Contact sheet (ID):                  {sheet_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46fda7f3-6d6d-4fd7-80fd-07b5e0c030f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal graph videos:\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 37)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 38)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 37)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 28)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 34)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 21)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 24)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 37)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 20)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 18)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 34)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 28)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT_LEFT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT_RIGHT__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK__graph_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK_LEFT__graph_temporal.mp4  (frames: 25)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK_RIGHT__graph_temporal.mp4  (frames: 40)\n"
     ]
    }
   ],
   "source": [
    "# === Temporal graph evolution videos (per scene × per camera) ===\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "VIS_DIR = Path(\"/data/Asad/NuScenesMiniNovel/vis_results\")\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Quick index: sd_token -> graph (val split)\n",
    "val_graph_by_sd = {g.meta[\"sd_token\"]: g for g in val_graphs}\n",
    "\n",
    "# Colors (BGR)\n",
    "CLR_BG   = (245,245,245)\n",
    "CLR_EDGE = (180,180,180)\n",
    "CLR_ID   = (40,170,40)\n",
    "CLR_OOD  = (30,60,220)\n",
    "\n",
    "def draw_graph_frame(g, size=800, point_sz=6, draw_edges=True):\n",
    "    W = H = size\n",
    "    canvas = np.full((H, W, 3), CLR_BG, dtype=np.uint8)\n",
    "    pos = g.pos.cpu().numpy()\n",
    "    y   = g.y.cpu().numpy()\n",
    "    cx = (pos[:,0]*W).astype(int)\n",
    "    cy = (pos[:,1]*H).astype(int)\n",
    "    ei = g.edge_index.cpu().numpy()\n",
    "    if draw_edges and ei.size > 0:\n",
    "        for s,t in zip(ei[0], ei[1]):\n",
    "            cv2.line(canvas, (cx[s],cy[s]), (cx[t],cy[t]), CLR_EDGE, 1, cv2.LINE_AA)\n",
    "    for i,(x,yid) in enumerate(zip(range(len(cx)), y)):\n",
    "        color = CLR_OOD if yid==1 else CLR_ID\n",
    "        cv2.circle(canvas, (cx[i], cy[i]), point_sz, color, -1, lineType=cv2.LINE_AA)\n",
    "    return canvas\n",
    "\n",
    "def make_temporal_graph_video(scene_name, camera, fps=6, max_frames=None, size=800, draw_edges=True):\n",
    "    scene_row = next((s for s in scenes if s[\"name\"]==scene_name), None)\n",
    "    if scene_row is None:\n",
    "        raise ValueError(f\"Scene {scene_name} not found\")\n",
    "    s_tok = scene_row[\"first_sample_token\"]\n",
    "    out_path = VIS_DIR / f\"{scene_name}__{camera}__graph_temporal.mp4\"\n",
    "    vw = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*'mp4v'), fps, (size, size))\n",
    "    n = 0\n",
    "    while s_tok:\n",
    "        ch2sd = sample_to_ch2sd.get(s_tok, {})\n",
    "        sd_tok = ch2sd.get(camera, None)\n",
    "        if sd_tok and sd_tok in val_graph_by_sd:\n",
    "            g = val_graph_by_sd[sd_tok]\n",
    "            frame = draw_graph_frame(g, size=size, draw_edges=draw_edges)\n",
    "            # tag\n",
    "            cv2.rectangle(frame, (10, size-40), (10+520, size-10), (0,0,0), -1)\n",
    "            cv2.putText(frame, f\"{scene_name} | {camera} | t={n}\", (16,size-16),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
    "            vw.write(frame)\n",
    "            n += 1\n",
    "            if max_frames and n>=max_frames:\n",
    "                break\n",
    "        s_tok = samples[s_tok][\"next\"]\n",
    "    vw.release()\n",
    "    return str(out_path), n\n",
    "\n",
    "# Render a few examples (change the lists as you like)\n",
    "scenes_to_render = [s[\"name\"] for s in scenes][:10]   # first two scenes; extend if you want more\n",
    "cams_to_render   = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "made = []\n",
    "for scn in scenes_to_render:\n",
    "    for cam in cams_to_render:\n",
    "        p, n = make_temporal_graph_video(scn, cam, fps=6, max_frames=150, size=800, draw_edges=True)\n",
    "        made.append((p,n))\n",
    "print(\"Temporal graph videos:\")\n",
    "for p,n in made:\n",
    "    print(f\"  {p}  (frames: {n})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eb40954-6b62-45cf-ba7c-39a80f84ec8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m VIS_DIR\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Collect VGAE scores\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sv_vgae \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv_out\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m y_all   \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([o[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m v_out])\n\u001b[1;32m     13\u001b[0m cams_all\u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# === ROC curves (overall + per camera) saved to vis_results ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "VIS_DIR = Path(\"/data/Asad/NuScenesMiniNovel/vis_results\")\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect VGAE scores\n",
    "sv_vgae = np.concatenate([o[\"scores\"] for o in v_out])\n",
    "y_all   = np.concatenate([o[\"labels\"] for o in v_out])\n",
    "cams_all= []\n",
    "for g, o in zip(val_graphs, v_out):\n",
    "    cams_all.extend([g.meta[\"channel\"]]*len(o[\"labels\"]))\n",
    "cams_all = np.array(cams_all)\n",
    "\n",
    "# Optional: context-Mahalanobis (if available)\n",
    "have_ctx = 'scores_all' in globals() and 'labels_all' in globals() and len(scores_all)==len(y_all)\n",
    "\n",
    "# Overall ROC\n",
    "plt.figure(figsize=(6,6))\n",
    "fpr_v, tpr_v, _ = roc_curve(y_all, sv_vgae); auc_v = roc_auc_score(y_all, sv_vgae)\n",
    "plt.plot(fpr_v, tpr_v, label=f\"VGAE (AUROC={auc_v:.3f})\")\n",
    "\n",
    "if have_ctx:\n",
    "    fpr_c, tpr_c, _ = roc_curve(y_all, scores_all); auc_c = roc_auc_score(y_all, scores_all)\n",
    "    plt.plot(fpr_c, tpr_c, label=f\"Context-Mahalanobis (AUROC={auc_c:.3f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1],'k--', linewidth=1)\n",
    "plt.xlim(0,1); plt.ylim(0,1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Overall ROC\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "out_overall = VIS_DIR / \"roc_overall.png\"\n",
    "plt.tight_layout(); plt.savefig(out_overall, dpi=200); plt.close()\n",
    "print(\"Saved:\", out_overall)\n",
    "\n",
    "# Per-camera ROCs\n",
    "for cam in sorted(set(cams_all)):\n",
    "    m = (cams_all==cam)\n",
    "    y_cam = y_all[m]; s_cam = sv_vgae[m]\n",
    "    if len(set(y_cam))<2: \n",
    "        continue\n",
    "    plt.figure(figsize=(6,6))\n",
    "    fpr_v, tpr_v, _ = roc_curve(y_cam, s_cam); auc_v = roc_auc_score(y_cam, s_cam)\n",
    "    plt.plot(fpr_v, tpr_v, label=f\"VGAE (AUROC={auc_v:.3f})\")\n",
    "    if have_ctx:\n",
    "        s_cam_c = scores_all[m]\n",
    "        fpr_c, tpr_c, _ = roc_curve(y_cam, s_cam_c); auc_c = roc_auc_score(y_cam, s_cam_c)\n",
    "        plt.plot(fpr_c, tpr_c, label=f\"Ctx-Maha (AUROC={auc_c:.3f})\")\n",
    "    plt.plot([0,1],[0,1],'k--', linewidth=1)\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC — {cam}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    out_cam = VIS_DIR / f\"roc_{cam}.png\"\n",
    "    plt.tight_layout(); plt.savefig(out_cam, dpi=200); plt.close()\n",
    "    print(\"Saved:\", out_cam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f833ce69-cf13-4e11-9c27-51e47e1838e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT__overlay_temporal.mp4  (frames: 39)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 39)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 39)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK__overlay_temporal.mp4  (frames: 39)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 39)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 39)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "Saved overlay video: /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "Done. Outputs:\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT__overlay_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK__overlay_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0061__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 39)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0103__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0553__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0655__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0757__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0796__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-0916__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1077__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 41)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1094__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_FRONT_RIGHT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK_LEFT__overlay_temporal.mp4  (frames: 40)\n",
      "  /data/Asad/NuScenesMiniNovel/vis_results/scene-1100__CAM_BACK_RIGHT__overlay_temporal.mp4  (frames: 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np, cv2, math\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "\n",
    "VIS_DIR = Path(\"/data/Asad/NuScenesMiniNovel/vis_results\")\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "val_graph_by_sd = {g.meta[\"sd_token\"]: g for g in val_graphs}\n",
    "\n",
    "# colors (BGR)\n",
    "CLR_ID   = (40,170,40)\n",
    "CLR_OOD  = (60,60,230)\n",
    "CLR_EDGE = (255,255,255)\n",
    "CLR_TXT  = (255,255,255)\n",
    "CLR_TRAIL= (255,215,0)   # gold trails\n",
    "\n",
    "def channel_for_sd(sd_token: str) -> str:\n",
    "    try:\n",
    "        calib = calib_by[sd_rows[sd_token][\"calibrated_sensor_token\"]]\n",
    "        return sensor_by[calib[\"sensor_token\"]][\"channel\"]\n",
    "    except Exception:\n",
    "        return \"?\"\n",
    "\n",
    "def boxes_and_labels(sd_token):\n",
    "    boxes, labels = [], []\n",
    "    for b in gt_id.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"]); labels.append(0)\n",
    "    for b in gt_ood.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"]); labels.append(1)\n",
    "    return boxes, labels\n",
    "\n",
    "def load_frame_bgr(sd_token):\n",
    "    img_path = DATAROOT / sd_rows[sd_token][\"filename\"]\n",
    "    if not img_path.exists():\n",
    "        return None\n",
    "    rgb = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def overlay_graph_on_image(bgr, g, boxes, labels, draw_edges=True):\n",
    "    H, W = bgr.shape[:2]\n",
    "    # boxes\n",
    "    for (x0,y0,x1,y1), lab in zip(boxes, labels):\n",
    "        p0 = (max(0,int(x0)), max(0,int(y0)))\n",
    "        p1 = (min(W-1,int(x1)), min(H-1,int(y1)))\n",
    "        cv2.rectangle(bgr, p0, p1, CLR_OOD if lab==1 else CLR_ID, 2)\n",
    "\n",
    "    # graph edges in image plane using normalized centers\n",
    "    if g is not None and g.pos is not None and g.pos.size(0)==len(boxes):\n",
    "        pos = g.pos.cpu().numpy()\n",
    "        cx, cy = (pos[:,0]*W).astype(int), (pos[:,1]*H).astype(int)\n",
    "        if draw_edges and g.edge_index is not None and g.edge_index.numel() > 0:\n",
    "            ei = g.edge_index.cpu().numpy()\n",
    "            for s,t in zip(ei[0], ei[1]):\n",
    "                cv2.line(bgr, (cx[s],cy[s]), (cx[t],cy[t]), CLR_EDGE, 1, cv2.LINE_AA)\n",
    "        # return centers for tracking\n",
    "        return bgr, np.stack([cx, cy], axis=1)\n",
    "    return bgr, None\n",
    "\n",
    "def associate_tracks(prev_xy, curr_xy, max_dist=60):\n",
    "\n",
    "    if prev_xy is None or len(prev_xy)==0 or curr_xy is None or len(curr_xy)==0:\n",
    "        return []\n",
    "    P, C = prev_xy.shape[0], curr_xy.shape[0]\n",
    "    d2 = ((prev_xy[:,None,:]-curr_xy[None,:,:])**2).sum(axis=2)  # (P,C)\n",
    "    pairs = []\n",
    "    used_p, used_c = set(), set()\n",
    "    # Flatten and sort by distance\n",
    "    flat = [(d2[i,j], i, j) for i in range(P) for j in range(C)]\n",
    "    flat.sort(key=lambda x: x[0])\n",
    "    for d, i, j in flat:\n",
    "        if i in used_p or j in used_c: \n",
    "            continue\n",
    "        if d <= max_dist**2:\n",
    "            used_p.add(i); used_c.add(j); pairs.append((i,j))\n",
    "    return pairs\n",
    "\n",
    "def make_temporal_overlay_video(scene_name, camera, fps=6, max_frames=None, trail_len=10, draw_edges=True, resize_to=None):\n",
    "\n",
    "    scene_row = next((s for s in scenes if s[\"name\"]==scene_name), None)\n",
    "    assert scene_row is not None, f\"Scene {scene_name} not found\"\n",
    "    s_tok = scene_row[\"first_sample_token\"]\n",
    "\n",
    "    # set up video writer after reading first valid frame to get size\n",
    "    first_frame = None\n",
    "    tmp_tok = s_tok\n",
    "    while tmp_tok and first_frame is None:\n",
    "        sd_tok = sample_to_ch2sd.get(tmp_tok, {}).get(camera)\n",
    "        if sd_tok:\n",
    "            first_frame = load_frame_bgr(sd_tok)\n",
    "        tmp_tok = samples[tmp_tok][\"next\"]\n",
    "    if first_frame is None:\n",
    "        raise RuntimeError(f\"No frames found for {scene_name} {camera}\")\n",
    "\n",
    "    if resize_to is not None:\n",
    "        W, H = resize_to\n",
    "        first_frame = cv2.resize(first_frame, (W,H), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        H, W = first_frame.shape[:2][0], first_frame.shape[:2][1]\n",
    "        H, W = first_frame.shape[0], first_frame.shape[1]\n",
    "\n",
    "    out_path = VIS_DIR / f\"{scene_name}__{camera}__overlay_temporal.mp4\"\n",
    "    vw = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\n",
    "\n",
    "    prev_centers = None\n",
    "    trails = []  # list[deque[(x,y)]]\n",
    "\n",
    "    n = 0\n",
    "    cur = s_tok\n",
    "    while cur:\n",
    "        sd_tok = sample_to_ch2sd.get(cur, {}).get(camera)\n",
    "        if not sd_tok:\n",
    "            cur = samples[cur][\"next\"]; \n",
    "            continue\n",
    "\n",
    "        img = load_frame_bgr(sd_tok)\n",
    "        if img is None:\n",
    "            cur = samples[cur][\"next\"]; \n",
    "            continue\n",
    "        if (img.shape[1], img.shape[0]) != (W, H):\n",
    "            img = cv2.resize(img, (W,H), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        g = val_graph_by_sd.get(sd_tok, None)\n",
    "        boxes, labels = boxes_and_labels(sd_tok)\n",
    "\n",
    "        # draw boxes + edges, get centers\n",
    "        img, centers = overlay_graph_on_image(img, g, boxes, labels, draw_edges=draw_edges)\n",
    "\n",
    "        # update trails\n",
    "        if centers is None or len(centers)==0:\n",
    "            # nothing to track; decay trails\n",
    "            for dq in trails:\n",
    "                if dq: dq.append(dq[-1])\n",
    "                while len(dq) > trail_len: dq.popleft()\n",
    "        else:\n",
    "            if prev_centers is None or len(prev_centers)==0 or len(trails)==0:\n",
    "                trails = [deque(maxlen=trail_len) for _ in range(len(centers))]\n",
    "                for i,(x,y) in enumerate(centers):\n",
    "                    trails[i].append((int(x),int(y)))\n",
    "            else:\n",
    "                pairs = associate_tracks(prev_centers, centers, max_dist=60)\n",
    "                # map previous trails to current order; unmatched get new trails\n",
    "                new_trails = [deque(maxlen=trail_len) for _ in range(len(centers))]\n",
    "                matched_curr = set()\n",
    "                for ip, jc in pairs:\n",
    "                    new_trails[jc] = trails[ip]  # re-use the deque\n",
    "                    new_trails[jc].append((int(centers[jc,0]), int(centers[jc,1])))\n",
    "                    matched_curr.add(jc)\n",
    "                for jc in range(len(centers)):\n",
    "                    if jc not in matched_curr:\n",
    "                        # new node → fresh trail\n",
    "                        new_trails[jc].append((int(centers[jc,0]), int(centers[jc,1])))\n",
    "                trails = new_trails\n",
    "\n",
    "        # draw trails with fading alpha\n",
    "        overlay = img.copy()\n",
    "        for dq in trails:\n",
    "            pts = list(dq)\n",
    "            for t in range(1, len(pts)):\n",
    "                alpha = t / len(pts)  # older is dimmer\n",
    "                color = (int(CLR_TRAIL[0]*alpha), int(CLR_TRAIL[1]*alpha), int(CLR_TRAIL[2]*alpha))\n",
    "                cv2.line(overlay, pts[t-1], pts[t], color, 2, cv2.LINE_AA)\n",
    "        img = cv2.addWeighted(overlay, 0.6, img, 0.4, 0)\n",
    "\n",
    "        # tag\n",
    "        cv2.rectangle(img, (10, H-40), (10+700, H-10), (0,0,0), -1)\n",
    "        cv2.putText(img, f\"{scene_name} | {camera} | t={n} | sd={sd_tok[:8]}\", (16, H-16),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, CLR_TXT, 1, cv2.LINE_AA)\n",
    "\n",
    "        vw.write(img)\n",
    "        n += 1\n",
    "        if max_frames and n>=max_frames:\n",
    "            break\n",
    "\n",
    "        prev_centers = centers\n",
    "        cur = samples[cur][\"next\"]\n",
    "\n",
    "    vw.release()\n",
    "    print(f\"Saved overlay video: {out_path}  (frames: {n})\")\n",
    "    return str(out_path), n\n",
    "\n",
    "# ---- render a few examples; tweak lists as needed ----\n",
    "scenes_to_render = [s[\"name\"] for s in scenes]\n",
    "\n",
    "cams_to_render   = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "made = []\n",
    "for scn in scenes_to_render:\n",
    "    for cam in cams_to_render:\n",
    "        try:\n",
    "            p, n = make_temporal_overlay_video(scn, cam, fps=6, max_frames=150, trail_len=12, draw_edges=True, resize_to=None)\n",
    "            made.append((p,n))\n",
    "        except Exception as e:\n",
    "            print(f\"Skip {scn} {cam} due to error: {e}\")\n",
    "\n",
    "print(\"Done. Outputs:\")\n",
    "for p,n in made:\n",
    "    print(f\"  {p}  (frames: {n})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c825d3a1-5425-485e-ac1a-1b4f0889d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "###########################################Testing Ir on Random Images multiview##############################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8c24a6a-1bf0-4bdf-b9c8-5b9dd36e0b18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m val_scores_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(val_scores_v)\n\u001b[1;32m    109\u001b[0m val_labels   \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(val_labels)\n\u001b[0;32m--> 110\u001b[0m fpr, tpr, thr \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_scores_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m thr95_vgae \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(thr[np\u001b[38;5;241m.\u001b[39margmin(np\u001b[38;5;241m.\u001b[39mabs(tpr \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.95\u001b[39m))])\n\u001b[1;32m    112\u001b[0m p_lo, p_hi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(val_scores_v, [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m95\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1095\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    994\u001b[0m     {\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m ):\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1095\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:810\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    808\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n\u001b[1;32m    809\u001b[0m assert_all_finite(y_true)\n\u001b[0;32m--> 810\u001b[0m \u001b[43massert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# Filter out zero-weighted samples, as they should not impact the result\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:200\u001b[0m, in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_all_finite\u001b[39m(\n\u001b[1;32m    175\u001b[0m     X,\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    180\u001b[0m ):\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Throw a ValueError if X contains NaN or infinity.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m        documentation.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# ======================= QUICK TESTER (CUDA) =======================\n",
    "# VGAE + Context-Mahalanobis (CUDA) overlay visualizations\n",
    "# Saves annotated PNGs + CSV to: /data/Asad/NuScenesMiniNovel/test_vis\n",
    "\n",
    "import os, csv, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# ---- sanity: required globals present ----\n",
    "req = [\"model_vgae\",\"device\",\"train_graphs\",\"val_graphs\",\"DATAROOT\",\"sd_rows\",\"samples\",\"scenes\",\"sample_to_ch2sd\",\"ALL_CAMS\",\"gt_id\",\"gt_ood\"]\n",
    "missing = [k for k in req if k not in globals()]\n",
    "assert not missing, f\"Missing globals: {missing}. Run the earlier setup/training cells first.\"\n",
    "\n",
    "OUTDIR = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/test_results\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fit_ctx_stats_cuda(train_graphs):\n",
    "    Xs, Cs = [], []\n",
    "    for g in train_graphs:\n",
    "        X = g.x.float().cpu()                       # (N,D) CPU for sklearn\n",
    "        ei = g.edge_index\n",
    "        N, D = X.shape\n",
    "        if ei.numel() == 0:\n",
    "            C = X\n",
    "        else:\n",
    "            src, dst = ei\n",
    "            ones = torch.ones_like(src, dtype=torch.float32)\n",
    "            deg  = torch.zeros(N, dtype=torch.float32).scatter_add_(0, src.cpu(), ones)\n",
    "            sumN = torch.zeros(N, D, dtype=torch.float32).index_add_(0, src.cpu(), X[dst.cpu()])\n",
    "            C = torch.where(deg.view(-1,1) > 0, sumN/deg.clamp_min(1.0).view(-1,1), X)\n",
    "        y = g.y.cpu().numpy()\n",
    "        m = (y == 0)                                # ID nodes only\n",
    "        if m.any():\n",
    "            Xs.append(X[m].numpy()); Cs.append(C[m].numpy())\n",
    "    assert Xs, \"No ID nodes in train_graphs to fit Mahalanobis stats.\"\n",
    "\n",
    "    Z_tr = np.concatenate([np.concatenate(Xs,0), np.concatenate(Cs,0)], axis=1).astype(np.float32)  # (M,2D)\n",
    "    mu   = Z_tr.mean(0, keepdims=True)\n",
    "    sd   = np.clip(Z_tr.std(0, keepdims=True), 1e-3, None)\n",
    "    Zs   = (Z_tr - mu)/sd\n",
    "    cov  = LedoitWolf().fit(Zs)\n",
    "    mu_loc = cov.location_.astype(np.float32)       # (2D,)\n",
    "    prec   = cov.precision_.astype(np.float32)      # (2D,2D)\n",
    "\n",
    "    # to CUDA tensors for fast scoring\n",
    "    maha_mu_t     = torch.from_numpy(mu).to(device)            # (1,2D)\n",
    "    maha_sd_t     = torch.from_numpy(sd).to(device)            # (1,2D)\n",
    "    maha_mu_loc_t = torch.from_numpy(mu_loc).to(device)        # (2D,)\n",
    "    maha_prec_t   = torch.from_numpy(prec).to(device)          # (2D,2D)\n",
    "    return maha_mu_t, maha_sd_t, maha_mu_loc_t, maha_prec_t\n",
    "\n",
    "# cache stats (fit once)\n",
    "if not all(k in globals() for k in [\"maha_mu_t\",\"maha_sd_t\",\"maha_mu_loc_t\",\"maha_prec_t\"]):\n",
    "    maha_mu_t, maha_sd_t, maha_mu_loc_t, maha_prec_t = fit_ctx_stats_cuda(train_graphs)\n",
    "\n",
    "@torch.no_grad()\n",
    "def ctx_maha_scores_cuda(g):\n",
    "    X = g.x.to(device).float()                                  # (N,D)\n",
    "    ei = g.edge_index\n",
    "    N, D = X.size()\n",
    "    if ei.numel() == 0:\n",
    "        C = X\n",
    "    else:\n",
    "        src, dst = ei.to(device)\n",
    "        ones = torch.ones_like(src, dtype=torch.float32, device=device)\n",
    "        deg  = torch.zeros(N, dtype=torch.float32, device=device).scatter_add_(0, src, ones)\n",
    "        sumN = torch.zeros(N, D, dtype=torch.float32, device=device).index_add_(0, src, X[dst])\n",
    "        C = torch.where(deg.view(-1,1) > 0, sumN/deg.clamp_min(1.0).view(-1,1), X)\n",
    "    Z  = torch.cat([X, C], dim=1)                               # (N,2D)\n",
    "    Zs = (Z - maha_mu_t) / maha_sd_t\n",
    "    d  = Zs - maha_mu_loc_t\n",
    "    s  = (d @ maha_prec_t) * d\n",
    "    return s.sum(dim=1)                                         # (N,)\n",
    "\n",
    "@torch.no_grad()\n",
    "def vgae_node_scores(g):\n",
    "    x = g.x.to(device); ei = g.edge_index.to(device)\n",
    "    z, xrec, _, _ = model_vgae(x, ei)\n",
    "    err = ((x - xrec)**2).sum(dim=1)\n",
    "    src, dst = ei\n",
    "    sim = torch.sigmoid((z[src]*z[dst]).sum(dim=1))\n",
    "    deg = torch.zeros(z.size(0), device=z.device).scatter_add_(0, src, torch.ones_like(src, dtype=torch.float32))\n",
    "    agg = torch.zeros(z.size(0), device=z.device).scatter_add_(0, src, sim)\n",
    "    mean_sim = torch.where(deg>0, agg/deg, torch.zeros_like(deg))\n",
    "    return (err + (1 - mean_sim)).cpu().numpy()\n",
    "\n",
    "def get_boxes_and_labels(sd_token: str) -> Tuple[List[List[float]], List[int]]:\n",
    "    boxes, labels = [], []\n",
    "    for b in gt_id.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"]); labels.append(0)\n",
    "    for b in gt_ood.get(sd_token, []):\n",
    "        boxes.append(b[\"bbox_2d\"]); labels.append(1)\n",
    "    return boxes, labels\n",
    "\n",
    "def img_path_for_sd(sd_token: str) -> Path:\n",
    "    return DATAROOT / sd_rows[sd_token][\"filename\"]\n",
    "\n",
    "val_scores_v, val_labels = [], []\n",
    "for g in val_graphs:\n",
    "    val_scores_v.append(vgae_node_scores(g))\n",
    "    val_labels.append(g.y.cpu().numpy())\n",
    "val_scores_v = np.concatenate(val_scores_v)\n",
    "val_labels   = np.concatenate(val_labels)\n",
    "fpr, tpr, thr = roc_curve(val_labels, val_scores_v)\n",
    "thr95_vgae = float(thr[np.argmin(np.abs(tpr - 0.95))])\n",
    "p_lo, p_hi = np.percentile(val_scores_v, [5, 95])\n",
    "p_lo, p_hi = float(p_lo), float(p_hi)\n",
    "\n",
    "def score_to_color(score, lo, hi):\n",
    "    if hi <= lo: hi = lo + 1e-6\n",
    "    t = float(np.clip((score - lo) / (hi - lo), 0.0, 1.0))\n",
    "    r = int(255 * t)\n",
    "    g = int(255 * (1 - 0.5*t))\n",
    "    b = 0\n",
    "    return (b, g, r)\n",
    "\n",
    "random.seed(0)\n",
    "scene_names = [s[\"name\"] for s in scenes]\n",
    "pick_scenes = scene_names[:2]\n",
    "frames = []\n",
    "for scn in pick_scenes:\n",
    "    s_tok = next(s for s in scenes if s[\"name\"]==scn)[\"first_sample_token\"]\n",
    "    cnt = 0\n",
    "    while s_tok and cnt < 10:\n",
    "        frames.append((scn, s_tok))\n",
    "        s_tok = samples[s_tok][\"next\"]\n",
    "        cnt += 1\n",
    "\n",
    "ALPHA = 1.0  \n",
    "\n",
    "csv_path = OUTDIR / \"predictions_index.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"scene\",\"sample_token\",\"sd_token\",\"channel\",\"det_idx\",\"x0\",\"y0\",\"x1\",\"y1\",\n",
    "                     \"score_vgae\",\"score_ctx\",\"score_fused\",\"pred_is_ood\",\"gt_label\"])\n",
    "    saved = 0\n",
    "\n",
    "    for scn, s_tok in frames:\n",
    "        ch2sd = sample_to_ch2sd.get(s_tok, {})\n",
    "        for ch in ALL_CAMS:\n",
    "            sd_tok = ch2sd.get(ch)\n",
    "            if not sd_tok: \n",
    "                continue\n",
    "            ip = img_path_for_sd(sd_tok)\n",
    "            if not ip.exists():\n",
    "                continue\n",
    "            # graph\n",
    "            g = next((gg for gg in val_graphs if gg.meta[\"sd_token\"]==sd_tok), None)\n",
    "            if g is None or g.x.size(0)==0:\n",
    "                continue\n",
    "\n",
    "            # scores\n",
    "            sv = vgae_node_scores(g)\n",
    "            sc = ctx_maha_scores_cuda(g).detach().cpu().numpy()\n",
    "            sF = ALPHA*sv + (1-ALPHA)*sc\n",
    "\n",
    "            # draw\n",
    "            boxes, labels = get_boxes_and_labels(sd_tok)\n",
    "            bgr = cv2.cvtColor(np.array(Image.open(ip).convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "            H, W = bgr.shape[:2]\n",
    "            for i, (bb, lab) in enumerate(zip(boxes, labels)):\n",
    "                x0,y0,x1,y1 = map(int, bb)\n",
    "                x0 = max(0, min(x0, W-1)); y0 = max(0, min(y0, H-1))\n",
    "                x1 = max(x0+1, min(x1, W)); y1 = max(y0+1, min(y1, H))\n",
    "                col = score_to_color(sF[i], p_lo, p_hi)\n",
    "                cv2.rectangle(bgr, (x0,y0), (x1,y1), col, 2)\n",
    "                tag = f\"{sF[i]:.2f}{' *' if sF[i]>=thr95_vgae else ''}\"\n",
    "                cv2.rectangle(bgr, (x0, max(0,y0-18)), (x0+max(60,8*len(tag)), y0-2), (0,0,0), -1)\n",
    "                cv2.putText(bgr, tag, (x0+2, y0-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "                writer.writerow([scn, s_tok, sd_tok, ch, i, x0,y0,x1,y1,\n",
    "                                 float(sv[i]), float(sc[i]), float(sF[i]),\n",
    "                                 int(sF[i]>=thr95_vgae), int(lab)])\n",
    "                saved += 1\n",
    "\n",
    "            # footer + save\n",
    "            cv2.rectangle(bgr, (10, H-40), (10+900, H-10), (0,0,0), -1)\n",
    "            cv2.putText(bgr, f\"{scn} | {ch} | s_tok={s_tok[:8]} | sd_tok={sd_tok[:8]} | alpha={ALPHA}\",\n",
    "                        (16, H-16), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
    "            out_img = OUTDIR / f\"{scn}__{ch}__{s_tok[:8]}__{sd_tok[:8]}.png\"\n",
    "            cv2.imwrite(str(out_img), bgr)\n",
    "\n",
    "print(f\"Saved annotated PNGs + CSV to: {OUTDIR}\")\n",
    "print(f\"CSV index: {csv_path}\")\n",
    "# ===================== END QUICK TESTER (CUDA) ======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a32878-97f5-4f2a-b1a5-e57340ac8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from pathlib import Path\n",
    "IMGDIR = Path(\"/data/Asad/NuScenesMiniNovel/test_vis\")\n",
    "for p in sorted(IMGDIR.glob(\"*.png\"))[:6]:\n",
    "    display(Image(filename=str(p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca83b8a-5967-47f4-9085-8116197c9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "csv_path = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/test_results/predictions_index.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# overall\n",
    "tp = ((df.pred_is_ood==1) & (df.gt_label==1)).sum()\n",
    "fp = ((df.pred_is_ood==1) & (df.gt_label==0)).sum()\n",
    "tn = ((df.pred_is_ood==0) & (df.gt_label==0)).sum()\n",
    "fn = ((df.pred_is_ood==0) & (df.gt_label==1)).sum()\n",
    "prec = tp / max(tp+fp,1); rec = tp / max(tp+fn,1); f1 = 2*prec*rec / max(prec+rec,1e-9)\n",
    "print(f\"Confusion @thr95  TP:{tp} FP:{fp} TN:{tn} FN:{fn}  |  Precision:{prec:.3f} Recall:{rec:.3f} F1:{f1:.3f}\")\n",
    "\n",
    "# per-camera\n",
    "print(\"\\nPer-camera:\")\n",
    "print(df.groupby(\"channel\")[[\"pred_is_ood\",\"gt_label\"]]\n",
    "      .apply(lambda g: pd.Series({\n",
    "          \"TP\": int(((g.pred_is_ood==1)&(g.gt_label==1)).sum()),\n",
    "          \"FP\": int(((g.pred_is_ood==1)&(g.gt_label==0)).sum()),\n",
    "          \"TN\": int(((g.pred_is_ood==0)&(g.gt_label==0)).sum()),\n",
    "          \"FN\": int(((g.pred_is_ood==0)&(g.gt_label==1)).sum()),\n",
    "      })))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31915f9f-a2ef-409e-a4ff-6156229bf2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.00  AUROC=0.9707  FPR@95=0.0805\n",
      "alpha=0.10  AUROC=0.9715  FPR@95=0.0774\n",
      "alpha=0.20  AUROC=0.9722  FPR@95=0.0750\n",
      "alpha=0.30  AUROC=0.9731  FPR@95=0.0744\n",
      "alpha=0.40  AUROC=0.9740  FPR@95=0.0714\n",
      "alpha=0.50  AUROC=0.9754  FPR@95=0.0665\n",
      "alpha=0.60  AUROC=0.9771  FPR@95=0.0569\n",
      "alpha=0.70  AUROC=0.9791  FPR@95=0.0520\n",
      "alpha=0.80  AUROC=0.9814  FPR@95=0.0448\n",
      "alpha=0.90  AUROC=0.9847  FPR@95=0.0472\n",
      "alpha=1.00  AUROC=0.9841  FPR@95=0.0665\n",
      "\n",
      "Best: (0.984690591885721, 0.9, 0.047186932849364795)\n",
      "alpha=0.00  AUROC=0.9707  FPR@95=0.0805\n",
      "alpha=0.10  AUROC=0.9715  FPR@95=0.0774\n",
      "alpha=0.20  AUROC=0.9722  FPR@95=0.0750\n",
      "alpha=0.30  AUROC=0.9731  FPR@95=0.0744\n",
      "alpha=0.40  AUROC=0.9740  FPR@95=0.0714\n",
      "alpha=0.50  AUROC=0.9754  FPR@95=0.0665\n",
      "alpha=0.60  AUROC=0.9771  FPR@95=0.0569\n",
      "alpha=0.70  AUROC=0.9791  FPR@95=0.0520\n",
      "alpha=0.80  AUROC=0.9814  FPR@95=0.0448\n",
      "alpha=0.90  AUROC=0.9847  FPR@95=0.0472\n",
      "alpha=1.00  AUROC=0.9841  FPR@95=0.0665\n",
      "\n",
      "Selected alpha (validation): 0.90\n",
      "Best validation AUROC: 0.9847  |  FPR@95: 0.0472\n",
      "\n",
      "Saved: /data/Asad/NuScenesMiniNovel/vis_results/test_results/predictions_index_with_fused.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "csv_path = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/test_results/predictions_index.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# recompute fused with different alpha, measure AUROC quickly\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "best = None\n",
    "for a in np.linspace(0,1,11):\n",
    "    s = a*df[\"score_vgae\"].values + (1-a)*df[\"score_ctx\"].values\n",
    "    auc = roc_auc_score(df[\"gt_label\"].values, s)\n",
    "    fpr, tpr, thr = roc_curve(df[\"gt_label\"].values, s)\n",
    "    i = np.argmin(np.abs(tpr-0.95))\n",
    "    fpr95 = fpr[i]\n",
    "    best = max(best or (-1,None,None), (auc, a, fpr95), key=lambda x: x[0])\n",
    "    print(f\"alpha={a:.2f}  AUROC={auc:.4f}  FPR@95={fpr95:.4f}\")\n",
    "print(\"\\nBest:\", best)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "csv_path = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/test_results/predictions_index.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ---- 1) Tune alpha on validation (grid search) ----\n",
    "best_auc, best_alpha, best_fpr95 = -1, None, None\n",
    "\n",
    "for a in np.linspace(0, 1, 11):  # {0.0, 0.1, ..., 1.0}\n",
    "    s = a * df[\"score_vgae\"].values + (1 - a) * df[\"score_ctx\"].values\n",
    "\n",
    "    auc = roc_auc_score(df[\"gt_label\"].values, s)\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(df[\"gt_label\"].values, s)\n",
    "    i = np.argmin(np.abs(tpr - 0.95))\n",
    "    fpr95 = fpr[i]\n",
    "\n",
    "    print(f\"alpha={a:.2f}  AUROC={auc:.4f}  FPR@95={fpr95:.4f}\")\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc, best_alpha, best_fpr95 = auc, a, fpr95\n",
    "\n",
    "print(f\"\\nSelected alpha (validation): {best_alpha:.2f}\")\n",
    "print(f\"Best validation AUROC: {best_auc:.4f}  |  FPR@95: {best_fpr95:.4f}\")\n",
    "\n",
    "# ---- 2) Fix alpha and store fused score (this is what paper claims) ----\n",
    "ALPHA = best_alpha\n",
    "df[\"score_fused\"] = ALPHA * df[\"score_vgae\"].values + (1 - ALPHA) * df[\"score_ctx\"].values\n",
    "\n",
    "# optional: save for downstream plotting/reporting\n",
    "out_path = csv_path.parent / \"predictions_index_with_fused.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"\\nSaved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8063f-5258-48c4-8ba9-03f995023a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "out = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/test_results/index.html\")\n",
    "imgs = sorted(Path(\"/data/Asad/NuScenesMiniNovel/vis_results/test_results\").glob(\"*.png\"))\n",
    "with open(out, \"w\") as f:\n",
    "    f.write(\"<html><body style='font-family: sans-serif'>\\n<h2>OOD Prediction Gallery</h2>\\n\")\n",
    "    for p in imgs:\n",
    "        f.write(f\"<div><img src='{p.name}' style='max-width: 100%;'><p>{p.name}</p></div><hr/>\\n\")\n",
    "    f.write(\"</body></html>\\n\")\n",
    "print(\"Gallery:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bc5e1-5ea4-467d-b7e2-3df5c4b32fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle, numpy as np, os\n",
    "save_dir = \"data/Asad/NuScenesMiniNovel/vis_results/test_results/checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "torch.save(model_vgae.state_dict(), f\"{save_dir}/vgae.pt\")\n",
    "\n",
    "# context-maha CUDA stats (save CPU copies)\n",
    "stats = {\n",
    "    \"mu\":      (maha_mu_t.detach().cpu().numpy()),\n",
    "    \"sd\":      (maha_sd_t.detach().cpu().numpy()),\n",
    "    \"mu_loc\":  (maha_mu_loc_t.detach().cpu().numpy()),\n",
    "    \"prec\":    (maha_prec_t.detach().cpu().numpy()),\n",
    "}\n",
    "with open(f\"{save_dir}/ctx_maha.pkl\", \"wb\") as f: pickle.dump(stats, f)\n",
    "print(\"Saved:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a8422-1742-45ea-a00b-26686626b73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8000621-7061-43c2-b066-d39abcd8949f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c971d9d7-0aa1-4f52-bc02-67ef70554039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.13.0+cu117 | CUDA: True | Device: cuda\n",
      "Loaded OOD frames: 500\n",
      "Model             AP@0.5    P@0.5     R@0.5     OOD-FP    AUROC     FPR@95    N       Time(s)   CamAvg(AUROC/FPR) \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "FasterRCNN_R50    0.174     0.421     0.262     0.040     0.7574    0.4115    500     15.2      0.774/0.393       \n",
      "   CAM_FRONT: AP=0.234, P=0.446, R=0.326, OOD-FP=0.03220654777748203, AUROC=0.7455914500850134, FPR@95=0.4972067039106145, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.171, P=0.472, R=0.250, OOD-FP=0.03484486873508353, AUROC=0.800706669972725, FPR@95=0.40825688073394495, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.192, P=0.372, R=0.292, OOD-FP=0.04242081447963801, AUROC=0.7565264358158794, FPR@95=0.4150513112884835, N=87\n",
      "   CAM_BACK: AP=0.174, P=0.463, R=0.237, OOD-FP=0.03866745984533016, AUROC=0.761265484714172, FPR@95=0.3786793953858393, N=82\n",
      "   CAM_BACK_LEFT: AP=0.114, P=0.324, R=0.235, OOD-FP=0.061744112030553785, AUROC=0.8225081699346405, FPR@95=0.3088235294117647, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.159, P=0.415, R=0.249, OOD-FP=0.03760071620411817, AUROC=0.7596895787139689, FPR@95=0.3472727272727273, N=84\n",
      "FasterRCNN_MBV3   0.149     0.393     0.233     0.043     0.7856    0.3852    500     9.8       0.795/0.376       \n",
      "   CAM_FRONT: AP=0.185, P=0.383, R=0.279, OOD-FP=0.02847640823113698, AUROC=0.7776536312849163, FPR@95=0.4424581005586592, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.169, P=0.414, R=0.282, OOD-FP=0.050983899821109124, AUROC=0.8278576741879494, FPR@95=0.4013761467889908, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.189, P=0.433, R=0.269, OOD-FP=0.038221528861154444, AUROC=0.7639280641741184, FPR@95=0.43899657924743446, N=87\n",
      "   CAM_BACK: AP=0.128, P=0.351, R=0.197, OOD-FP=0.04248985115020298, AUROC=0.7947778156608705, FPR@95=0.3229912490055688, N=82\n",
      "   CAM_BACK_LEFT: AP=0.119, P=0.338, R=0.221, OOD-FP=0.09241379310344827, AUROC=0.8473243464052288, FPR@95=0.30392156862745096, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.133, P=0.378, R=0.222, OOD-FP=0.04204081632653061, AUROC=0.7603325942350332, FPR@95=0.3490909090909091, N=84\n",
      "RetinaNet_R50     0.171     0.397     0.265     0.043     0.7355    0.5011    500     18.2      0.744/0.485       \n",
      "   CAM_FRONT: AP=0.231, P=0.461, R=0.323, OOD-FP=0.02855773181755924, AUROC=0.7521253339810542, FPR@95=0.576536312849162, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.176, P=0.408, R=0.259, OOD-FP=0.04535037993004463, AUROC=0.7606000495908753, FPR@95=0.5068807339449541, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.196, P=0.399, R=0.283, OOD-FP=0.048903071400079774, AUROC=0.6613054872071856, FPR@95=0.5473204104903079, N=87\n",
      "   CAM_BACK: AP=0.172, P=0.405, R=0.249, OOD-FP=0.04389859639071899, AUROC=0.7688515740425048, FPR@95=0.45505171042163883, N=82\n",
      "   CAM_BACK_LEFT: AP=0.100, P=0.339, R=0.201, OOD-FP=0.06673356748620171, AUROC=0.7588848039215687, FPR@95=0.4019607843137255, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.151, P=0.442, R=0.220, OOD-FP=0.0370939645455477, AUROC=0.7630155210643016, FPR@95=0.42363636363636364, N=84\n",
      "SSDLite_MBV3      0.076     0.271     0.109     0.010     0.7537    0.3610    500     18.4      0.755/0.359       \n",
      "   CAM_FRONT: AP=0.062, P=0.222, R=0.084, OOD-FP=0.007283763277693475, AUROC=0.7587563760019431, FPR@95=0.35083798882681566, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.084, P=0.271, R=0.131, OOD-FP=0.011606982184631996, AUROC=0.7712310934787998, FPR@95=0.3486238532110092, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.130, P=0.346, R=0.173, OOD-FP=0.010235673216655083, AUROC=0.7052351517333814, FPR@95=0.44013683010262256, N=87\n",
      "   CAM_BACK: AP=0.048, P=0.121, R=0.107, OOD-FP=0.012062181513255499, AUROC=0.7797761109216956, FPR@95=0.29912490055688146, N=82\n",
      "   CAM_BACK_LEFT: AP=0.103, P=0.342, R=0.123, OOD-FP=0.014170242490769385, AUROC=0.7774714052287581, FPR@95=0.3137254901960784, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.106, P=0.401, R=0.155, OOD-FP=0.0078119501724259275, AUROC=0.7356984478935699, FPR@95=0.4, N=84\n",
      "SSD300_VGG16      0.155     0.360     0.233     0.009     0.7478    0.4200    500     12.0      0.759/0.409       \n",
      "   CAM_FRONT: AP=0.195, P=0.396, R=0.272, OOD-FP=0.0050631106040077015, AUROC=0.7660675248967694, FPR@95=0.4491620111731844, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.172, P=0.366, R=0.243, OOD-FP=0.010912563088255355, AUROC=0.7542772129928093, FPR@95=0.4151376146788991, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.200, P=0.417, R=0.257, OOD-FP=0.009707112970711296, AUROC=0.7425433595390984, FPR@95=0.47206385404789053, N=87\n",
      "   CAM_BACK: AP=0.107, P=0.334, R=0.185, OOD-FP=0.009007678676904902, AUROC=0.7478690760313673, FPR@95=0.35799522673031026, N=82\n",
      "   CAM_BACK_LEFT: AP=0.155, P=0.386, R=0.216, OOD-FP=0.013710747456877488, AUROC=0.8071895424836601, FPR@95=0.3235294117647059, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.182, P=0.437, R=0.227, OOD-FP=0.008752983971808572, AUROC=0.7348115299334812, FPR@95=0.43454545454545457, N=84\n",
      "YOLOv8n           0.140     0.349     0.231     0.056     0.8203    0.3164    500     6.4       0.832/0.305       \n",
      "   CAM_FRONT: AP=0.175, P=0.376, R=0.273, OOD-FP=0.03331482509716824, AUROC=0.8040077726499879, FPR@95=0.3653631284916201, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.154, P=0.351, R=0.291, OOD-FP=0.060158910329171394, AUROC=0.8480969501611704, FPR@95=0.33256880733944955, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.169, P=0.334, R=0.254, OOD-FP=0.056708160442600276, AUROC=0.7996559243033468, FPR@95=0.3443557582668187, N=87\n",
      "   CAM_BACK: AP=0.115, P=0.341, R=0.193, OOD-FP=0.046365914786967416, AUROC=0.8209739743152631, FPR@95=0.26730310262529833, N=82\n",
      "   CAM_BACK_LEFT: AP=0.117, P=0.328, R=0.201, OOD-FP=0.15254237288135594, AUROC=0.8788807189542484, FPR@95=0.22549019607843138, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.149, P=0.476, R=0.213, OOD-FP=0.06650544135429262, AUROC=0.8418625277161862, FPR@95=0.2927272727272727, N=84\n",
      "YOLOv8s           0.155     0.416     0.245     0.048     0.7954    0.3475    500     6.5       0.810/0.337       \n",
      "   CAM_FRONT: AP=0.204, P=0.472, R=0.270, OOD-FP=0.030103995621237, AUROC=0.7845275686179257, FPR@95=0.41564245810055866, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.147, P=0.401, R=0.273, OOD-FP=0.05737704918032787, AUROC=0.8166377386560874, FPR@95=0.3646788990825688, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.200, P=0.402, R=0.308, OOD-FP=0.04732080723729993, AUROC=0.782972253895857, FPR@95=0.38426453819840367, N=87\n",
      "   CAM_BACK: AP=0.131, P=0.406, R=0.212, OOD-FP=0.039634146341463415, AUROC=0.8029605637004206, FPR@95=0.294351630867144, N=82\n",
      "   CAM_BACK_LEFT: AP=0.105, P=0.295, R=0.240, OOD-FP=0.11560693641618497, AUROC=0.8555964052287581, FPR@95=0.25980392156862747, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.154, P=0.517, R=0.222, OOD-FP=0.05481120584652863, AUROC=0.8175609756097562, FPR@95=0.3018181818181818, N=84\n",
      "\n",
      "Saved CSV → /data/Asad/NuScenesMiniNovel/vis_results/ood_report.csv\n",
      "\n",
      "Notes:\n",
      "• AP@0.5/P/R use only ID GT (single-class, greedy 1–1 matches at IoU≥0.5).\n",
      "• OOD-FP is the fraction of detections that overlap any OOD GT (IoU≥0.5).\n",
      "• AUROC/FPR@95 use per-GT scores: matched → 1−confidence (MSP for YOLO, conf for others),\n",
      "  missed ID → 0.0, missed OOD → 1.0. Reported overall and per camera, plus camera-average.\n"
     ]
    }
   ],
   "source": [
    "# === nuScenes-OOD: multi-model OOD evaluation with per-camera AUROC and CSV export ===\n",
    "# - Dataset layout: <NUSCENES_OOD_ROOT>/<JSONDIR_NAME> with:\n",
    "#     sample.json, sample_data.json, scene.json, calibrated_sensor.json, sensor.json,\n",
    "#     detection_id.json, detection_novel.json (both contain {\"results\": {sd_token: [{\"bbox_2d\":[x1,y1,x2,y2]}, ...]}})\n",
    "# - KPIs (overall + per camera): AP@0.5, P@0.5, R@0.5, OOD-FP, AUROC, FPR@95, N, Time(s)\n",
    "# - Models (try/except): FasterRCNN_R50, FasterRCNN_MBV3, RetinaNet_R50, SSDLite_MBV3, SSD300_VGG16, YOLOv8n/s (optional)\n",
    "\n",
    "import os, json, time, math, random, warnings, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from PIL import Image, ImageFile\n",
    "import torchvision.transforms as T\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "NUSCENES_OOD_ROOT  = Path(\"/data/Asad/NuScenesMiniNovel\")   \n",
    "JSONDIR_NAME       = \"v1.0-mini\"                            \n",
    "MAX_IMAGES         = 500                                     \n",
    "IOU_MATCH          = 0.5                                     \n",
    "SCORE_THRESH       = 0.05                                    \n",
    "CSV_DIR            = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/\")\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_OUT            = CSV_DIR / \"ood_report.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch: {torch.__version__} | CUDA: {torch.cuda.is_available()} | Device: {device}\")\n",
    "\n",
    "# ------------------ DATASET ------------------\n",
    "def load_json(p): \n",
    "    with open(p,\"r\") as f: \n",
    "        return json.load(f)\n",
    "\n",
    "def load_frame_index(dataroot: Path):\n",
    "    jsondir = dataroot / JSONDIR_NAME\n",
    "    sd_rows = {d[\"token\"]: d for d in load_json(jsondir / \"sample_data.json\")}\n",
    "    samples = {s[\"token\"]: s for s in load_json(jsondir / \"sample.json\")}\n",
    "    scenes  = load_json(jsondir / \"scene.json\")\n",
    "    calib_by  = {c[\"token\"]: c for c in load_json(jsondir / \"calibrated_sensor.json\")}\n",
    "    sensor_by = {s[\"token\"]: s for s in load_json(jsondir / \"sensor.json\")}\n",
    "    id_path   = jsondir / \"detection_id.json\"\n",
    "    ood_path  = jsondir / \"detection_novel.json\"\n",
    "    gt_id  = load_json(id_path)[\"results\"] if id_path.exists() else {}\n",
    "    gt_ood = load_json(ood_path)[\"results\"] if ood_path.exists() else {}\n",
    "\n",
    "    def channel_of_sd_row(sd_row):\n",
    "        calib  = calib_by[sd_row[\"calibrated_sensor_token\"]]\n",
    "        sensor = sensor_by[calib[\"sensor_token\"]]\n",
    "        return sensor[\"channel\"]\n",
    "\n",
    "    sample_to_ch2sd = {}\n",
    "    for sd in sd_rows.values():\n",
    "        ch = channel_of_sd_row(sd)\n",
    "        if not ch.startswith(\"CAM_\"): \n",
    "            continue\n",
    "        st = sd[\"sample_token\"]\n",
    "        sample_to_ch2sd.setdefault(st, {})[ch] = sd[\"token\"]\n",
    "\n",
    "    frames = []\n",
    "    ALL_CAMS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "    for sc in scenes:\n",
    "        s_tok = sc[\"first_sample_token\"]\n",
    "        while s_tok:\n",
    "            sample = samples[s_tok]\n",
    "            ch2sd  = sample_to_ch2sd.get(s_tok, {})\n",
    "            for ch in ALL_CAMS:\n",
    "                sd_tok = ch2sd.get(ch)\n",
    "                if not sd_tok: \n",
    "                    continue\n",
    "                fn = sd_rows[sd_tok][\"filename\"]\n",
    "                img_path = dataroot / fn\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                id_boxes  = [b[\"bbox_2d\"] for b in gt_id.get(sd_tok, [])]\n",
    "                ood_boxes = [b[\"bbox_2d\"] for b in gt_ood.get(sd_tok, [])]\n",
    "                frames.append((\n",
    "                    str(img_path),            # 0\n",
    "                    sd_tok,                   # 1\n",
    "                    ch,                       # 2 (camera)\n",
    "                    np.array(id_boxes,  dtype=np.float32),  # 3\n",
    "                    np.array(ood_boxes, dtype=np.float32)   # 4\n",
    "                ))\n",
    "            s_tok = sample[\"next\"]\n",
    "    return frames\n",
    "\n",
    "def pick_subset(frames, k=None, seed=13):\n",
    "    if (k is None) or (k>=len(frames)): \n",
    "        return frames\n",
    "    rng = random.Random(seed)\n",
    "    idx = list(range(len(frames)))\n",
    "    rng.shuffle(idx)\n",
    "    return [frames[i] for i in idx[:k]]\n",
    "\n",
    "# ------------------ MODELS ------------------\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "# Torchvision detectors (COCO-trained)\n",
    "def load_frcnn_r50():\n",
    "    m = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_frcnn_mbv3():\n",
    "    m = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_retinanet_r50():\n",
    "    m = torchvision.models.detection.retinanet_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssdlite_mbv3():\n",
    "    m = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssd300_vgg16():\n",
    "    m = torchvision.models.detection.ssd300_vgg16(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def infer_torchvision_detector(model, pil_img):\n",
    "    x = to_tensor(pil_img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model([x])[0]\n",
    "    boxes  = out[\"boxes\"].detach().float().cpu().numpy()\n",
    "    scores = out[\"scores\"].detach().float().cpu().numpy()\n",
    "    return boxes.astype(np.float32), scores.astype(np.float32), \"conf\"\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    _has_yolo = True\n",
    "except Exception:\n",
    "    _has_yolo = False\n",
    "\n",
    "def load_yolov8n():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "def load_yolov8s():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8s.pt\").to(device)\n",
    "\n",
    "def infer_yolov8(model, pil_img):\n",
    "    im = np.array(pil_img.convert(\"RGB\"))\n",
    "    ydev = 0 if device.type == \"cuda\" else \"cpu\"\n",
    "    r = model.predict(source=im, verbose=False, conf=0.001, device=ydev)[0]\n",
    "    if r is None or r.boxes is None or len(r.boxes) == 0:\n",
    "        return np.zeros((0,4), dtype=np.float32), np.zeros((0,), dtype=np.float32), \"conf\"\n",
    "    xyxy = r.boxes.xyxy.cpu().numpy().astype(np.float32)\n",
    "    if hasattr(r, \"probs\") and r.probs is not None:\n",
    "        scores = r.probs.data.cpu().numpy().max(axis=1).astype(np.float32)\n",
    "        return xyxy, scores, \"MSP\"\n",
    "    else:\n",
    "        scores = r.boxes.conf.cpu().numpy().astype(np.float32)\n",
    "        return xyxy, scores, \"conf\"\n",
    "\n",
    "# ------------------ METRICS ------------------\n",
    "def iou_xyxy(a, b):\n",
    "    Na, Nb = a.shape[0], b.shape[0]\n",
    "    if Na==0 or Nb==0:\n",
    "        return np.zeros((Na,Nb), dtype=np.float32)\n",
    "    ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n",
    "    bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
    "    inter_x1 = np.maximum(ax1[:,None], bx1[None,:])\n",
    "    inter_y1 = np.maximum(ay1[:,None], by1[None,:])\n",
    "    inter_x2 = np.minimum(ax2[:,None], bx2[None,:])\n",
    "    inter_y2 = np.minimum(ay2[:,None], by2[None,:])\n",
    "    inter_w = np.clip(inter_x2 - inter_x1, 0, None)\n",
    "    inter_h = np.clip(inter_y2 - inter_y1, 0, None)\n",
    "    inter = inter_w * inter_h\n",
    "    area_a = (ax2-ax1)*(ay2-ay1)\n",
    "    area_b = (bx2-bx1)*(by2-by1)\n",
    "    union = area_a[:,None] + area_b[None,:] - inter\n",
    "    return np.where(union>0, inter/union, 0.0)\n",
    "\n",
    "def ap50_single_class(all_scores, all_tp, total_gt_pos):\n",
    "    if len(all_scores)==0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    order = np.argsort(-np.array(all_scores))\n",
    "    tp = np.array(all_tp)[order].astype(np.float32)\n",
    "    fp = 1.0 - tp\n",
    "    cum_tp = np.cumsum(tp)\n",
    "    cum_fp = np.cumsum(fp)\n",
    "    recall = cum_tp / max(1, total_gt_pos)\n",
    "    precision = cum_tp / np.maximum(1, (cum_tp+cum_fp))\n",
    "    # all-points interpolation\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "    for i in range(mpre.size-1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = float(np.sum((mrec[idx+1]-mrec[idx]) * mpre[idx+1]))\n",
    "    best_i = int(np.argmax(2*precision*recall/(precision+recall+1e-9)))\n",
    "    return ap, float(precision[best_i]), float(recall[best_i])\n",
    "\n",
    "# ------------------ EVAL ------------------\n",
    "ALL_CAMS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "def evaluate_dataset(frames, model_name, model, infer_fn, compute_ood_roc=True):\n",
    "    # global accumulators\n",
    "    all_scores, all_tp = [], []\n",
    "    total_id_gt = 0\n",
    "    det_ood_hits = 0\n",
    "    total_dets = 0\n",
    "    roc_scores_glob, roc_labels_glob = [], []\n",
    "\n",
    "    # per-camera accumulators\n",
    "    per_cam = {\n",
    "        cam: {\n",
    "            \"scores\": [], \"tp\": [], \"id_gt\": 0,\n",
    "            \"OOD_FP_hits\": 0, \"detections\": 0,\n",
    "            \"roc_scores\": [], \"roc_labels\": [], \"N\": 0\n",
    "        } for cam in ALL_CAMS\n",
    "    }\n",
    "\n",
    "    have_any_ood = any(len(fr[4])>0 for fr in frames)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for (img_path, sd_tok, cam, id_boxes, ood_boxes) in frames:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        boxes, conf_like, score_type = infer_fn(model, img)\n",
    "        keep = conf_like >= SCORE_THRESH if conf_like is not None and len(conf_like)>0 else np.array([], dtype=bool)\n",
    "        boxes = boxes[keep] if boxes.size else boxes\n",
    "        conf_like = conf_like[keep] if keep.size else conf_like\n",
    "\n",
    "        # === ID AP accounting (global + per-cam) ===\n",
    "        total_id_gt += int(id_boxes.shape[0])\n",
    "        used = np.zeros((id_boxes.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and id_boxes.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, id_boxes)\n",
    "            order = np.argsort(-conf_like)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not used[j]:\n",
    "                    all_scores.append(float(conf_like[di])); all_tp.append(1); used[j]=True\n",
    "                else:\n",
    "                    all_scores.append(float(conf_like[di])); all_tp.append(0)\n",
    "        else:\n",
    "            for s in (conf_like if conf_like is not None else []):\n",
    "                all_scores.append(float(s)); all_tp.append(0)\n",
    "\n",
    "        # per-camera AP accum\n",
    "        cam_used = np.zeros((id_boxes.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and id_boxes.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, id_boxes)\n",
    "            order = np.argsort(-conf_like)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not cam_used[j]:\n",
    "                    per_cam[cam][\"scores\"].append(float(conf_like[di]))\n",
    "                    per_cam[cam][\"tp\"].append(1)\n",
    "                    cam_used[j]=True\n",
    "                else:\n",
    "                    per_cam[cam][\"scores\"].append(float(conf_like[di]))\n",
    "                    per_cam[cam][\"tp\"].append(0)\n",
    "        else:\n",
    "            for s in (conf_like if conf_like is not None else []):\n",
    "                per_cam[cam][\"scores\"].append(float(s))\n",
    "                per_cam[cam][\"tp\"].append(0)\n",
    "        per_cam[cam][\"id_gt\"] += int(id_boxes.shape[0])\n",
    "\n",
    "        # === OOD-FP rate: any det overlapping any OOD GT ===\n",
    "        total_dets += int(boxes.shape[0])\n",
    "        per_cam[cam][\"detections\"] += int(boxes.shape[0])\n",
    "        if boxes.shape[0] > 0 and ood_boxes.shape[0] > 0:\n",
    "            IoU_ood = iou_xyxy(boxes, ood_boxes)\n",
    "            hits = (IoU_ood.max(axis=1) >= IOU_MATCH).sum()\n",
    "            det_ood_hits += int(hits)\n",
    "            per_cam[cam][\"OOD_FP_hits\"] += int(hits)\n",
    "\n",
    "        # === AUROC/FPR@95 per-GT (global + per-cam), only if OOD exists ===\n",
    "        if compute_ood_roc and have_any_ood:\n",
    "            if boxes.shape[0] > 0:\n",
    "                IoU_id  = iou_xyxy(id_boxes,  boxes) if id_boxes.shape[0]>0 else np.zeros((0, boxes.shape[0]))\n",
    "                IoU_ood = iou_xyxy(ood_boxes, boxes) if ood_boxes.shape[0]>0 else np.zeros((0, boxes.shape[0]))\n",
    "            else:\n",
    "                IoU_id  = np.zeros((id_boxes.shape[0],  0))\n",
    "                IoU_ood = np.zeros((ood_boxes.shape[0], 0))\n",
    "            det_ood_score = (1.0 - conf_like) if (conf_like is not None and len(conf_like)>0) else np.array([])\n",
    "\n",
    "            # ID GT -> label 0\n",
    "            for gi in range(id_boxes.shape[0]):\n",
    "                if IoU_id.shape[1] > 0 and IoU_id[gi].max() >= IOU_MATCH:\n",
    "                    di = int(IoU_id[gi].argmax())\n",
    "                    s = float(det_ood_score[di]) if det_ood_score.size>0 else 0.0\n",
    "                    roc_scores_glob.append(s); roc_labels_glob.append(0)\n",
    "                    per_cam[cam][\"roc_scores\"].append(s); per_cam[cam][\"roc_labels\"].append(0)\n",
    "                else:\n",
    "                    roc_scores_glob.append(0.0); roc_labels_glob.append(0)\n",
    "                    per_cam[cam][\"roc_scores\"].append(0.0); per_cam[cam][\"roc_labels\"].append(0)\n",
    "            # OOD GT -> label 1\n",
    "            for gi in range(ood_boxes.shape[0]):\n",
    "                if IoU_ood.shape[1] > 0 and IoU_ood[gi].max() >= IOU_MATCH:\n",
    "                    di = int(IoU_ood[gi].argmax())\n",
    "                    s = float(det_ood_score[di]) if det_ood_score.size>0 else 1.0\n",
    "                    roc_scores_glob.append(s); roc_labels_glob.append(1)\n",
    "                    per_cam[cam][\"roc_scores\"].append(s); per_cam[cam][\"roc_labels\"].append(1)\n",
    "                else:\n",
    "                    roc_scores_glob.append(1.0); roc_labels_glob.append(1)\n",
    "                    per_cam[cam][\"roc_scores\"].append(1.0); per_cam[cam][\"roc_labels\"].append(1)\n",
    "\n",
    "        per_cam[cam][\"N\"] += 1\n",
    "\n",
    "    # Aggregate global AP on ID\n",
    "    ap, p, r = ap50_single_class(all_scores, all_tp, total_id_gt)\n",
    "    ood_fp_rate = det_ood_hits / max(1, total_dets)\n",
    "    results = {\n",
    "        \"AP50\": ap, \"P@0.5\": p, \"R@0.5\": r,\n",
    "        \"OOD_FP_rate\": ood_fp_rate,\n",
    "        \"N_imgs\": len(frames),\n",
    "        \"time_s\": time.time()-t0\n",
    "    }\n",
    "\n",
    "    # Global AUROC/FPR@95\n",
    "    if compute_ood_roc and len(set(roc_labels_glob))>1:\n",
    "        from sklearn.metrics import roc_auc_score, roc_curve\n",
    "        scores = np.array(roc_scores_glob); labels = np.array(roc_labels_glob)\n",
    "        auroc = float(roc_auc_score(labels, scores))\n",
    "        fpr, tpr, thr = roc_curve(labels, scores)\n",
    "        i95 = int(np.argmin(np.abs(tpr - 0.95)))\n",
    "        results[\"AUROC\"]  = auroc\n",
    "        results[\"FPR@95\"] = float(fpr[i95])\n",
    "\n",
    "    # Per-camera AP + AUROC/FPR@95 + OOD-FP\n",
    "    results[\"per_camera\"] = {}\n",
    "    for cam in ALL_CAMS:\n",
    "        A = per_cam[cam]\n",
    "        ap_c, p_c, r_c = ap50_single_class(A[\"scores\"], A[\"tp\"], A[\"id_gt\"])\n",
    "        oodfp_c = (A[\"OOD_FP_hits\"] / max(1, A[\"detections\"])) if A[\"detections\"]>0 else float('nan')\n",
    "        cam_res = {\"AP50\": ap_c, \"P@0.5\": p_c, \"R@0.5\": r_c, \"OOD_FP\": oodfp_c, \"N\": A[\"N\"]}\n",
    "        # per-cam AUROC\n",
    "        if compute_ood_roc and len(A[\"roc_labels\"])>0 and len(set(A[\"roc_labels\"]))>1:\n",
    "            from sklearn.metrics import roc_auc_score, roc_curve\n",
    "            sc = np.array(A[\"roc_scores\"]); lb = np.array(A[\"roc_labels\"])\n",
    "            try:\n",
    "                au = float(roc_auc_score(lb, sc))\n",
    "                fpr, tpr, thr = roc_curve(lb, sc)\n",
    "                i95 = int(np.argmin(np.abs(tpr - 0.95)))\n",
    "                cam_res[\"AUROC\"]  = au\n",
    "                cam_res[\"FPR@95\"] = float(fpr[i95])\n",
    "            except Exception:\n",
    "                pass\n",
    "        results[\"per_camera\"][cam] = cam_res\n",
    "\n",
    "    # Camera-average (macro) for printing/CSV convenience\n",
    "    def cam_mean(key):\n",
    "        vals = [results[\"per_camera\"][c].get(key, float('nan')) for c in ALL_CAMS]\n",
    "        vals = [v for v in vals if not (isinstance(v,float) and math.isnan(v))]\n",
    "        return float(np.mean(vals)) if len(vals)>0 else float('nan')\n",
    "    results[\"camera_avg\"] = {\n",
    "        \"AP50\": cam_mean(\"AP50\"),\n",
    "        \"P@0.5\": cam_mean(\"P@0.5\"),\n",
    "        \"R@0.5\": cam_mean(\"R@0.5\"),\n",
    "        \"OOD_FP\": cam_mean(\"OOD_FP\"),\n",
    "        \"AUROC\": cam_mean(\"AUROC\"),\n",
    "        \"FPR@95\": cam_mean(\"FPR@95\")\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def pretty_row(cols, widths):\n",
    "    return \"  \".join(str(c).ljust(w) for c,w in zip(cols, widths))\n",
    "\n",
    "# Load frames\n",
    "frames_all = load_frame_index(NUSCENES_OOD_ROOT)\n",
    "frames = pick_subset(frames_all, MAX_IMAGES)\n",
    "print(f\"Loaded OOD frames: {len(frames)}\")\n",
    "\n",
    "# Build model list\n",
    "models = []\n",
    "try:   models.append((\"FasterRCNN_R50\", load_frcnn_r50(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_R50 not available: {e}\")\n",
    "try:   models.append((\"FasterRCNN_MBV3\", load_frcnn_mbv3(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_MBV3 not available: {e}\")\n",
    "try:   models.append((\"RetinaNet_R50\",  load_retinanet_r50(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"RetinaNet_R50 not available: {e}\")\n",
    "try:   models.append((\"SSDLite_MBV3\",   load_ssdlite_mbv3(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSDLite_MBV3 not available: {e}\")\n",
    "try:   models.append((\"SSD300_VGG16\",   load_ssd300_vgg16(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSD300_VGG16 not available: {e}\")\n",
    "\n",
    "try:\n",
    "    y8n = load_yolov8n()\n",
    "    if y8n is not None: models.append((\"YOLOv8n\", y8n, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8n not available: {e}\")\n",
    "try:\n",
    "    y8s = load_yolov8s()\n",
    "    if y8s is not None: models.append((\"YOLOv8s\", y8s, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8s not available: {e}\")\n",
    "\n",
    "# Print header\n",
    "hdr = [\"Model\",\"AP@0.5\",\"P@0.5\",\"R@0.5\",\"OOD-FP\",\"AUROC\",\"FPR@95\",\"N\",\"Time(s)\",\"CamAvg(AUROC/FPR)\"]\n",
    "w   = [16, 8, 8, 8, 8, 8, 8, 6, 8, 18]\n",
    "print(pretty_row(hdr, w))\n",
    "print(\"-\"*120)\n",
    "\n",
    "rows_for_csv = []\n",
    "for name, model, infer_fn in models:\n",
    "    res = evaluate_dataset(frames, name, model, infer_fn, compute_ood_roc=True)\n",
    "    auroc = res.get(\"AUROC\", float('nan'))\n",
    "    fpr95 = res.get(\"FPR@95\", float('nan'))\n",
    "    cam_avg = res[\"camera_avg\"]\n",
    "    cam_avg_str = f\"{cam_avg['AUROC']:.3f}/{cam_avg['FPR@95']:.3f}\" if not math.isnan(cam_avg[\"AUROC\"]) else \"—\"\n",
    "\n",
    "    print(pretty_row([\n",
    "        name,\n",
    "        f\"{res['AP50']:.3f}\",\n",
    "        f\"{res['P@0.5']:.3f}\",\n",
    "        f\"{res['R@0.5']:.3f}\",\n",
    "        f\"{res['OOD_FP_rate']:.3f}\",\n",
    "        f\"{auroc:.4f}\" if not math.isnan(auroc) else \"—\",\n",
    "        f\"{fpr95:.4f}\" if not math.isnan(fpr95) else \"—\",\n",
    "        res[\"N_imgs\"],\n",
    "        f\"{res['time_s']:.1f}\",\n",
    "        cam_avg_str\n",
    "    ], w))\n",
    "\n",
    "    # Per-camera print\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        out_auroc = c.get(\"AUROC\", float('nan'))\n",
    "        out_fpr95 = c.get(\"FPR@95\", float('nan'))\n",
    "        print(f\"   {cam}: AP={c['AP50']:.3f}, P={c['P@0.5']:.3f}, R={c['R@0.5']:.3f}, \"\n",
    "              f\"OOD-FP={c['OOD_FP'] if not isinstance(c['OOD_FP'],float) or not math.isnan(c['OOD_FP']) else '—'}, \"\n",
    "              f\"AUROC={out_auroc if not math.isnan(out_auroc) else '—'}, \"\n",
    "              f\"FPR@95={out_fpr95 if not math.isnan(out_fpr95) else '—'}, N={c['N']}\")\n",
    "\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"overall_AP50\": res[\"AP50\"], \"overall_P@0.5\": res[\"P@0.5\"], \"overall_R@0.5\": res[\"R@0.5\"],\n",
    "        \"overall_OOD_FP\": res[\"OOD_FP_rate\"], \"overall_AUROC\": auroc if not math.isnan(auroc) else \"\",\n",
    "        \"overall_FPR@95\": fpr95 if not math.isnan(fpr95) else \"\",\n",
    "        \"N\": res[\"N_imgs\"], \"time_s\": res[\"time_s\"],\n",
    "        \"camera_avg_AP50\": cam_avg[\"AP50\"], \"camera_avg_P@0.5\": cam_avg[\"P@0.5\"], \"camera_avg_R@0.5\": cam_avg[\"R@0.5\"],\n",
    "        \"camera_avg_OOD_FP\": cam_avg[\"OOD_FP\"], \"camera_avg_AUROC\": cam_avg[\"AUROC\"] if not math.isnan(cam_avg[\"AUROC\"]) else \"\",\n",
    "        \"camera_avg_FPR@95\": cam_avg[\"FPR@95\"] if not math.isnan(cam_avg[\"FPR@95\"]) else \"\"\n",
    "    }\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        row[f\"{cam}_AP50\"]   = c[\"AP50\"]\n",
    "        row[f\"{cam}_P@0.5\"]  = c[\"P@0.5\"]\n",
    "        row[f\"{cam}_R@0.5\"]  = c[\"R@0.5\"]\n",
    "        row[f\"{cam}_OOD_FP\"] = c[\"OOD_FP\"] if not (isinstance(c[\"OOD_FP\"], float) and math.isnan(c[\"OOD_FP\"])) else \"\"\n",
    "        row[f\"{cam}_AUROC\"]  = c.get(\"AUROC\",\"\")\n",
    "        row[f\"{cam}_FPR@95\"] = c.get(\"FPR@95\",\"\")\n",
    "        row[f\"{cam}_N\"]      = c[\"N\"]\n",
    "    rows_for_csv.append(row)\n",
    "\n",
    "# Save CSV\n",
    "if rows_for_csv:\n",
    "    fieldnames = list(rows_for_csv[0].keys())\n",
    "    with open(CSV_OUT, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in rows_for_csv:\n",
    "            writer.writerow(r)\n",
    "    print(f\"\\nSaved CSV → {CSV_OUT}\")\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"• AP@0.5/P/R use only ID GT (single-class, greedy 1–1 matches at IoU≥0.5).\")\n",
    "print(\"• OOD-FP is the fraction of detections that overlap any OOD GT (IoU≥0.5).\")\n",
    "print(\"• AUROC/FPR@95 use per-GT scores: matched → 1−confidence (MSP for YOLO, conf for others),\")\n",
    "print(\"  missed ID → 0.0, missed OOD → 1.0. Reported overall and per camera, plus camera-average.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a30adaa4-e4c2-4d3d-bf16-f9e045a52008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.13.0+cu117 | CUDA: True | Device: cuda\n",
      "Loaded OOD frames: 500\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['numpy>=1.23.5', 'tqdm>=4.66.3', 'setuptools>=70.0.0', 'urllib3>=2.5.0 ; python_version > \"3.8\"'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/asad/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0mYOLOv5 🚀 2025-11-7 Python-3.10.13 torch-1.13.0+cu117 CUDA:0 (NVIDIA RTX 6000 Ada Generation, 48647MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['numpy>=1.23.5', 'tqdm>=4.66.3', 'setuptools>=70.0.0', 'urllib3>=2.5.0 ; python_version > \"3.8\"'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/asad/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0mYOLOv5 🚀 2025-11-7 Python-3.10.13 torch-1.13.0+cu117 CUDA:0 (NVIDIA RTX 6000 Ada Generation, 48647MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model             AP@0.5    P@0.5     R@0.5     OOD-FP    AUROC     FPR@95    N       Time(s)   CamAvg(AUROC/FPR) \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "FasterRCNN_R50    0.174     0.421     0.262     0.040     0.7574    0.4115    500     15.3      0.774/0.393       \n",
      "   CAM_FRONT: AP=0.234, P=0.446, R=0.326, OOD-FP=0.03220654777748203, AUROC=0.7455914500850134, FPR@95=0.4972067039106145, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.171, P=0.472, R=0.250, OOD-FP=0.03484486873508353, AUROC=0.800706669972725, FPR@95=0.40825688073394495, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.192, P=0.372, R=0.292, OOD-FP=0.04242081447963801, AUROC=0.7565264358158794, FPR@95=0.4150513112884835, N=87\n",
      "   CAM_BACK: AP=0.174, P=0.463, R=0.237, OOD-FP=0.03866745984533016, AUROC=0.761265484714172, FPR@95=0.3786793953858393, N=82\n",
      "   CAM_BACK_LEFT: AP=0.114, P=0.324, R=0.235, OOD-FP=0.061744112030553785, AUROC=0.8225081699346405, FPR@95=0.3088235294117647, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.159, P=0.415, R=0.249, OOD-FP=0.03760071620411817, AUROC=0.7596895787139689, FPR@95=0.3472727272727273, N=84\n",
      "FasterRCNN_MBV3   0.149     0.393     0.233     0.043     0.7856    0.3852    500     10.0      0.795/0.376       \n",
      "   CAM_FRONT: AP=0.185, P=0.383, R=0.279, OOD-FP=0.02847640823113698, AUROC=0.7776536312849163, FPR@95=0.4424581005586592, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.169, P=0.414, R=0.282, OOD-FP=0.050983899821109124, AUROC=0.8278576741879494, FPR@95=0.4013761467889908, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.189, P=0.433, R=0.269, OOD-FP=0.038221528861154444, AUROC=0.7639280641741184, FPR@95=0.43899657924743446, N=87\n",
      "   CAM_BACK: AP=0.128, P=0.351, R=0.197, OOD-FP=0.04248985115020298, AUROC=0.7947778156608705, FPR@95=0.3229912490055688, N=82\n",
      "   CAM_BACK_LEFT: AP=0.119, P=0.338, R=0.221, OOD-FP=0.09241379310344827, AUROC=0.8473243464052288, FPR@95=0.30392156862745096, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.133, P=0.378, R=0.222, OOD-FP=0.04204081632653061, AUROC=0.7603325942350332, FPR@95=0.3490909090909091, N=84\n",
      "RetinaNet_R50     0.171     0.397     0.265     0.043     0.7355    0.5011    500     18.2      0.744/0.485       \n",
      "   CAM_FRONT: AP=0.231, P=0.461, R=0.323, OOD-FP=0.02855773181755924, AUROC=0.7521253339810542, FPR@95=0.576536312849162, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.176, P=0.408, R=0.259, OOD-FP=0.04535037993004463, AUROC=0.7606000495908753, FPR@95=0.5068807339449541, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.196, P=0.399, R=0.283, OOD-FP=0.048903071400079774, AUROC=0.6613054872071856, FPR@95=0.5473204104903079, N=87\n",
      "   CAM_BACK: AP=0.172, P=0.405, R=0.249, OOD-FP=0.04389859639071899, AUROC=0.7688515740425048, FPR@95=0.45505171042163883, N=82\n",
      "   CAM_BACK_LEFT: AP=0.100, P=0.339, R=0.201, OOD-FP=0.06673356748620171, AUROC=0.7588848039215687, FPR@95=0.4019607843137255, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.151, P=0.442, R=0.220, OOD-FP=0.0370939645455477, AUROC=0.7630155210643016, FPR@95=0.42363636363636364, N=84\n",
      "SSDLite_MBV3      0.076     0.271     0.109     0.010     0.7537    0.3610    500     18.4      0.755/0.359       \n",
      "   CAM_FRONT: AP=0.062, P=0.222, R=0.084, OOD-FP=0.007283763277693475, AUROC=0.7587563760019431, FPR@95=0.35083798882681566, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.084, P=0.271, R=0.131, OOD-FP=0.011606982184631996, AUROC=0.7712310934787998, FPR@95=0.3486238532110092, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.130, P=0.346, R=0.173, OOD-FP=0.010235673216655083, AUROC=0.7052351517333814, FPR@95=0.44013683010262256, N=87\n",
      "   CAM_BACK: AP=0.048, P=0.121, R=0.107, OOD-FP=0.012062181513255499, AUROC=0.7797761109216956, FPR@95=0.29912490055688146, N=82\n",
      "   CAM_BACK_LEFT: AP=0.103, P=0.342, R=0.123, OOD-FP=0.014170242490769385, AUROC=0.7774714052287581, FPR@95=0.3137254901960784, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.106, P=0.401, R=0.155, OOD-FP=0.0078119501724259275, AUROC=0.7356984478935699, FPR@95=0.4, N=84\n",
      "SSD300_VGG16      0.155     0.360     0.233     0.009     0.7478    0.4200    500     11.6      0.759/0.409       \n",
      "   CAM_FRONT: AP=0.195, P=0.396, R=0.272, OOD-FP=0.0050631106040077015, AUROC=0.7660675248967694, FPR@95=0.4491620111731844, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.172, P=0.366, R=0.243, OOD-FP=0.010912563088255355, AUROC=0.7542772129928093, FPR@95=0.4151376146788991, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.200, P=0.417, R=0.257, OOD-FP=0.009707112970711296, AUROC=0.7425433595390984, FPR@95=0.47206385404789053, N=87\n",
      "   CAM_BACK: AP=0.107, P=0.334, R=0.185, OOD-FP=0.009007678676904902, AUROC=0.7478690760313673, FPR@95=0.35799522673031026, N=82\n",
      "   CAM_BACK_LEFT: AP=0.155, P=0.386, R=0.216, OOD-FP=0.013710747456877488, AUROC=0.8071895424836601, FPR@95=0.3235294117647059, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.182, P=0.437, R=0.227, OOD-FP=0.008752983971808572, AUROC=0.7348115299334812, FPR@95=0.43454545454545457, N=84\n",
      "YOLOv8n           0.140     0.349     0.231     0.056     0.8203    0.3164    500     6.5       0.832/0.305       \n",
      "   CAM_FRONT: AP=0.175, P=0.376, R=0.273, OOD-FP=0.03331482509716824, AUROC=0.8040077726499879, FPR@95=0.3653631284916201, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.154, P=0.351, R=0.291, OOD-FP=0.060158910329171394, AUROC=0.8480969501611704, FPR@95=0.33256880733944955, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.169, P=0.334, R=0.254, OOD-FP=0.056708160442600276, AUROC=0.7996559243033468, FPR@95=0.3443557582668187, N=87\n",
      "   CAM_BACK: AP=0.115, P=0.341, R=0.193, OOD-FP=0.046365914786967416, AUROC=0.8209739743152631, FPR@95=0.26730310262529833, N=82\n",
      "   CAM_BACK_LEFT: AP=0.117, P=0.328, R=0.201, OOD-FP=0.15254237288135594, AUROC=0.8788807189542484, FPR@95=0.22549019607843138, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.149, P=0.476, R=0.213, OOD-FP=0.06650544135429262, AUROC=0.8418625277161862, FPR@95=0.2927272727272727, N=84\n",
      "YOLOv8s           0.155     0.416     0.245     0.048     0.7954    0.3475    500     6.6       0.810/0.337       \n",
      "   CAM_FRONT: AP=0.204, P=0.472, R=0.270, OOD-FP=0.030103995621237, AUROC=0.7845275686179257, FPR@95=0.41564245810055866, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.147, P=0.401, R=0.273, OOD-FP=0.05737704918032787, AUROC=0.8166377386560874, FPR@95=0.3646788990825688, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.200, P=0.402, R=0.308, OOD-FP=0.04732080723729993, AUROC=0.782972253895857, FPR@95=0.38426453819840367, N=87\n",
      "   CAM_BACK: AP=0.131, P=0.406, R=0.212, OOD-FP=0.039634146341463415, AUROC=0.8029605637004206, FPR@95=0.294351630867144, N=82\n",
      "   CAM_BACK_LEFT: AP=0.105, P=0.295, R=0.240, OOD-FP=0.11560693641618497, AUROC=0.8555964052287581, FPR@95=0.25980392156862747, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.154, P=0.517, R=0.222, OOD-FP=0.05481120584652863, AUROC=0.8175609756097562, FPR@95=0.3018181818181818, N=84\n",
      "YOLOv8m           0.157     0.413     0.266     0.052     0.7250    0.3624    500     7.5       0.740/0.358       \n",
      "   CAM_FRONT: AP=0.208, P=0.445, R=0.304, OOD-FP=0.03645200486026731, AUROC=0.7149866407578334, FPR@95=0.4312849162011173, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.144, P=0.428, R=0.259, OOD-FP=0.05251141552511415, AUROC=0.7636374907017108, FPR@95=0.3853211009174312, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.191, P=0.443, R=0.291, OOD-FP=0.05243161094224924, AUROC=0.6901718378043169, FPR@95=0.3899657924743444, N=87\n",
      "   CAM_BACK: AP=0.142, P=0.450, R=0.217, OOD-FP=0.04276985743380855, AUROC=0.7336202977611092, FPR@95=0.3134447096260939, N=82\n",
      "   CAM_BACK_LEFT: AP=0.104, P=0.355, R=0.245, OOD-FP=0.11775700934579439, AUROC=0.7918709150326797, FPR@95=0.3137254901960784, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.159, P=0.419, R=0.276, OOD-FP=0.059278350515463915, AUROC=0.7448115299334812, FPR@95=0.31636363636363635, N=84\n",
      "YOLOv8l           0.167     0.427     0.280     0.054     0.7077    0.3676    500     9.1       0.720/0.364       \n",
      "   CAM_FRONT: AP=0.223, P=0.455, R=0.328, OOD-FP=0.038461538461538464, AUROC=0.6950935146951664, FPR@95=0.4312849162011173, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.149, P=0.405, R=0.289, OOD-FP=0.05030674846625767, AUROC=0.721981155467394, FPR@95=0.3853211009174312, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.199, P=0.500, R=0.282, OOD-FP=0.05578011317704123, AUROC=0.6911120446498229, FPR@95=0.3865450399087799, N=87\n",
      "   CAM_BACK: AP=0.157, P=0.463, R=0.253, OOD-FP=0.04432757325319309, AUROC=0.7161467212183203, FPR@95=0.3309466984884646, N=82\n",
      "   CAM_BACK_LEFT: AP=0.111, P=0.335, R=0.279, OOD-FP=0.1036468330134357, AUROC=0.747140522875817, FPR@95=0.3333333333333333, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.163, P=0.478, R=0.256, OOD-FP=0.06919945725915876, AUROC=0.7456541019955654, FPR@95=0.31636363636363635, N=84\n",
      "YOLOv11n          0.140     0.374     0.221     0.049     0.8115    0.3297    500     7.1       0.828/0.316       \n",
      "   CAM_FRONT: AP=0.181, P=0.417, R=0.230, OOD-FP=0.03007889546351085, AUROC=0.8172941462229779, FPR@95=0.39553072625698327, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.149, P=0.387, R=0.252, OOD-FP=0.05851063829787234, AUROC=0.8653607736176543, FPR@95=0.3302752293577982, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.169, P=0.388, R=0.251, OOD-FP=0.04482323232323232, AUROC=0.7801916421612755, FPR@95=0.3637400228050171, N=87\n",
      "   CAM_BACK: AP=0.121, P=0.424, R=0.179, OOD-FP=0.03876852907639681, AUROC=0.8002613933401522, FPR@95=0.284805091487669, N=82\n",
      "   CAM_BACK_LEFT: AP=0.131, P=0.365, R=0.186, OOD-FP=0.13293650793650794, AUROC=0.8601919934640523, FPR@95=0.24019607843137256, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.139, P=0.449, R=0.207, OOD-FP=0.06064209274673008, AUROC=0.8443458980044346, FPR@95=0.28, N=84\n",
      "YOLOv11s          0.158     0.382     0.266     0.051     0.7699    0.3610    500     7.3       0.785/0.357       \n",
      "   CAM_FRONT: AP=0.206, P=0.399, R=0.299, OOD-FP=0.03755868544600939, AUROC=0.7488705367986398, FPR@95=0.42793296089385474, N=79\n",
      "   CAM_FRONT_LEFT: AP=0.156, P=0.433, R=0.273, OOD-FP=0.05714285714285714, AUROC=0.8053868088271758, FPR@95=0.3944954128440367, N=76\n",
      "   CAM_FRONT_RIGHT: AP=0.194, P=0.421, R=0.294, OOD-FP=0.04839910647803425, AUROC=0.7587669287243194, FPR@95=0.3831242873432155, N=87\n",
      "   CAM_BACK: AP=0.139, P=0.432, R=0.214, OOD-FP=0.0410612760581175, AUROC=0.7686526878054324, FPR@95=0.3118536197295147, N=82\n",
      "   CAM_BACK_LEFT: AP=0.111, P=0.329, R=0.245, OOD-FP=0.11560693641618497, AUROC=0.8178104575163399, FPR@95=0.3088235294117647, N=92\n",
      "   CAM_BACK_RIGHT: AP=0.163, P=0.493, R=0.244, OOD-FP=0.05620608899297424, AUROC=0.8083592017738359, FPR@95=0.3145454545454546, N=84\n",
      "\n",
      "Saved CSV → /data/Asad/NuScenesMiniNovel/vis_results/ood_report.csv\n",
      "\n",
      "Notes:\n",
      "AP@0.5/P/R use only ID GT (single-class, greedy 1–1 matches at IoU≥0.5).\n",
      "OOD-FP is the fraction of detections that overlap any OOD GT (IoU≥0.5).\n",
      "AUROC/FPR@95 use per-GT scores: matched → 1−confidence (MSP for YOLO, conf for others),\n",
      "missed ID -> 0.0, missed OOD -> 1.0. Reported overall and per camera, plus camera-average.\n"
     ]
    }
   ],
   "source": [
    "# === nuScenes-OOD: multi-model OOD evaluation with per-camera AUROC and CSV export MORE DETECTORS===\n",
    "# - Dataset layout: <NUSCENES_OOD_ROOT>/<JSONDIR_NAME> with:\n",
    "#     sample.json, sample_data.json, scene.json, calibrated_sensor.json, sensor.json,\n",
    "#     detection_id.json, detection_novel.json (both contain {\"results\": {sd_token: [{\"bbox_2d\":[x1,y1,x2,y2]}, ...]}})\n",
    "# - KPIs (overall + per camera): AP@0.5, P@0.5, R@0.5, OOD-FP, AUROC, FPR@95, N, Time(s)\n",
    "# - Models (try/except): FasterRCNN_R50, FasterRCNN_MBV3, RetinaNet_R50, SSDLite_MBV3, SSD300_VGG16,\n",
    "#                        YOLOv8n/s/m/l, YOLOv11n/s (if present), YOLOv5s/m (torch.hub),\n",
    "#                        DETR_R50/DC5/Deformable (torchvision)\n",
    "\n",
    "import os, json, time, math, random, warnings, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from PIL import Image, ImageFile\n",
    "import torchvision.transforms as T\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "NUSCENES_OOD_ROOT  = Path(\"/data/Asad/NuScenesMiniNovel\")   \n",
    "JSONDIR_NAME       = \"v1.0-mini\"                             \n",
    "MAX_IMAGES         = 500                                     \n",
    "IOU_MATCH          = 0.5                                    \n",
    "SCORE_THRESH       = 0.05                                   \n",
    "CSV_DIR            = Path(\"/data/Asad/NuScenesMiniNovel/vis_results/\")\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_OUT            = CSV_DIR / \"ood_report.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch: {torch.__version__} | CUDA: {torch.cuda.is_available()} | Device: {device}\")\n",
    "\n",
    "# ------------------ DATASET ------------------\n",
    "def load_json(p): \n",
    "    with open(p,\"r\") as f: \n",
    "        return json.load(f)\n",
    "\n",
    "def load_frame_index(dataroot: Path):\n",
    "    jsondir = dataroot / JSONDIR_NAME\n",
    "    sd_rows = {d[\"token\"]: d for d in load_json(jsondir / \"sample_data.json\")}\n",
    "    samples = {s[\"token\"]: s for s in load_json(jsondir / \"sample.json\")}\n",
    "    scenes  = load_json(jsondir / \"scene.json\")\n",
    "    calib_by  = {c[\"token\"]: c for c in load_json(jsondir / \"calibrated_sensor.json\")}\n",
    "    sensor_by = {s[\"token\"]: s for s in load_json(jsondir / \"sensor.json\")}\n",
    "    id_path   = jsondir / \"detection_id.json\"\n",
    "    ood_path  = jsondir / \"detection_novel.json\"\n",
    "    gt_id  = load_json(id_path)[\"results\"] if id_path.exists() else {}\n",
    "    gt_ood = load_json(ood_path)[\"results\"] if ood_path.exists() else {}\n",
    "\n",
    "    def channel_of_sd_row(sd_row):\n",
    "        calib  = calib_by[sd_row[\"calibrated_sensor_token\"]]\n",
    "        sensor = sensor_by[calib[\"sensor_token\"]]\n",
    "        return sensor[\"channel\"]\n",
    "\n",
    "    sample_to_ch2sd = {}\n",
    "    for sd in sd_rows.values():\n",
    "        ch = channel_of_sd_row(sd)\n",
    "        if not ch.startswith(\"CAM_\"): \n",
    "            continue\n",
    "        st = sd[\"sample_token\"]\n",
    "        sample_to_ch2sd.setdefault(st, {})[ch] = sd[\"token\"]\n",
    "\n",
    "    frames = []\n",
    "    ALL_CAMS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "    for sc in scenes:\n",
    "        s_tok = sc[\"first_sample_token\"]\n",
    "        while s_tok:\n",
    "            sample = samples[s_tok]\n",
    "            ch2sd  = sample_to_ch2sd.get(s_tok, {})\n",
    "            for ch in ALL_CAMS:\n",
    "                sd_tok = ch2sd.get(ch)\n",
    "                if not sd_tok: \n",
    "                    continue\n",
    "                fn = sd_rows[sd_tok][\"filename\"]\n",
    "                img_path = dataroot / fn\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                id_boxes  = [b[\"bbox_2d\"] for b in gt_id.get(sd_tok, [])]\n",
    "                ood_boxes = [b[\"bbox_2d\"] for b in gt_ood.get(sd_tok, [])]\n",
    "                frames.append((\n",
    "                    str(img_path),            # 0\n",
    "                    sd_tok,                   # 1\n",
    "                    ch,                       # 2 (camera)\n",
    "                    np.array(id_boxes,  dtype=np.float32),  # 3\n",
    "                    np.array(ood_boxes, dtype=np.float32)   # 4\n",
    "                ))\n",
    "            s_tok = sample[\"next\"]\n",
    "    return frames\n",
    "\n",
    "def pick_subset(frames, k=None, seed=13):\n",
    "    if (k is None) or (k>=len(frames)): \n",
    "        return frames\n",
    "    rng = random.Random(seed)\n",
    "    idx = list(range(len(frames)))\n",
    "    rng.shuffle(idx)\n",
    "    return [frames[i] for i in idx[:k]]\n",
    "\n",
    "# ------------------ MODELS ------------------\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "# Torchvision detectors (COCO-trained)\n",
    "def load_frcnn_r50():\n",
    "    m = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_frcnn_mbv3():\n",
    "    m = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_retinanet_r50():\n",
    "    m = torchvision.models.detection.retinanet_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssdlite_mbv3():\n",
    "    m = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssd300_vgg16():\n",
    "    m = torchvision.models.detection.ssd300_vgg16(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "# --- New: DETR family (torchvision) ---\n",
    "def load_detr_r50():\n",
    "    m = torchvision.models.detection.detr_resnet50(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_detr_r50_dc5():\n",
    "    m = torchvision.models.detection.detr_resnet50_dc5(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_deformable_detr_r50():\n",
    "    m = torchvision.models.detection.deformable_detr_resnet50(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def infer_torchvision_detector(model, pil_img):\n",
    "    x = to_tensor(pil_img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model([x])[0]\n",
    "    boxes  = out[\"boxes\"].detach().float().cpu().numpy()\n",
    "    scores = out[\"scores\"].detach().float().cpu().numpy()\n",
    "    # Return \"conf\" to mean raw detection confidence (vs MSP)\n",
    "    return boxes.astype(np.float32), scores.astype(np.float32), \"conf\"\n",
    "\n",
    "# YOLOv8 / YOLOv11 (optional via ultralytics)\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    _has_yolo = True\n",
    "except Exception:\n",
    "    _has_yolo = False\n",
    "\n",
    "def load_yolov8n():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8n.pt\").to(device)\n",
    "def load_yolov8s():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8s.pt\").to(device)\n",
    "\n",
    "# --- New: more YOLOv8 sizes ---\n",
    "def load_yolov8m():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8m.pt\").to(device)\n",
    "def load_yolov8l():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8l.pt\").to(device)\n",
    "\n",
    "# --- New: YOLOv11 (if present in your ultralytics version) ---\n",
    "def load_yolo11n():\n",
    "    if not _has_yolo: return None\n",
    "    try:\n",
    "        return YOLO(\"yolo11n.pt\").to(device)\n",
    "    except Exception:\n",
    "        return None\n",
    "def load_yolo11s():\n",
    "    if not _has_yolo: return None\n",
    "    try:\n",
    "        return YOLO(\"yolo11s.pt\").to(device)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def infer_yolov8(model, pil_img):\n",
    "    import numpy as _np\n",
    "    im = _np.array(pil_img.convert(\"RGB\"))\n",
    "    ydev = 0 if device.type == \"cuda\" else \"cpu\"\n",
    "    r = model.predict(source=im, verbose=False, conf=0.001, device=ydev)[0]\n",
    "    if r is None or r.boxes is None or len(r.boxes) == 0:\n",
    "        return np.zeros((0,4), dtype=np.float32), np.zeros((0,), dtype=np.float32), \"conf\"\n",
    "    xyxy = r.boxes.xyxy.cpu().numpy().astype(np.float32)\n",
    "    # If class probabilities available, use MSP for OOD ROC (1 - max prob)\n",
    "    if hasattr(r, \"probs\") and r.probs is not None:\n",
    "        scores = r.probs.data.cpu().numpy().max(axis=1).astype(np.float32)\n",
    "        return xyxy, scores, \"MSP\"\n",
    "    else:\n",
    "        scores = r.boxes.conf.cpu().numpy().astype(np.float32)\n",
    "        return xyxy, scores, \"conf\"\n",
    "\n",
    "# --- New: YOLOv5 via torch.hub (optional; needs internet or local cache) ---\n",
    "def _try_import_yaml():\n",
    "    try:\n",
    "        import yaml  # noqa: F401\n",
    "    except Exception:\n",
    "        pass\n",
    "_try_import_yaml()\n",
    "\n",
    "def load_yolov5s():\n",
    "    try:\n",
    "        m = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)  # noqa: E402\n",
    "        return m.autoshape().to(device).eval()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_yolov5m():\n",
    "    try:\n",
    "        m = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)  # noqa: E402\n",
    "        return m.autoshape().to(device).eval()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def infer_yolov5(model, pil_img):\n",
    "    import numpy as _np\n",
    "    im = _np.array(pil_img.convert(\"RGB\"))\n",
    "    with torch.no_grad():\n",
    "        r = model(im, size=640)\n",
    "    if r is None or len(getattr(r, \"xyxy\", [])) == 0 or r.xyxy[0].numel() == 0:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32), \"conf\"\n",
    "    det = r.xyxy[0].detach().cpu().numpy()  # [N,6]: x1,y1,x2,y2,conf,cls\n",
    "    boxes  = det[:, :4].astype(np.float32)\n",
    "    scores = det[:, 4].astype(np.float32)\n",
    "    return boxes, scores, \"conf\"\n",
    "\n",
    "# ------------------ METRICS ------------------\n",
    "def iou_xyxy(a, b):\n",
    "    Na, Nb = a.shape[0], b.shape[0]\n",
    "    if Na==0 or Nb==0:\n",
    "        return np.zeros((Na,Nb), dtype=np.float32)\n",
    "    ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n",
    "    bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
    "    inter_x1 = np.maximum(ax1[:,None], bx1[None,:])\n",
    "    inter_y1 = np.maximum(ay1[:,None], by1[None,:])\n",
    "    inter_x2 = np.minimum(ax2[:,None], bx2[None,:])\n",
    "    inter_y2 = np.minimum(ay2[:,None], by2[None,:])\n",
    "    inter_w = np.clip(inter_x2 - inter_x1, 0, None)\n",
    "    inter_h = np.clip(inter_y2 - inter_y1, 0, None)\n",
    "    inter = inter_w * inter_h\n",
    "    area_a = (ax2-ax1)*(ay2-ay1)\n",
    "    area_b = (bx2-bx1)*(by2-by1)\n",
    "    union = area_a[:,None] + area_b[None,:] - inter\n",
    "    return np.where(union>0, inter/union, 0.0)\n",
    "\n",
    "def ap50_single_class(all_scores, all_tp, total_gt_pos):\n",
    "    if len(all_scores)==0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    order = np.argsort(-np.array(all_scores))\n",
    "    tp = np.array(all_tp)[order].astype(np.float32)\n",
    "    fp = 1.0 - tp\n",
    "    cum_tp = np.cumsum(tp)\n",
    "    cum_fp = np.cumsum(fp)\n",
    "    recall = cum_tp / max(1, total_gt_pos)\n",
    "    precision = cum_tp / np.maximum(1, (cum_tp+cum_fp))\n",
    "    # all-points interpolation\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "    for i in range(mpre.size-1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = float(np.sum((mrec[idx+1]-mrec[idx]) * mpre[idx+1]))\n",
    "    best_i = int(np.argmax(2*precision*recall/(precision+recall+1e-9)))\n",
    "    return ap, float(precision[best_i]), float(recall[best_i])\n",
    "\n",
    "# ------------------ EVAL ------------------\n",
    "ALL_CAMS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "def evaluate_dataset(frames, model_name, model, infer_fn, compute_ood_roc=True):\n",
    "    # global accumulators\n",
    "    all_scores, all_tp = [], []\n",
    "    total_id_gt = 0\n",
    "    det_ood_hits = 0\n",
    "    total_dets = 0\n",
    "    roc_scores_glob, roc_labels_glob = [], []\n",
    "\n",
    "    # per-camera accumulators\n",
    "    per_cam = {\n",
    "        cam: {\n",
    "            \"scores\": [], \"tp\": [], \"id_gt\": 0,\n",
    "            \"OOD_FP_hits\": 0, \"detections\": 0,\n",
    "            \"roc_scores\": [], \"roc_labels\": [], \"N\": 0\n",
    "        } for cam in ALL_CAMS\n",
    "    }\n",
    "\n",
    "    have_any_ood = any(len(fr[4])>0 for fr in frames)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for (img_path, sd_tok, cam, id_boxes, ood_boxes) in frames:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        boxes, conf_like, score_type = infer_fn(model, img)\n",
    "        keep = conf_like >= SCORE_THRESH if conf_like is not None and len(conf_like)>0 else np.array([], dtype=bool)\n",
    "        boxes = boxes[keep] if boxes.size else boxes\n",
    "        conf_like = conf_like[keep] if keep.size else conf_like\n",
    "\n",
    "        # === ID AP accounting (global + per-cam) ===\n",
    "        total_id_gt += int(id_boxes.shape[0])\n",
    "        used = np.zeros((id_boxes.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and id_boxes.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, id_boxes)\n",
    "            order = np.argsort(-conf_like)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not used[j]:\n",
    "                    all_scores.append(float(conf_like[di])); all_tp.append(1); used[j]=True\n",
    "                else:\n",
    "                    all_scores.append(float(conf_like[di])); all_tp.append(0)\n",
    "        else:\n",
    "            for s in (conf_like if conf_like is not None else []):\n",
    "                all_scores.append(float(s)); all_tp.append(0)\n",
    "\n",
    "        # per-camera AP accum\n",
    "        cam_used = np.zeros((id_boxes.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and id_boxes.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, id_boxes)\n",
    "            order = np.argsort(-conf_like)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not cam_used[j]:\n",
    "                    per_cam[cam][\"scores\"].append(float(conf_like[di]))\n",
    "                    per_cam[cam][\"tp\"].append(1)\n",
    "                    cam_used[j]=True\n",
    "                else:\n",
    "                    per_cam[cam][\"scores\"].append(float(conf_like[di]))\n",
    "                    per_cam[cam][\"tp\"].append(0)\n",
    "        else:\n",
    "            for s in (conf_like if conf_like is not None else []):\n",
    "                per_cam[cam][\"scores\"].append(float(s))\n",
    "                per_cam[cam][\"tp\"].append(0)\n",
    "        per_cam[cam][\"id_gt\"] += int(id_boxes.shape[0])\n",
    "\n",
    "        # === OOD-FP rate: any det overlapping any OOD GT ===\n",
    "        total_dets += int(boxes.shape[0])\n",
    "        per_cam[cam][\"detections\"] += int(boxes.shape[0])\n",
    "        if boxes.shape[0] > 0 and ood_boxes.shape[0] > 0:\n",
    "            IoU_ood = iou_xyxy(boxes, ood_boxes)\n",
    "            hits = (IoU_ood.max(axis=1) >= IOU_MATCH).sum()\n",
    "            det_ood_hits += int(hits)\n",
    "            per_cam[cam][\"OOD_FP_hits\"] += int(hits)\n",
    "\n",
    "        if compute_ood_roc and have_any_ood:\n",
    "            if boxes.shape[0] > 0:\n",
    "                IoU_id  = iou_xyxy(id_boxes,  boxes) if id_boxes.shape[0]>0 else np.zeros((0, boxes.shape[0]))\n",
    "                IoU_ood = iou_xyxy(ood_boxes, boxes) if ood_boxes.shape[0]>0 else np.zeros((0, boxes.shape[0]))\n",
    "            else:\n",
    "                IoU_id  = np.zeros((id_boxes.shape[0],  0))\n",
    "                IoU_ood = np.zeros((ood_boxes.shape[0], 0))\n",
    "            det_ood_score = (1.0 - conf_like) if (conf_like is not None and len(conf_like)>0) else np.array([])\n",
    "\n",
    "            # ID GT -> label 0\n",
    "            for gi in range(id_boxes.shape[0]):\n",
    "                if IoU_id.shape[1] > 0 and IoU_id[gi].max() >= IOU_MATCH:\n",
    "                    di = int(IoU_id[gi].argmax())\n",
    "                    s = float(det_ood_score[di]) if det_ood_score.size>0 else 0.0\n",
    "                    roc_scores_glob.append(s); roc_labels_glob.append(0)\n",
    "                    per_cam[cam][\"roc_scores\"].append(s); per_cam[cam][\"roc_labels\"].append(0)\n",
    "                else:\n",
    "                    roc_scores_glob.append(0.0); roc_labels_glob.append(0)\n",
    "                    per_cam[cam][\"roc_scores\"].append(0.0); per_cam[cam][\"roc_labels\"].append(0)\n",
    "            # OOD GT -> label 1\n",
    "            for gi in range(ood_boxes.shape[0]):\n",
    "                if IoU_ood.shape[1] > 0 and IoU_ood[gi].max() >= IOU_MATCH:\n",
    "                    di = int(IoU_ood[gi].argmax())\n",
    "                    s = float(det_ood_score[di]) if det_ood_score.size>0 else 1.0\n",
    "                    roc_scores_glob.append(s); roc_labels_glob.append(1)\n",
    "                    per_cam[cam][\"roc_scores\"].append(s); per_cam[cam][\"roc_labels\"].append(1)\n",
    "                else:\n",
    "                    roc_scores_glob.append(1.0); roc_labels_glob.append(1)\n",
    "                    per_cam[cam][\"roc_scores\"].append(1.0); per_cam[cam][\"roc_labels\"].append(1)\n",
    "\n",
    "        per_cam[cam][\"N\"] += 1\n",
    "\n",
    "    # Aggregate global AP on ID\n",
    "    ap, p, r = ap50_single_class(all_scores, all_tp, total_id_gt)\n",
    "    ood_fp_rate = det_ood_hits / max(1, total_dets)\n",
    "    results = {\n",
    "        \"AP50\": ap, \"P@0.5\": p, \"R@0.5\": r,\n",
    "        \"OOD_FP_rate\": ood_fp_rate,\n",
    "        \"N_imgs\": len(frames),\n",
    "        \"time_s\": time.time()-t0\n",
    "    }\n",
    "\n",
    "    # Global AUROC/FPR@95\n",
    "    if compute_ood_roc and len(set(roc_labels_glob))>1:\n",
    "        from sklearn.metrics import roc_auc_score, roc_curve\n",
    "        scores = np.array(roc_scores_glob); labels = np.array(roc_labels_glob)\n",
    "        auroc = float(roc_auc_score(labels, scores))\n",
    "        fpr, tpr, thr = roc_curve(labels, scores)\n",
    "        i95 = int(np.argmin(np.abs(tpr - 0.95)))\n",
    "        results[\"AUROC\"]  = auroc\n",
    "        results[\"FPR@95\"] = float(fpr[i95])\n",
    "\n",
    "    # Per-camera AP + AUROC/FPR@95 + OOD-FP\n",
    "    results[\"per_camera\"] = {}\n",
    "    for cam in ALL_CAMS:\n",
    "        A = per_cam[cam]\n",
    "        ap_c, p_c, r_c = ap50_single_class(A[\"scores\"], A[\"tp\"], A[\"id_gt\"])\n",
    "        oodfp_c = (A[\"OOD_FP_hits\"] / max(1, A[\"detections\"])) if A[\"detections\"]>0 else float('nan')\n",
    "        cam_res = {\"AP50\": ap_c, \"P@0.5\": p_c, \"R@0.5\": r_c, \"OOD_FP\": oodfp_c, \"N\": A[\"N\"]}\n",
    "        # per-cam AUROC\n",
    "        if compute_ood_roc and len(A[\"roc_labels\"])>0 and len(set(A[\"roc_labels\"]))>1:\n",
    "            from sklearn.metrics import roc_auc_score, roc_curve\n",
    "            sc = np.array(A[\"roc_scores\"]); lb = np.array(A[\"roc_labels\"])\n",
    "            try:\n",
    "                au = float(roc_auc_score(lb, sc))\n",
    "                fpr, tpr, thr = roc_curve(lb, sc)\n",
    "                i95 = int(np.argmin(np.abs(tpr - 0.95)))\n",
    "                cam_res[\"AUROC\"]  = au\n",
    "                cam_res[\"FPR@95\"] = float(fpr[i95])\n",
    "            except Exception:\n",
    "                pass\n",
    "        results[\"per_camera\"][cam] = cam_res\n",
    "\n",
    "    # Camera-average (macro) for printing/CSV convenience\n",
    "    def cam_mean(key):\n",
    "        vals = [results[\"per_camera\"][c].get(key, float('nan')) for c in ALL_CAMS]\n",
    "        vals = [v for v in vals if not (isinstance(v,float) and math.isnan(v))]\n",
    "        return float(np.mean(vals)) if len(vals)>0 else float('nan')\n",
    "    results[\"camera_avg\"] = {\n",
    "        \"AP50\": cam_mean(\"AP50\"),\n",
    "        \"P@0.5\": cam_mean(\"P@0.5\"),\n",
    "        \"R@0.5\": cam_mean(\"R@0.5\"),\n",
    "        \"OOD_FP\": cam_mean(\"OOD_FP\"),\n",
    "        \"AUROC\": cam_mean(\"AUROC\"),\n",
    "        \"FPR@95\": cam_mean(\"FPR@95\")\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# ------------------ RUN ------------------\n",
    "def pretty_row(cols, widths):\n",
    "    return \"  \".join(str(c).ljust(w) for c,w in zip(cols, widths))\n",
    "\n",
    "# Load frames\n",
    "frames_all = load_frame_index(NUSCENES_OOD_ROOT)\n",
    "frames = pick_subset(frames_all, MAX_IMAGES)\n",
    "print(f\"Loaded OOD frames: {len(frames)}\")\n",
    "\n",
    "# Build model list\n",
    "models = []\n",
    "try:   models.append((\"FasterRCNN_R50\", load_frcnn_r50(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_R50 not available: {e}\")\n",
    "try:   models.append((\"FasterRCNN_MBV3\", load_frcnn_mbv3(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_MBV3 not available: {e}\")\n",
    "try:   models.append((\"RetinaNet_R50\",  load_retinanet_r50(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"RetinaNet_R50 not available: {e}\")\n",
    "try:   models.append((\"SSDLite_MBV3\",   load_ssdlite_mbv3(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSDLite_MBV3 not available: {e}\")\n",
    "try:   models.append((\"SSD300_VGG16\",   load_ssd300_vgg16(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSD300_VGG16 not available: {e}\")\n",
    "\n",
    "# DETR family\n",
    "try:   models.append((\"DETR_R50\",          load_detr_r50(),            infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"DETR_R50 not available: {e}\")\n",
    "try:   models.append((\"DETR_R50_DC5\",      load_detr_r50_dc5(),        infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"DETR_R50_DC5 not available: {e}\")\n",
    "try:   models.append((\"DeformableDETR_R50\",load_deformable_detr_r50(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"DeformableDETR_R50 not available: {e}\")\n",
    "\n",
    "# YOLOv8 baseline sizes\n",
    "try:\n",
    "    y8n = load_yolov8n()\n",
    "    if y8n is not None: models.append((\"YOLOv8n\", y8n, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8n not available: {e}\")\n",
    "try:\n",
    "    y8s = load_yolov8s()\n",
    "    if y8s is not None: models.append((\"YOLOv8s\", y8s, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8s not available: {e}\")\n",
    "\n",
    "# YOLOv8 larger\n",
    "try:\n",
    "    y8m = load_yolov8m()\n",
    "    if y8m is not None: models.append((\"YOLOv8m\", y8m, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8m not available: {e}\")\n",
    "try:\n",
    "    y8l = load_yolov8l()\n",
    "    if y8l is not None: models.append((\"YOLOv8l\", y8l, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8l not available: {e}\")\n",
    "\n",
    "# YOLOv11 (if present)\n",
    "try:\n",
    "    y11n = load_yolo11n()\n",
    "    if y11n is not None: models.append((\"YOLOv11n\", y11n, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv11n not available: {e}\")\n",
    "try:\n",
    "    y11s = load_yolo11s()\n",
    "    if y11s is not None: models.append((\"YOLOv11s\", y11s, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv11s not available: {e}\")\n",
    "\n",
    "# YOLOv5 (torch.hub)\n",
    "try:\n",
    "    y5s = load_yolov5s()\n",
    "    if y5s is not None: models.append((\"YOLOv5s\", y5s, infer_yolov5))\n",
    "except Exception as e: warnings.warn(f\"YOLOv5s not available: {e}\")\n",
    "try:\n",
    "    y5m = load_yolov5m()\n",
    "    if y5m is not None: models.append((\"YOLOv5m\", y5m, infer_yolov5))\n",
    "except Exception as e: warnings.warn(f\"YOLOv5m not available: {e}\")\n",
    "\n",
    "# Print header\n",
    "hdr = [\"Model\",\"AP@0.5\",\"P@0.5\",\"R@0.5\",\"OOD-FP\",\"AUROC\",\"FPR@95\",\"N\",\"Time(s)\",\"CamAvg(AUROC/FPR)\"]\n",
    "w   = [16, 8, 8, 8, 8, 8, 8, 6, 8, 18]\n",
    "print(pretty_row(hdr, w))\n",
    "print(\"-\"*120)\n",
    "\n",
    "# Evaluate + write CSV\n",
    "rows_for_csv = []\n",
    "for name, model, infer_fn in models:\n",
    "    res = evaluate_dataset(frames, name, model, infer_fn, compute_ood_roc=True)\n",
    "    auroc = res.get(\"AUROC\", float('nan'))\n",
    "    fpr95 = res.get(\"FPR@95\", float('nan'))\n",
    "    cam_avg = res[\"camera_avg\"]\n",
    "    cam_avg_str = f\"{cam_avg['AUROC']:.3f}/{cam_avg['FPR@95']:.3f}\" if not math.isnan(cam_avg[\"AUROC\"]) else \"—\"\n",
    "\n",
    "    print(pretty_row([\n",
    "        name,\n",
    "        f\"{res['AP50']:.3f}\",\n",
    "        f\"{res['P@0.5']:.3f}\",\n",
    "        f\"{res['R@0.5']:.3f}\",\n",
    "        f\"{res['OOD_FP_rate']:.3f}\",\n",
    "        f\"{auroc:.4f}\" if not math.isnan(auroc) else \"—\",\n",
    "        f\"{fpr95:.4f}\" if not math.isnan(fpr95) else \"—\",\n",
    "        res[\"N_imgs\"],\n",
    "        f\"{res['time_s']:.1f}\",\n",
    "        cam_avg_str\n",
    "    ], w))\n",
    "\n",
    "    # Per-camera print\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        out_auroc = c.get(\"AUROC\", float('nan'))\n",
    "        out_fpr95 = c.get(\"FPR@95\", float('nan'))\n",
    "        print(f\"   {cam}: AP={c['AP50']:.3f}, P={c['P@0.5']:.3f}, R={c['R@0.5']:.3f}, \"\n",
    "              f\"OOD-FP={c['OOD_FP'] if not isinstance(c['OOD_FP'],float) or not math.isnan(c['OOD_FP']) else '—'}, \"\n",
    "              f\"AUROC={out_auroc if not math.isnan(out_auroc) else '—'}, \"\n",
    "              f\"FPR@95={out_fpr95 if not math.isnan(out_fpr95) else '—'}, N={c['N']}\")\n",
    "\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"overall_AP50\": res[\"AP50\"], \"overall_P@0.5\": res[\"P@0.5\"], \"overall_R@0.5\": res[\"R@0.5\"],\n",
    "        \"overall_OOD_FP\": res[\"OOD_FP_rate\"], \"overall_AUROC\": auroc if not math.isnan(auroc) else \"\",\n",
    "        \"overall_FPR@95\": fpr95 if not math.isnan(fpr95) else \"\",\n",
    "        \"N\": res[\"N_imgs\"], \"time_s\": res[\"time_s\"],\n",
    "        \"camera_avg_AP50\": cam_avg[\"AP50\"], \"camera_avg_P@0.5\": cam_avg[\"P@0.5\"], \"camera_avg_R@0.5\": cam_avg[\"R@0.5\"],\n",
    "        \"camera_avg_OOD_FP\": cam_avg[\"OOD_FP\"], \"camera_avg_AUROC\": cam_avg[\"AUROC\"] if not math.isnan(cam_avg[\"AUROC\"]) else \"\",\n",
    "        \"camera_avg_FPR@95\": cam_avg[\"FPR@95\"] if not math.isnan(cam_avg[\"FPR@95\"]) else \"\"\n",
    "    }\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        row[f\"{cam}_AP50\"]   = c[\"AP50\"]\n",
    "        row[f\"{cam}_P@0.5\"]  = c[\"P@0.5\"]\n",
    "        row[f\"{cam}_R@0.5\"]  = c[\"R@0.5\"]\n",
    "        row[f\"{cam}_OOD_FP\"] = c[\"OOD_FP\"] if not (isinstance(c[\"OOD_FP\"], float) and math.isnan(c[\"OOD_FP\"])) else \"\"\n",
    "        row[f\"{cam}_AUROC\"]  = c.get(\"AUROC\",\"\")\n",
    "        row[f\"{cam}_FPR@95\"] = c.get(\"FPR@95\",\"\")\n",
    "        row[f\"{cam}_N\"]      = c[\"N\"]\n",
    "    rows_for_csv.append(row)\n",
    "\n",
    "# Save CSV\n",
    "if rows_for_csv:\n",
    "    fieldnames = list(rows_for_csv[0].keys())\n",
    "    with open(CSV_OUT, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in rows_for_csv:\n",
    "            writer.writerow(r)\n",
    "    print(f\"\\nSaved CSV → {CSV_OUT}\")\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"AP@0.5/P/R use only ID GT (single-class, greedy 1–1 matches at IoU≥0.5).\")\n",
    "print(\"OOD-FP is the fraction of detections that overlap any OOD GT (IoU≥0.5).\")\n",
    "print(\"AUROC/FPR@95 use per-GT scores: matched → 1−confidence (MSP for YOLO, conf for others),\")\n",
    "print(\"missed ID -> 0.0, missed OOD -> 1.0. Reported overall and per camera, plus camera-average.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ec46a-bc9b-4cca-8823-8d7e73ee7018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "733aacce-4123-4ec3-8bda-ab220394d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.13.0+cu117 | CUDA: True | Device: cuda\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.193 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Collected frames with vehicle GT: 500\n",
      "Model             AP@0.5    P@0.5     R@0.5     N       Time(s)   CamAvg(AP/P/R)      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "FasterRCNN_R50    0.368     0.557     0.385     500     14.9      0.364/0.587/0.373   \n",
      "   CAM_FRONT: AP=0.398, P=0.549, R=0.435, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.360, P=0.622, R=0.365, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.307, P=0.454, R=0.331, N=82\n",
      "   CAM_BACK: AP=0.434, P=0.761, R=0.405, N=93\n",
      "   CAM_BACK_LEFT: AP=0.332, P=0.559, R=0.369, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.350, P=0.579, R=0.329, N=76\n",
      "FasterRCNN_MBV3   0.362     0.612     0.354     500     9.6       0.363/0.658/0.354   \n",
      "   CAM_FRONT: AP=0.388, P=0.561, R=0.403, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.379, P=0.696, R=0.348, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.322, P=0.507, R=0.347, N=82\n",
      "   CAM_BACK: AP=0.362, P=0.717, R=0.349, N=93\n",
      "   CAM_BACK_LEFT: AP=0.379, P=0.835, R=0.345, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.346, P=0.634, R=0.334, N=76\n",
      "RetinaNet_R50     0.395     0.603     0.386     500     17.9      0.390/0.606/0.389   \n",
      "   CAM_FRONT: AP=0.442, P=0.580, R=0.450, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.417, P=0.677, R=0.374, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.327, P=0.506, R=0.335, N=82\n",
      "   CAM_BACK: AP=0.433, P=0.749, R=0.405, N=93\n",
      "   CAM_BACK_LEFT: AP=0.356, P=0.514, R=0.432, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.366, P=0.611, R=0.339, N=76\n",
      "SSDLite_MBV3      0.224     0.463     0.204     500     18.0      0.245/0.511/0.235   \n",
      "   CAM_FRONT: AP=0.212, P=0.304, R=0.225, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.283, P=0.835, R=0.237, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.221, P=0.510, R=0.203, N=82\n",
      "   CAM_BACK: AP=0.141, P=0.293, R=0.165, N=93\n",
      "   CAM_BACK_LEFT: AP=0.322, P=0.500, R=0.316, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.290, P=0.624, R=0.265, N=76\n",
      "SSD300_VGG16      0.366     0.622     0.348     500     10.9      0.364/0.702/0.334   \n",
      "   CAM_FRONT: AP=0.409, P=0.678, R=0.376, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.369, P=0.844, R=0.301, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.318, P=0.552, R=0.320, N=82\n",
      "   CAM_BACK: AP=0.337, P=0.697, R=0.322, N=93\n",
      "   CAM_BACK_LEFT: AP=0.406, P=0.813, R=0.359, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.347, P=0.630, R=0.324, N=76\n",
      "YOLOv8n           0.327     0.606     0.314     500     6.3       0.327/0.661/0.319   \n",
      "   CAM_FRONT: AP=0.360, P=0.494, R=0.396, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.365, P=0.770, R=0.342, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.265, P=0.639, R=0.240, N=82\n",
      "   CAM_BACK: AP=0.317, P=0.621, R=0.315, N=93\n",
      "   CAM_BACK_LEFT: AP=0.353, P=0.840, R=0.330, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.301, P=0.600, R=0.293, N=76\n",
      "YOLOv8s           0.350     0.581     0.359     500     6.3       0.348/0.681/0.344   \n",
      "   CAM_FRONT: AP=0.380, P=0.540, R=0.411, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.378, P=0.816, R=0.351, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.292, P=0.562, R=0.300, N=82\n",
      "   CAM_BACK: AP=0.368, P=0.743, R=0.351, N=93\n",
      "   CAM_BACK_LEFT: AP=0.359, P=0.691, R=0.369, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.308, P=0.732, R=0.280, N=76\n",
      "\n",
      "Saved CSV → /data/Asad/NuScenesMini/vis_results/nuscenes_report.csv\n",
      "\n",
      "Notes:\n",
      "This evaluates single-class VEHICLE detection (COCO vehicle-like classes) on RGB cameras.\n",
      "AP@0.5/P/R use greedy one-to-one matching at IoU≥0.5 against 2D GT projected from 3D boxes.\n",
      "Reported overall metrics, per-camera metrics, and camera-average (macro).\n"
     ]
    }
   ],
   "source": [
    "# === nuScenes (plain) VEHICLE detection benchmark (no OOD labels) ==========================\n",
    "# - Builds 2D vehicle GT by projecting nuScenes 3D boxes into each camera.\n",
    "# - KPIs (overall + per camera): AP@0.5, P@0.5, R@0.5, N, Time(s). (No AUROC/FPR here.)\n",
    "# - Models (try/except): FasterRCNN_R50, FasterRCNN_MBV3, RetinaNet_R50, SSDLite_MBV3, SSD300_VGG16, YOLOv8n/s (optional)\n",
    "\n",
    "import os, time, math, csv, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# --- Paths & Config ---\n",
    "NUSCENES_ROOT = Path(\"/data/Asad/NuScenesMini\")   # contains v1.0-mini/ or v1.0-trainval/\n",
    "VERSION       = \"v1.0-mini\"                        # or \"v1.0-trainval\"\n",
    "MAX_IMAGES    = 500                                # cap total frames across all cameras (None for all)\n",
    "IOU_MATCH     = 0.5\n",
    "SCORE_THRESH  = 0.05\n",
    "\n",
    "CSV_DIR = Path(\"/data/Asad/NuScenesMini/vis_results/\")\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_OUT = CSV_DIR / \"nuscenes_report.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch: {torch.__version__} | CUDA: {torch.cuda.is_available()} | Device: {device}\")\n",
    "\n",
    "# --- nuScenes devkit ---\n",
    "try:\n",
    "    from nuscenes.nuscenes import NuScenes\n",
    "    from nuscenes.utils.data_classes import Box\n",
    "    from nuscenes.utils.geometry_utils import view_points\n",
    "    from pyquaternion import Quaternion\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"This cell requires the nuScenes devkit:\\n\"\n",
    "        \"  pip install nuscenes-devkit pyquaternion\"\n",
    "    ) from e\n",
    "\n",
    "# ----------------- 1) Build 2D VEHICLE GT (by projecting 3D) -----------------\n",
    "ALL_CAMS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\n",
    "            \"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "VEHICLE_PREFIXES = (\n",
    "    \"vehicle.car\", \"vehicle.bus\", \"vehicle.truck\", \"vehicle.trailer\",\n",
    "    \"vehicle.construction\", \"vehicle.emergency\", \"vehicle.other\",\n",
    "    \"vehicle.motorcycle\", \"vehicle.bicycle\"\n",
    ")\n",
    "\n",
    "def is_vehicle_category(cat: str) -> bool:\n",
    "    return any(cat.startswith(p) for p in VEHICLE_PREFIXES)\n",
    "\n",
    "def project_box_to_image(nusc: NuScenes, ann_token: str, sd_token: str, img_hw):\n",
    "    ann = nusc.get(\"sample_annotation\", ann_token)\n",
    "    if not is_vehicle_category(ann[\"category_name\"]):\n",
    "        return None\n",
    "\n",
    "    box = Box(center=ann[\"translation\"], size=ann[\"size\"],\n",
    "              orientation=Quaternion(ann[\"rotation\"]),\n",
    "              name=ann[\"category_name\"], token=ann_token)\n",
    "\n",
    "    sd = nusc.get(\"sample_data\", sd_token)\n",
    "    ep = nusc.get(\"ego_pose\", sd[\"ego_pose_token\"])\n",
    "    cs = nusc.get(\"calibrated_sensor\", sd[\"calibrated_sensor_token\"])\n",
    "\n",
    "    # World -> ego\n",
    "    box.translate(-np.array(ep[\"translation\"]))\n",
    "    box.rotate(Quaternion(ep[\"rotation\"]).inverse)\n",
    "\n",
    "    # Ego -> camera\n",
    "    box.translate(-np.array(cs[\"translation\"]))\n",
    "    box.rotate(Quaternion(cs[\"rotation\"]).inverse)\n",
    "\n",
    "    if box.center[2] <= 0.1:\n",
    "        return None\n",
    "\n",
    "    K = np.array(cs[\"camera_intrinsic\"], dtype=np.float32)\n",
    "    pts = view_points(box.corners(), K, normalize=True)  # (3,8)\n",
    "\n",
    "    h, w = img_hw\n",
    "    xs, ys = pts[0], pts[1]\n",
    "    x1 = float(np.clip(xs.min(), 0, w-1))\n",
    "    y1 = float(np.clip(ys.min(), 0, h-1))\n",
    "    x2 = float(np.clip(xs.max(), 0, w-1))\n",
    "    y2 = float(np.clip(ys.max(), 0, h-1))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    if (x2-x1)*(y2-y1) < 20:  # tiny\n",
    "        return None\n",
    "    return np.array([x1, y1, x2, y2], dtype=np.float32)\n",
    "\n",
    "def collect_frames_with_vehicle_gt(nusc: NuScenes, max_images=None):\n",
    "    frames = []\n",
    "    count = 0\n",
    "    for sample in nusc.sample:\n",
    "        anns = sample[\"anns\"]\n",
    "        for cam in ALL_CAMS:\n",
    "            sd_tok = sample[\"data\"].get(cam)\n",
    "            if sd_tok is None: \n",
    "                continue\n",
    "            sd = nusc.get(\"sample_data\", sd_tok)\n",
    "            img_path = Path(nusc.dataroot) / sd[\"filename\"]\n",
    "            if not img_path.exists():\n",
    "                continue\n",
    "            try:\n",
    "                with Image.open(img_path) as im:\n",
    "                    w, h = im.size\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            gts = []\n",
    "            for ann_tok in anns:\n",
    "                bb = project_box_to_image(nusc, ann_tok, sd_tok, (h, w))\n",
    "                if bb is not None:\n",
    "                    gts.append(bb)\n",
    "            if len(gts) == 0:\n",
    "                continue\n",
    "\n",
    "            frames.append((str(img_path), sd_tok, cam,\n",
    "                           np.stack(gts, axis=0).astype(np.float32)))\n",
    "            count += 1\n",
    "            if (max_images is not None) and (count >= max_images):\n",
    "                return frames\n",
    "    return frames\n",
    "\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "# COCO vehicle class ids for torchvision models\n",
    "VEHICLE_COCO_IDS = {2, 3, 4, 6, 7, 8}  # bicycle, car, motorcycle, bus, train, truck\n",
    "\n",
    "def load_frcnn_r50():\n",
    "    m = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_frcnn_mbv3():\n",
    "    m = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_retinanet_r50():\n",
    "    m = torchvision.models.detection.retinanet_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssdlite_mbv3():\n",
    "    m = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssd300_vgg16():\n",
    "    m = torchvision.models.detection.ssd300_vgg16(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def infer_torchvision_detector(model, pil_img):\n",
    "    x = to_tensor(pil_img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model([x])[0]\n",
    "    boxes  = out[\"boxes\"].detach().float().cpu().numpy()\n",
    "    scores = out[\"scores\"].detach().float().cpu().numpy()\n",
    "    labels = out[\"labels\"].detach().cpu().numpy()\n",
    "    keep   = np.isin(labels, list(VEHICLE_COCO_IDS))\n",
    "    return boxes[keep].astype(np.float32), scores[keep].astype(np.float32)\n",
    "\n",
    "# YOLOv8 (optional)\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    _has_yolo = True\n",
    "except Exception:\n",
    "    _has_yolo = False\n",
    "\n",
    "YOLO_VEHICLE_NAMES = {\"bicycle\",\"car\",\"motorcycle\",\"bus\",\"train\",\"truck\"}\n",
    "\n",
    "def load_yolov8n():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "def load_yolov8s():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8s.pt\").to(device)\n",
    "\n",
    "def infer_yolov8(model, pil_img):\n",
    "    im = np.array(pil_img.convert(\"RGB\"))\n",
    "    ydev = 0 if device.type==\"cuda\" else \"cpu\"\n",
    "    r = model.predict(source=im, verbose=False, conf=0.001, device=ydev)[0]\n",
    "    if r is None or r.boxes is None or len(r.boxes)==0:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "    xyxy = r.boxes.xyxy.cpu().numpy().astype(np.float32)\n",
    "    cls  = r.boxes.cls.cpu().numpy().astype(int)\n",
    "    if hasattr(r, \"probs\") and r.probs is not None:\n",
    "        scores = r.probs.data.cpu().numpy().max(axis=1).astype(np.float32)\n",
    "    else:\n",
    "        scores = r.boxes.conf.cpu().numpy().astype(np.float32)\n",
    "    names = r.names if hasattr(r,\"names\") else model.model.names\n",
    "    cls_names = [names[c] for c in cls]\n",
    "    keep = np.array([n in YOLO_VEHICLE_NAMES for n in cls_names], dtype=bool)\n",
    "    return xyxy[keep], scores[keep]\n",
    "\n",
    "def iou_xyxy(a, b):\n",
    "    Na, Nb = a.shape[0], b.shape[0]\n",
    "    if Na==0 or Nb==0:\n",
    "        return np.zeros((Na,Nb), dtype=np.float32)\n",
    "    ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n",
    "    bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
    "    inter_x1 = np.maximum(ax1[:,None], bx1[None,:])\n",
    "    inter_y1 = np.maximum(ay1[:,None], by1[None,:])\n",
    "    inter_x2 = np.minimum(ax2[:,None], bx2[None,:])\n",
    "    inter_y2 = np.minimum(ay2[:,None], by2[None,:])\n",
    "    inter_w  = np.clip(inter_x2 - inter_x1, 0, None)\n",
    "    inter_h  = np.clip(inter_y2 - inter_y1, 0, None)\n",
    "    inter    = inter_w * inter_h\n",
    "    area_a   = (ax2-ax1)*(ay2-ay1)\n",
    "    area_b   = (bx2-bx1)*(by2-by1)\n",
    "    union    = area_a[:,None] + area_b[None,:] - inter\n",
    "    return np.where(union>0, inter/union, 0.0)\n",
    "\n",
    "def ap50_single_class(all_scores, all_tp, total_gt_pos):\n",
    "    if len(all_scores)==0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    order = np.argsort(-np.array(all_scores))\n",
    "    tp = np.array(all_tp)[order].astype(np.float32)\n",
    "    fp = 1.0 - tp\n",
    "    cum_tp = np.cumsum(tp); cum_fp = np.cumsum(fp)\n",
    "    recall = cum_tp / max(1, total_gt_pos)\n",
    "    precision = cum_tp / np.maximum(1, (cum_tp+cum_fp))\n",
    "    # all-points interpolation\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "    for i in range(mpre.size-1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = float(np.sum((mrec[idx+1]-mrec[idx]) * mpre[idx+1]))\n",
    "    best_i = int(np.argmax(2*precision*recall/(precision+recall+1e-9)))\n",
    "    return ap, float(precision[best_i]), float(recall[best_i])\n",
    "\n",
    "# ----------------- 4) Evaluation -----------------\n",
    "def evaluate_dataset(frames, model_name, model, infer_fn):\n",
    "    all_scores, all_tp = [], []\n",
    "    total_gt = 0\n",
    "    per_cam = {cam: {\"scores\":[], \"tp\":[], \"gt\":0, \"N\":0} for cam in ALL_CAMS}\n",
    "\n",
    "    t0 = time.time()\n",
    "    for (img_path, sd_tok, cam, gt) in frames:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        boxes, scores = infer_fn(model, img)\n",
    "        keep = scores >= SCORE_THRESH\n",
    "        boxes = boxes[keep]; scores = scores[keep]\n",
    "\n",
    "        total_gt += int(gt.shape[0])\n",
    "        # global\n",
    "        used = np.zeros((gt.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and gt.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, gt)\n",
    "            order = np.argsort(-scores)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not used[j]:\n",
    "                    all_scores.append(float(scores[di])); all_tp.append(1); used[j]=True\n",
    "                else:\n",
    "                    all_scores.append(float(scores[di])); all_tp.append(0)\n",
    "        else:\n",
    "            for s in scores:\n",
    "                all_scores.append(float(s)); all_tp.append(0)\n",
    "\n",
    "        # per-camera\n",
    "        cA = per_cam[cam]\n",
    "        cA[\"gt\"] += int(gt.shape[0]); cA[\"N\"] += 1\n",
    "        used_c = np.zeros((gt.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and gt.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, gt)\n",
    "            order = np.argsort(-scores)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not used_c[j]:\n",
    "                    cA[\"scores\"].append(float(scores[di])); cA[\"tp\"].append(1); used_c[j]=True\n",
    "                else:\n",
    "                    cA[\"scores\"].append(float(scores[di])); cA[\"tp\"].append(0)\n",
    "        else:\n",
    "            for s in scores:\n",
    "                cA[\"scores\"].append(float(s)); cA[\"tp\"].append(0)\n",
    "\n",
    "    ap, p, r = ap50_single_class(all_scores, all_tp, total_gt)\n",
    "    results = {\n",
    "        \"AP50\": ap, \"P@0.5\": p, \"R@0.5\": r,\n",
    "        \"N_imgs\": len(frames), \"time_s\": time.time()-t0,\n",
    "        \"per_camera\": {}\n",
    "    }\n",
    "\n",
    "    # per-camera metrics\n",
    "    for cam in ALL_CAMS:\n",
    "        A = per_cam[cam]\n",
    "        ap_c, p_c, r_c = ap50_single_class(A[\"scores\"], A[\"tp\"], A[\"gt\"])\n",
    "        results[\"per_camera\"][cam] = {\"AP50\": ap_c, \"P@0.5\": p_c, \"R@0.5\": r_c, \"N\": A[\"N\"]}\n",
    "\n",
    "    # camera-average (macro)\n",
    "    def cam_mean(key):\n",
    "        vals = [results[\"per_camera\"][c][key] for c in ALL_CAMS if results[\"per_camera\"][c][\"N\"]>0]\n",
    "        return float(np.mean(vals)) if len(vals)>0 else float('nan')\n",
    "    results[\"camera_avg\"] = {\n",
    "        \"AP50\": cam_mean(\"AP50\"), \"P@0.5\": cam_mean(\"P@0.5\"), \"R@0.5\": cam_mean(\"R@0.5\")\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# ----------------- 5) Run -----------------\n",
    "def pretty_row(cols, widths):\n",
    "    return \"  \".join(str(c).ljust(w) for c,w in zip(cols, widths))\n",
    "\n",
    "# Init nuScenes + collect frames\n",
    "nusc = NuScenes(version=VERSION, dataroot=str(NUSCENES_ROOT), verbose=True)\n",
    "frames_all = collect_frames_with_vehicle_gt(nusc, max_images=None)\n",
    "frames = frames_all[:MAX_IMAGES] if (MAX_IMAGES is not None) else frames_all\n",
    "print(f\"Collected frames with vehicle GT: {len(frames)}\")\n",
    "\n",
    "# Models\n",
    "models = []\n",
    "try:   models.append((\"FasterRCNN_R50\",  load_frcnn_r50(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_R50 not available: {e}\")\n",
    "try:   models.append((\"FasterRCNN_MBV3\", load_frcnn_mbv3(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_MBV3 not available: {e}\")\n",
    "try:   models.append((\"RetinaNet_R50\",   load_retinanet_r50(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"RetinaNet_R50 not available: {e}\")\n",
    "try:   models.append((\"SSDLite_MBV3\",    load_ssdlite_mbv3(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSDLite_MBV3 not available: {e}\")\n",
    "try:   models.append((\"SSD300_VGG16\",    load_ssd300_vgg16(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSD300_VGG16 not available: {e}\")\n",
    "\n",
    "try:\n",
    "    y8n = load_yolov8n()\n",
    "    if y8n is not None: models.append((\"YOLOv8n\", y8n, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8n not available: {e}\")\n",
    "try:\n",
    "    y8s = load_yolov8s()\n",
    "    if y8s is not None: models.append((\"YOLOv8s\", y8s, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8s not available: {e}\")\n",
    "\n",
    "# Header\n",
    "hdr = [\"Model\",\"AP@0.5\",\"P@0.5\",\"R@0.5\",\"N\",\"Time(s)\",\"CamAvg(AP/P/R)\"]\n",
    "w   = [16, 8, 8, 8, 6, 8, 20]\n",
    "print(pretty_row(hdr, w))\n",
    "print(\"-\"*100)\n",
    "\n",
    "# CSV rows\n",
    "rows_for_csv = []\n",
    "\n",
    "for name, model, infer_fn in models:\n",
    "    res = evaluate_dataset(frames, name, model, infer_fn)\n",
    "    cam_avg = res[\"camera_avg\"]\n",
    "    cam_avg_str = f\"{cam_avg['AP50']:.3f}/{cam_avg['P@0.5']:.3f}/{cam_avg['R@0.5']:.3f}\"\n",
    "\n",
    "    print(pretty_row([\n",
    "        name,\n",
    "        f\"{res['AP50']:.3f}\",\n",
    "        f\"{res['P@0.5']:.3f}\",\n",
    "        f\"{res['R@0.5']:.3f}\",\n",
    "        res[\"N_imgs\"],\n",
    "        f\"{res['time_s']:.1f}\",\n",
    "        cam_avg_str\n",
    "    ], w))\n",
    "\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        print(f\"   {cam}: AP={c['AP50']:.3f}, P={c['P@0.5']:.3f}, R={c['R@0.5']:.3f}, N={c['N']}\")\n",
    "\n",
    "    # CSV row (overall + per camera + camera-average)\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"overall_AP50\": res[\"AP50\"], \"overall_P@0.5\": res[\"P@0.5\"], \"overall_R@0.5\": res[\"R@0.5\"],\n",
    "        \"N\": res[\"N_imgs\"], \"time_s\": res[\"time_s\"],\n",
    "        \"camera_avg_AP50\": cam_avg[\"AP50\"], \"camera_avg_P@0.5\": cam_avg[\"P@0.5\"], \"camera_avg_R@0.5\": cam_avg[\"R@0.5\"],\n",
    "    }\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        row[f\"{cam}_AP50\"]  = c[\"AP50\"]\n",
    "        row[f\"{cam}_P@0.5\"] = c[\"P@0.5\"]\n",
    "        row[f\"{cam}_R@0.5\"] = c[\"R@0.5\"]\n",
    "        row[f\"{cam}_N\"]     = c[\"N\"]\n",
    "    rows_for_csv.append(row)\n",
    "\n",
    "# Save CSV\n",
    "if rows_for_csv:\n",
    "    fieldnames = list(rows_for_csv[0].keys())\n",
    "    with open(CSV_OUT, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in rows_for_csv:\n",
    "            writer.writerow(r)\n",
    "    print(f\"\\nSaved CSV → {CSV_OUT}\")\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"This evaluates single-class VEHICLE detection (COCO vehicle-like classes) on RGB cameras.\")\n",
    "print(\"AP@0.5/P/R use greedy one-to-one matching at IoU≥0.5 against 2D GT projected from 3D boxes.\")\n",
    "print(\"Reported overall metrics, per-camera metrics, and camera-average (macro).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f1a92d7e-1875-49ca-8a63-eebdc02f3978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.13.0+cu117 | CUDA: True | Device: cuda\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.166 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Collected frames with vehicle GT: 500\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['numpy>=1.23.5', 'tqdm>=4.66.3', 'setuptools>=70.0.0', 'urllib3>=2.5.0 ; python_version > \"3.8\"'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/asad/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0mYOLOv5 🚀 2025-11-7 Python-3.10.13 torch-1.13.0+cu117 CUDA:0 (NVIDIA RTX 6000 Ada Generation, 48647MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['numpy>=1.23.5', 'tqdm>=4.66.3', 'setuptools>=70.0.0', 'urllib3>=2.5.0 ; python_version > \"3.8\"'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/asad/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache-dir \"numpy>=1.23.5\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" \"urllib3>=2.5.0 ; python_version > \"3.8\"\" ' returned non-zero exit status 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 280, in __init__\n",
      "    self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 253, in parse_marker\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 257, in _parse_full_marker\n",
      "    retval = _parse_marker(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
      "    marker = _parse_marker_item(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_parser.py\", line 319, in _parse_marker_var\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/_tokenizer.py\", line 168, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "pip._vendor.packaging._tokenizer.ParserSyntaxError: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 240, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 407, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_internal/req/constructors.py\", line 320, in parse_req_from_line\n",
      "    markers = Marker(markers_as_string)\n",
      "  File \"/home/asad/miniconda3/envs/py310/lib/python3.10/site-packages/pip/_vendor/packaging/markers.py\", line 298, in __init__\n",
      "    raise InvalidMarker(str(e)) from e\n",
      "pip._vendor.packaging.markers.InvalidMarker: Expected a marker variable or quoted string\n",
      "    python_version > 3.8\n",
      "                     ^\u001b[0m\u001b[31m\n",
      "\u001b[0mYOLOv5 🚀 2025-11-7 Python-3.10.13 torch-1.13.0+cu117 CUDA:0 (NVIDIA RTX 6000 Ada Generation, 48647MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model             AP@0.5    P@0.5     R@0.5     N       Time(s)   CamAvg(AP/P/R)      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "FasterRCNN_R50    0.368     0.557     0.385     500     15.2      0.364/0.587/0.373   \n",
      "   CAM_FRONT: AP=0.398, P=0.549, R=0.435, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.360, P=0.622, R=0.365, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.307, P=0.454, R=0.331, N=82\n",
      "   CAM_BACK: AP=0.434, P=0.761, R=0.405, N=93\n",
      "   CAM_BACK_LEFT: AP=0.332, P=0.559, R=0.369, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.350, P=0.579, R=0.329, N=76\n",
      "FasterRCNN_MBV3   0.362     0.612     0.354     500     9.9       0.363/0.658/0.354   \n",
      "   CAM_FRONT: AP=0.388, P=0.561, R=0.403, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.379, P=0.696, R=0.348, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.322, P=0.507, R=0.347, N=82\n",
      "   CAM_BACK: AP=0.362, P=0.717, R=0.349, N=93\n",
      "   CAM_BACK_LEFT: AP=0.379, P=0.835, R=0.345, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.346, P=0.634, R=0.334, N=76\n",
      "RetinaNet_R50     0.395     0.603     0.386     500     18.2      0.390/0.606/0.389   \n",
      "   CAM_FRONT: AP=0.442, P=0.580, R=0.450, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.417, P=0.677, R=0.374, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.327, P=0.506, R=0.335, N=82\n",
      "   CAM_BACK: AP=0.433, P=0.749, R=0.405, N=93\n",
      "   CAM_BACK_LEFT: AP=0.356, P=0.514, R=0.432, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.366, P=0.611, R=0.339, N=76\n",
      "SSDLite_MBV3      0.224     0.463     0.204     500     17.9      0.245/0.511/0.235   \n",
      "   CAM_FRONT: AP=0.212, P=0.304, R=0.225, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.283, P=0.835, R=0.237, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.221, P=0.510, R=0.203, N=82\n",
      "   CAM_BACK: AP=0.141, P=0.293, R=0.165, N=93\n",
      "   CAM_BACK_LEFT: AP=0.322, P=0.500, R=0.316, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.290, P=0.624, R=0.265, N=76\n",
      "SSD300_VGG16      0.366     0.622     0.348     500     11.3      0.364/0.702/0.334   \n",
      "   CAM_FRONT: AP=0.409, P=0.678, R=0.376, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.369, P=0.844, R=0.301, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.318, P=0.552, R=0.320, N=82\n",
      "   CAM_BACK: AP=0.337, P=0.697, R=0.322, N=93\n",
      "   CAM_BACK_LEFT: AP=0.406, P=0.813, R=0.359, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.347, P=0.630, R=0.324, N=76\n",
      "YOLOv8n           0.327     0.606     0.314     500     6.0       0.327/0.661/0.319   \n",
      "   CAM_FRONT: AP=0.360, P=0.494, R=0.396, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.365, P=0.770, R=0.342, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.265, P=0.639, R=0.240, N=82\n",
      "   CAM_BACK: AP=0.317, P=0.621, R=0.315, N=93\n",
      "   CAM_BACK_LEFT: AP=0.353, P=0.840, R=0.330, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.301, P=0.600, R=0.293, N=76\n",
      "YOLOv8s           0.350     0.581     0.359     500     6.2       0.348/0.681/0.344   \n",
      "   CAM_FRONT: AP=0.380, P=0.540, R=0.411, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.378, P=0.816, R=0.351, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.292, P=0.562, R=0.300, N=82\n",
      "   CAM_BACK: AP=0.368, P=0.743, R=0.351, N=93\n",
      "   CAM_BACK_LEFT: AP=0.359, P=0.691, R=0.369, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.308, P=0.732, R=0.280, N=76\n",
      "YOLOv8m           0.375     0.645     0.368     500     7.0       0.374/0.687/0.367   \n",
      "   CAM_FRONT: AP=0.397, P=0.637, R=0.399, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.403, P=0.786, R=0.365, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.329, P=0.565, R=0.347, N=82\n",
      "   CAM_BACK: AP=0.387, P=0.778, R=0.365, N=93\n",
      "   CAM_BACK_LEFT: AP=0.390, P=0.721, R=0.388, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.340, P=0.633, R=0.337, N=76\n",
      "YOLOv8l           0.384     0.658     0.373     500     8.8       0.381/0.731/0.367   \n",
      "   CAM_FRONT: AP=0.413, P=0.665, R=0.399, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.410, P=0.778, R=0.389, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.342, P=0.616, R=0.345, N=82\n",
      "   CAM_BACK: AP=0.409, P=0.803, R=0.382, N=93\n",
      "   CAM_BACK_LEFT: AP=0.368, P=0.693, R=0.383, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.345, P=0.830, R=0.301, N=76\n",
      "YOLOv11n          0.343     0.610     0.334     500     6.6       0.338/0.640/0.333   \n",
      "   CAM_FRONT: AP=0.380, P=0.572, R=0.379, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.366, P=0.689, R=0.357, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.302, P=0.486, R=0.331, N=82\n",
      "   CAM_BACK: AP=0.329, P=0.627, R=0.319, N=93\n",
      "   CAM_BACK_LEFT: AP=0.343, P=0.782, R=0.330, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.308, P=0.688, R=0.283, N=76\n",
      "YOLOv11s          0.358     0.666     0.344     500     6.9       0.358/0.651/0.362   \n",
      "   CAM_FRONT: AP=0.382, P=0.609, R=0.399, N=93\n",
      "   CAM_FRONT_LEFT: AP=0.399, P=0.740, R=0.374, N=80\n",
      "   CAM_FRONT_RIGHT: AP=0.306, P=0.444, R=0.355, N=82\n",
      "   CAM_BACK: AP=0.367, P=0.700, R=0.365, N=93\n",
      "   CAM_BACK_LEFT: AP=0.365, P=0.667, R=0.379, N=76\n",
      "   CAM_BACK_RIGHT: AP=0.326, P=0.748, R=0.298, N=76\n",
      "\n",
      "Saved CSV → /data/Asad/NuScenesMini/vis_results/nuscenes_report.csv\n",
      "\n",
      "Notes:\n",
      "This evaluates single-class VEHICLE detection (COCO vehicle-like classes) on RGB cameras.\n",
      "AP@0.5/P/R use greedy one-to-one matching at IoU≥0.5 against 2D GT projected from 3D boxes.\n",
      "Reported overall metrics, per-camera metrics, and camera-average (macro).\n"
     ]
    }
   ],
   "source": [
    "# === nuScenes (plain) VEHICLE detection benchmark (no OOD labels) with more detectors ==========================\n",
    "# - Builds 2D vehicle GT by projecting nuScenes 3D boxes into each camera.\n",
    "# - KPIs (overall + per camera): AP@0.5, P@0.5, R@0.5, N, Time(s). (No AUROC/FPR here.)\n",
    "# - Models (try/except): FasterRCNN_R50, FasterRCNN_MBV3, RetinaNet_R50, SSDLite_MBV3, SSD300_VGG16,\n",
    "#                        YOLOv8n/s/m/l, YOLOv11n/s (if available), YOLOv5s/m, DETR_R50/DC5/Deformable\n",
    "\n",
    "import os, time, math, csv, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# --- Paths & Config ---\n",
    "NUSCENES_ROOT = Path(\"/data/Asad/NuScenesMini\")   \n",
    "VERSION       = \"v1.0-mini\"                        \n",
    "MAX_IMAGES    = 500                               \n",
    "IOU_MATCH     = 0.5\n",
    "SCORE_THRESH  = 0.05\n",
    "\n",
    "CSV_DIR = Path(\"/data/Asad/NuScenesMini/vis_results/\")\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_OUT = CSV_DIR / \"nuscenes_report.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch: {torch.__version__} | CUDA: {torch.cuda.is_available()} | Device: {device}\")\n",
    "\n",
    "# --- nuScenes devkit ---\n",
    "try:\n",
    "    from nuscenes.nuscenes import NuScenes\n",
    "    from nuscenes.utils.data_classes import Box\n",
    "    from nuscenes.utils.geometry_utils import view_points\n",
    "    from pyquaternion import Quaternion\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"This cell requires the nuScenes devkit:\\n\"\n",
    "        \"  pip install nuscenes-devkit pyquaternion\"\n",
    "    ) from e\n",
    "\n",
    "# ----------------- 1) Build 2D VEHICLE GT (by projecting 3D) -----------------\n",
    "ALL_CAMS = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\n",
    "            \"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n",
    "\n",
    "VEHICLE_PREFIXES = (\n",
    "    \"vehicle.car\", \"vehicle.bus\", \"vehicle.truck\", \"vehicle.trailer\",\n",
    "    \"vehicle.construction\", \"vehicle.emergency\", \"vehicle.other\",\n",
    "    \"vehicle.motorcycle\", \"vehicle.bicycle\"\n",
    ")\n",
    "\n",
    "def is_vehicle_category(cat: str) -> bool:\n",
    "    return any(cat.startswith(p) for p in VEHICLE_PREFIXES)\n",
    "\n",
    "def project_box_to_image(nusc: NuScenes, ann_token: str, sd_token: str, img_hw):\n",
    "    ann = nusc.get(\"sample_annotation\", ann_token)\n",
    "    if not is_vehicle_category(ann[\"category_name\"]):\n",
    "        return None\n",
    "\n",
    "    box = Box(center=ann[\"translation\"], size=ann[\"size\"],\n",
    "              orientation=Quaternion(ann[\"rotation\"]),\n",
    "              name=ann[\"category_name\"], token=ann_token)\n",
    "\n",
    "    sd = nusc.get(\"sample_data\", sd_token)\n",
    "    ep = nusc.get(\"ego_pose\", sd[\"ego_pose_token\"])\n",
    "    cs = nusc.get(\"calibrated_sensor\", sd[\"calibrated_sensor_token\"])\n",
    "\n",
    "    # World -> ego\n",
    "    box.translate(-np.array(ep[\"translation\"]))\n",
    "    box.rotate(Quaternion(ep[\"rotation\"]).inverse)\n",
    "\n",
    "    # Ego -> camera\n",
    "    box.translate(-np.array(cs[\"translation\"]))\n",
    "    box.rotate(Quaternion(cs[\"rotation\"]).inverse)\n",
    "\n",
    "    if box.center[2] <= 0.1:\n",
    "        return None\n",
    "\n",
    "    K = np.array(cs[\"camera_intrinsic\"], dtype=np.float32)\n",
    "    pts = view_points(box.corners(), K, normalize=True)  # (3,8)\n",
    "\n",
    "    h, w = img_hw\n",
    "    xs, ys = pts[0], pts[1]\n",
    "    x1 = float(np.clip(xs.min(), 0, w-1))\n",
    "    y1 = float(np.clip(ys.min(), 0, h-1))\n",
    "    x2 = float(np.clip(xs.max(), 0, w-1))\n",
    "    y2 = float(np.clip(ys.max(), 0, h-1))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    if (x2-x1)*(y2-y1) < 20:  # tiny\n",
    "        return None\n",
    "    return np.array([x1, y1, x2, y2], dtype=np.float32)\n",
    "\n",
    "def collect_frames_with_vehicle_gt(nusc: NuScenes, max_images=None):\n",
    "    frames = []\n",
    "    count = 0\n",
    "    for sample in nusc.sample:\n",
    "        anns = sample[\"anns\"]\n",
    "        for cam in ALL_CAMS:\n",
    "            sd_tok = sample[\"data\"].get(cam)\n",
    "            if sd_tok is None: \n",
    "                continue\n",
    "            sd = nusc.get(\"sample_data\", sd_tok)\n",
    "            img_path = Path(nusc.dataroot) / sd[\"filename\"]\n",
    "            if not img_path.exists():\n",
    "                continue\n",
    "            try:\n",
    "                with Image.open(img_path) as im:\n",
    "                    w, h = im.size\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            gts = []\n",
    "            for ann_tok in anns:\n",
    "                bb = project_box_to_image(nusc, ann_tok, sd_tok, (h, w))\n",
    "                if bb is not None:\n",
    "                    gts.append(bb)\n",
    "            if len(gts) == 0:\n",
    "                continue\n",
    "\n",
    "            frames.append((str(img_path), sd_tok, cam,\n",
    "                           np.stack(gts, axis=0).astype(np.float32)))\n",
    "            count += 1\n",
    "            if (max_images is not None) and (count >= max_images):\n",
    "                return frames\n",
    "    return frames\n",
    "\n",
    "# ----------------- 2) Models (lite SOTA) -----------------\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "VEHICLE_COCO_IDS = {2, 3, 4, 6, 7, 8} \n",
    "\n",
    "def load_frcnn_r50():\n",
    "    m = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_frcnn_mbv3():\n",
    "    m = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_retinanet_r50():\n",
    "    m = torchvision.models.detection.retinanet_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssdlite_mbv3():\n",
    "    m = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_ssd300_vgg16():\n",
    "    m = torchvision.models.detection.ssd300_vgg16(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "# ----- New: DETR family (torchvision) -----\n",
    "def load_detr_r50():\n",
    "    m = torchvision.models.detection.detr_resnet50(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_detr_r50_dc5():\n",
    "    m = torchvision.models.detection.detr_resnet50_dc5(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def load_deformable_detr_r50():\n",
    "    # available in newer torchvision; try/except at callsite too\n",
    "    m = torchvision.models.detection.deformable_detr_resnet50(weights=\"DEFAULT\")\n",
    "    return m.to(device).eval()\n",
    "\n",
    "def infer_torchvision_detector(model, pil_img):\n",
    "    x = to_tensor(pil_img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model([x])[0]\n",
    "    boxes  = out[\"boxes\"].detach().float().cpu().numpy()\n",
    "    scores = out[\"scores\"].detach().float().cpu().numpy()\n",
    "    labels = out[\"labels\"].detach().cpu().numpy()\n",
    "    keep   = np.isin(labels, list(VEHICLE_COCO_IDS))\n",
    "    return boxes[keep].astype(np.float32), scores[keep].astype(np.float32)\n",
    "\n",
    "# YOLOv8 / YOLOv11 (optional via ultralytics)\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    _has_yolo = True\n",
    "except Exception:\n",
    "    _has_yolo = False\n",
    "\n",
    "YOLO_VEHICLE_NAMES = {\"bicycle\",\"car\",\"motorcycle\",\"bus\",\"train\",\"truck\"}\n",
    "\n",
    "def load_yolov8n():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "def load_yolov8s():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8s.pt\").to(device)\n",
    "\n",
    "# ----- New: more YOLOv8 sizes -----\n",
    "def load_yolov8m():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8m.pt\").to(device)\n",
    "\n",
    "def load_yolov8l():\n",
    "    if not _has_yolo: return None\n",
    "    return YOLO(\"yolov8l.pt\").to(device)\n",
    "\n",
    "# ----- New: YOLOv11 (if present in your ultralytics version) -----\n",
    "def load_yolo11n():\n",
    "    if not _has_yolo: return None\n",
    "    try:\n",
    "        return YOLO(\"yolo11n.pt\").to(device)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_yolo11s():\n",
    "    if not _has_yolo: return None\n",
    "    try:\n",
    "        return YOLO(\"yolo11s.pt\").to(device)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def infer_yolov8(model, pil_img):\n",
    "    im = np.array(pil_img.convert(\"RGB\"))\n",
    "    ydev = 0 if device.type==\"cuda\" else \"cpu\"\n",
    "    r = model.predict(source=im, verbose=False, conf=0.001, device=ydev)[0]\n",
    "    if r is None or r.boxes is None or len(r.boxes)==0:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "    xyxy = r.boxes.xyxy.cpu().numpy().astype(np.float32)\n",
    "    cls  = r.boxes.cls.cpu().numpy().astype(int)\n",
    "    if hasattr(r, \"probs\") and r.probs is not None:\n",
    "        scores = r.probs.data.cpu().numpy().max(axis=1).astype(np.float32)\n",
    "    else:\n",
    "        scores = r.boxes.conf.cpu().numpy().astype(np.float32)\n",
    "    names = r.names if hasattr(r,\"names\") else model.model.names\n",
    "    cls_names = [names[c] for c in cls]\n",
    "    keep = np.array([n in YOLO_VEHICLE_NAMES for n in cls_names], dtype=bool)\n",
    "    return xyxy[keep], scores[keep]\n",
    "\n",
    "# ----- New: YOLOv5 via torch.hub (optional; needs internet or local cache) -----\n",
    "def _try_import_yaml():\n",
    "    try:\n",
    "        import yaml  # noqa: F401\n",
    "    except Exception:\n",
    "        pass\n",
    "_try_import_yaml()\n",
    "\n",
    "def load_yolov5s():\n",
    "    try:\n",
    "        m = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)  # noqa\n",
    "        return m.autoshape().to(device).eval()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_yolov5m():\n",
    "    try:\n",
    "        m = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)  # noqa\n",
    "        return m.autoshape().to(device).eval()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "YOLOV5_VEHICLE_NAMES = {\"bicycle\",\"car\",\"motorcycle\",\"bus\",\"train\",\"truck\"}\n",
    "\n",
    "def infer_yolov5(model, pil_img):\n",
    "    im = np.array(pil_img.convert(\"RGB\"))\n",
    "    with torch.no_grad():\n",
    "        r = model(im, size=640)\n",
    "    # r.xyxy is a list (one per image)\n",
    "    if r is None or len(getattr(r, \"xyxy\", [])) == 0 or r.xyxy[0].numel() == 0:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "    det = r.xyxy[0].detach().cpu().numpy()  # [N,6]: x1,y1,x2,y2,conf,cls\n",
    "    boxes  = det[:, :4].astype(np.float32)\n",
    "    scores = det[:, 4].astype(np.float32)\n",
    "    cls    = det[:, 5].astype(int)\n",
    "    names = r.names if hasattr(r, \"names\") else getattr(model, \"names\", {})\n",
    "    # names can be dict or list; normalize get\n",
    "    if isinstance(names, list):\n",
    "        get_name = lambda i: names[i] if (0 <= i < len(names)) else str(i)\n",
    "    else:\n",
    "        get_name = lambda i: names.get(int(i), str(int(i)))\n",
    "    cls_names = [get_name(int(c)) for c in cls]\n",
    "    keep = np.array([n in YOLOV5_VEHICLE_NAMES for n in cls_names], dtype=bool)\n",
    "    return boxes[keep], scores[keep]\n",
    "\n",
    "# ----------------- 3) Metrics -----------------\n",
    "def iou_xyxy(a, b):\n",
    "    Na, Nb = a.shape[0], b.shape[0]\n",
    "    if Na==0 or Nb==0:\n",
    "        return np.zeros((Na,Nb), dtype=np.float32)\n",
    "    ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n",
    "    bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
    "    inter_x1 = np.maximum(ax1[:,None], bx1[None,:])\n",
    "    inter_y1 = np.maximum(ay1[:,None], by1[None,:])\n",
    "    inter_x2 = np.minimum(ax2[:,None], bx2[None,:])\n",
    "    inter_y2 = np.minimum(ay2[:,None], by2[None,:])\n",
    "    inter_w  = np.clip(inter_x2 - inter_x1, 0, None)\n",
    "    inter_h  = np.clip(inter_y2 - inter_y1, 0, None)\n",
    "    inter    = inter_w * inter_h\n",
    "    area_a   = (ax2-ax1)*(ay2-ay1)\n",
    "    area_b   = (bx2-bx1)*(by2-by1)\n",
    "    union    = area_a[:,None] + area_b[None,:] - inter\n",
    "    return np.where(union>0, inter/union, 0.0)\n",
    "\n",
    "def ap50_single_class(all_scores, all_tp, total_gt_pos):\n",
    "    if len(all_scores)==0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    order = np.argsort(-np.array(all_scores))\n",
    "    tp = np.array(all_tp)[order].astype(np.float32)\n",
    "    fp = 1.0 - tp\n",
    "    cum_tp = np.cumsum(tp); cum_fp = np.cumsum(fp)\n",
    "    recall = cum_tp / max(1, total_gt_pos)\n",
    "    precision = cum_tp / np.maximum(1, (cum_tp+cum_fp))\n",
    "    # all-points interpolation\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "    for i in range(mpre.size-1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = float(np.sum((mrec[idx+1]-mrec[idx]) * mpre[idx+1]))\n",
    "    best_i = int(np.argmax(2*precision*recall/(precision+recall+1e-9)))\n",
    "    return ap, float(precision[best_i]), float(recall[best_i])\n",
    "\n",
    "# ----------------- 4) Evaluation -----------------\n",
    "def evaluate_dataset(frames, model_name, model, infer_fn):\n",
    "    all_scores, all_tp = [], []\n",
    "    total_gt = 0\n",
    "    per_cam = {cam: {\"scores\":[], \"tp\":[], \"gt\":0, \"N\":0} for cam in ALL_CAMS}\n",
    "\n",
    "    t0 = time.time()\n",
    "    for (img_path, sd_tok, cam, gt) in frames:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        boxes, scores = infer_fn(model, img)\n",
    "        keep = scores >= SCORE_THRESH\n",
    "        boxes = boxes[keep]; scores = scores[keep]\n",
    "\n",
    "        total_gt += int(gt.shape[0])\n",
    "        # global\n",
    "        used = np.zeros((gt.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and gt.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, gt)\n",
    "            order = np.argsort(-scores)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not used[j]:\n",
    "                    all_scores.append(float(scores[di])); all_tp.append(1); used[j]=True\n",
    "                else:\n",
    "                    all_scores.append(float(scores[di])); all_tp.append(0)\n",
    "        else:\n",
    "            for s in scores:\n",
    "                all_scores.append(float(s)); all_tp.append(0)\n",
    "\n",
    "        # per-camera\n",
    "        cA = per_cam[cam]\n",
    "        cA[\"gt\"] += int(gt.shape[0]); cA[\"N\"] += 1\n",
    "        used_c = np.zeros((gt.shape[0],), dtype=bool)\n",
    "        if boxes.shape[0] > 0 and gt.shape[0] > 0:\n",
    "            IoU = iou_xyxy(boxes, gt)\n",
    "            order = np.argsort(-scores)\n",
    "            for di in order:\n",
    "                j = int(np.argmax(IoU[di]))\n",
    "                if IoU[di, j] >= IOU_MATCH and not used_c[j]:\n",
    "                    cA[\"scores\"].append(float(scores[di])); cA[\"tp\"].append(1); used_c[j]=True\n",
    "                else:\n",
    "                    cA[\"scores\"].append(float(scores[di])); cA[\"tp\"].append(0)\n",
    "        else:\n",
    "            for s in scores:\n",
    "                cA[\"scores\"].append(float(s)); cA[\"tp\"].append(0)\n",
    "\n",
    "    ap, p, r = ap50_single_class(all_scores, all_tp, total_gt)\n",
    "    results = {\n",
    "        \"AP50\": ap, \"P@0.5\": p, \"R@0.5\": r,\n",
    "        \"N_imgs\": len(frames), \"time_s\": time.time()-t0,\n",
    "        \"per_camera\": {}\n",
    "    }\n",
    "\n",
    "    # per-camera metrics\n",
    "    for cam in ALL_CAMS:\n",
    "        A = per_cam[cam]\n",
    "        ap_c, p_c, r_c = ap50_single_class(A[\"scores\"], A[\"tp\"], A[\"gt\"])\n",
    "        results[\"per_camera\"][cam] = {\"AP50\": ap_c, \"P@0.5\": p_c, \"R@0.5\": r_c, \"N\": A[\"N\"]}\n",
    "\n",
    "    # camera-average (macro)\n",
    "    def cam_mean(key):\n",
    "        vals = [results[\"per_camera\"][c][key] for c in ALL_CAMS if results[\"per_camera\"][c][\"N\"]>0]\n",
    "        return float(np.mean(vals)) if len(vals)>0 else float('nan')\n",
    "    results[\"camera_avg\"] = {\n",
    "        \"AP50\": cam_mean(\"AP50\"), \"P@0.5\": cam_mean(\"P@0.5\"), \"R@0.5\": cam_mean(\"R@0.5\")\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# ----------------- 5) Run -----------------\n",
    "def pretty_row(cols, widths):\n",
    "    return \"  \".join(str(c).ljust(w) for c,w in zip(cols, widths))\n",
    "\n",
    "# Init nuScenes + collect frames\n",
    "nusc = NuScenes(version=VERSION, dataroot=str(NUSCENES_ROOT), verbose=True)\n",
    "frames_all = collect_frames_with_vehicle_gt(nusc, max_images=None)\n",
    "frames = frames_all[:MAX_IMAGES] if (MAX_IMAGES is not None) else frames_all\n",
    "print(f\"Collected frames with vehicle GT: {len(frames)}\")\n",
    "\n",
    "# Models\n",
    "models = []\n",
    "try:   models.append((\"FasterRCNN_R50\",  load_frcnn_r50(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_R50 not available: {e}\")\n",
    "try:   models.append((\"FasterRCNN_MBV3\", load_frcnn_mbv3(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"FasterRCNN_MBV3 not available: {e}\")\n",
    "try:   models.append((\"RetinaNet_R50\",   load_retinanet_r50(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"RetinaNet_R50 not available: {e}\")\n",
    "try:   models.append((\"SSDLite_MBV3\",    load_ssdlite_mbv3(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSDLite_MBV3 not available: {e}\")\n",
    "try:   models.append((\"SSD300_VGG16\",    load_ssd300_vgg16(),  infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"SSD300_VGG16 not available: {e}\")\n",
    "\n",
    "# DETR family\n",
    "try:   models.append((\"DETR_R50\",          load_detr_r50(),            infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"DETR_R50 not available: {e}\")\n",
    "try:   models.append((\"DETR_R50_DC5\",      load_detr_r50_dc5(),        infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"DETR_R50_DC5 not available: {e}\")\n",
    "try:   models.append((\"DeformableDETR_R50\",load_deformable_detr_r50(), infer_torchvision_detector))\n",
    "except Exception as e: warnings.warn(f\"DeformableDETR_R50 not available: {e}\")\n",
    "\n",
    "# YOLOv8 baseline sizes\n",
    "try:\n",
    "    y8n = load_yolov8n()\n",
    "    if y8n is not None: models.append((\"YOLOv8n\", y8n, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8n not available: {e}\")\n",
    "try:\n",
    "    y8s = load_yolov8s()\n",
    "    if y8s is not None: models.append((\"YOLOv8s\", y8s, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8s not available: {e}\")\n",
    "\n",
    "# YOLOv8 larger\n",
    "try:\n",
    "    y8m = load_yolov8m()\n",
    "    if y8m is not None: models.append((\"YOLOv8m\", y8m, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8m not available: {e}\")\n",
    "try:\n",
    "    y8l = load_yolov8l()\n",
    "    if y8l is not None: models.append((\"YOLOv8l\", y8l, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv8l not available: {e}\")\n",
    "\n",
    "# YOLOv11 (if present)\n",
    "try:\n",
    "    y11n = load_yolo11n()\n",
    "    if y11n is not None: models.append((\"YOLOv11n\", y11n, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv11n not available: {e}\")\n",
    "try:\n",
    "    y11s = load_yolo11s()\n",
    "    if y11s is not None: models.append((\"YOLOv11s\", y11s, infer_yolov8))\n",
    "except Exception as e: warnings.warn(f\"YOLOv11s not available: {e}\")\n",
    "\n",
    "# YOLOv5 (torch.hub)\n",
    "try:\n",
    "    y5s = load_yolov5s()\n",
    "    if y5s is not None: models.append((\"YOLOv5s\", y5s, infer_yolov5))\n",
    "except Exception as e: warnings.warn(f\"YOLOv5s not available: {e}\")\n",
    "try:\n",
    "    y5m = load_yolov5m()\n",
    "    if y5m is not None: models.append((\"YOLOv5m\", y5m, infer_yolov5))\n",
    "except Exception as e: warnings.warn(f\"YOLOv5m not available: {e}\")\n",
    "\n",
    "# Header\n",
    "hdr = [\"Model\",\"AP@0.5\",\"P@0.5\",\"R@0.5\",\"N\",\"Time(s)\",\"CamAvg(AP/P/R)\"]\n",
    "w   = [16, 8, 8, 8, 6, 8, 20]\n",
    "print(pretty_row(hdr, w))\n",
    "print(\"-\"*100)\n",
    "\n",
    "# CSV rows\n",
    "rows_for_csv = []\n",
    "\n",
    "for name, model, infer_fn in models:\n",
    "    res = evaluate_dataset(frames, name, model, infer_fn)\n",
    "    cam_avg = res[\"camera_avg\"]\n",
    "    cam_avg_str = f\"{cam_avg['AP50']:.3f}/{cam_avg['P@0.5']:.3f}/{cam_avg['R@0.5']:.3f}\"\n",
    "\n",
    "    print(pretty_row([\n",
    "        name,\n",
    "        f\"{res['AP50']:.3f}\",\n",
    "        f\"{res['P@0.5']:.3f}\",\n",
    "        f\"{res['R@0.5']:.3f}\",\n",
    "        res[\"N_imgs\"],\n",
    "        f\"{res['time_s']:.1f}\",\n",
    "        cam_avg_str\n",
    "    ], w))\n",
    "\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        print(f\"   {cam}: AP={c['AP50']:.3f}, P={c['P@0.5']:.3f}, R={c['R@0.5']:.3f}, N={c['N']}\")\n",
    "\n",
    "    # CSV row (overall + per camera + camera-average)\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"overall_AP50\": res[\"AP50\"], \"overall_P@0.5\": res[\"P@0.5\"], \"overall_R@0.5\": res[\"R@0.5\"],\n",
    "        \"N\": res[\"N_imgs\"], \"time_s\": res[\"time_s\"],\n",
    "        \"camera_avg_AP50\": cam_avg[\"AP50\"], \"camera_avg_P@0.5\": cam_avg[\"P@0.5\"], \"camera_avg_R@0.5\": cam_avg[\"R@0.5\"],\n",
    "    }\n",
    "    for cam in ALL_CAMS:\n",
    "        c = res[\"per_camera\"][cam]\n",
    "        row[f\"{cam}_AP50\"]  = c[\"AP50\"]\n",
    "        row[f\"{cam}_P@0.5\"] = c[\"P@0.5\"]\n",
    "        row[f\"{cam}_R@0.5\"] = c[\"R@0.5\"]\n",
    "        row[f\"{cam}_N\"]     = c[\"N\"]\n",
    "    rows_for_csv.append(row)\n",
    "\n",
    "# Save CSV\n",
    "if rows_for_csv:\n",
    "    fieldnames = list(rows_for_csv[0].keys())\n",
    "    with open(CSV_OUT, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in rows_for_csv:\n",
    "            writer.writerow(r)\n",
    "    print(f\"\\nSaved CSV → {CSV_OUT}\")\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"This evaluates single-class VEHICLE detection (COCO vehicle-like classes) on RGB cameras.\")\n",
    "print(\"AP@0.5/P/R use greedy one-to-one matching at IoU≥0.5 against 2D GT projected from 3D boxes.\")\n",
    "print(\"Reported overall metrics, per-camera metrics, and camera-average (macro).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c2528-9e86-476b-905a-9d9164781852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c077cf-946b-4ab3-822c-f670212ad528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7dd4aacc-8e4a-4225-a02d-f3aa307b7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.194 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "\n",
      "Counting all road objects appearing in camera views...\n",
      "\n",
      "Unique road objects appearing in cameras: 14682\n",
      "CAM_FRONT      : 4191\n",
      "CAM_FRONT_RIGHT: 3428\n",
      "CAM_BACK_RIGHT : 2467\n",
      "CAM_BACK       : 5041\n",
      "CAM_BACK_LEFT  : 811\n",
      "CAM_FRONT_LEFT : 1692\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Count all road objects that appear\n",
    "# in ANY of the 6 NuScenes cameras\n",
    "# ================================\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import BoxVisibility\n",
    "\n",
    "# Your dataset path\n",
    "DST = Path(\"/data/Asad/NuScenesMiniNovel\")\n",
    "\n",
    "# Load NuScenes\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=str(DST), verbose=True)\n",
    "\n",
    "# Six cameras in NuScenes\n",
    "CAM_CHANNELS = [\n",
    "    \"CAM_FRONT\",\n",
    "    \"CAM_FRONT_RIGHT\",\n",
    "    \"CAM_BACK_RIGHT\",\n",
    "    \"CAM_BACK\",\n",
    "    \"CAM_BACK_LEFT\",\n",
    "    \"CAM_FRONT_LEFT\",\n",
    "]\n",
    "\n",
    "# Define \"road objects\": vehicles + pedestrians\n",
    "def is_road_object(category):\n",
    "    return category.startswith(\"vehicle.\") or category.startswith(\"human.pedestrian.\")\n",
    "\n",
    "# Per-camera counts\n",
    "per_cam_total = Counter()\n",
    "\n",
    "# Count of unique road objects appearing in ANY camera\n",
    "unique_objects_seen = set()\n",
    "\n",
    "print(\"\\nCounting all road objects appearing in camera views...\\n\")\n",
    "\n",
    "for sample in nusc.sample:\n",
    "    sample_ann_tokens = sample[\"anns\"]\n",
    "\n",
    "    # Track which annotations for this sample appear in at least one camera\n",
    "    sample_objects_seen_in_any_camera = set()\n",
    "\n",
    "    for cam in CAM_CHANNELS:\n",
    "        cam_token = sample[\"data\"][cam]\n",
    "\n",
    "        # Get boxes that appear in this camera's FOV\n",
    "        _, boxes, _ = nusc.get_sample_data(\n",
    "            cam_token,\n",
    "            box_vis_level=BoxVisibility.ANY,\n",
    "            selected_anntokens=sample_ann_tokens\n",
    "        )\n",
    "\n",
    "        for box in boxes:\n",
    "            category = box.name\n",
    "\n",
    "            if not is_road_object(category):\n",
    "                continue\n",
    "\n",
    "            # Count per camera\n",
    "            per_cam_total[cam] += 1\n",
    "\n",
    "            # Mark that this object's annotation appears in this sample\n",
    "            sample_objects_seen_in_any_camera.add(box.token)\n",
    "\n",
    "    # Add these objects to the global set\n",
    "    unique_objects_seen.update(sample_objects_seen_in_any_camera)\n",
    "\n",
    "\n",
    "print(f\"Unique road objects appearing in cameras: {len(unique_objects_seen)}\")\n",
    "\n",
    "\n",
    "for cam in CAM_CHANNELS:\n",
    "    print(f\"{cam:15s}: {per_cam_total[cam]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "37348a43-149e-4429-a99b-441a1fed2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.190 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "\n",
      "Counting road objects and visibility per camera...\n",
      "\n",
      "Camera          |   Total | Part.Vis | Part.% |    Full |  Full%\n",
      "----------------------------------------------------------------------\n",
      "CAM_FRONT       |    4191 |     2099 |   50.1 |    2092 |   49.9\n",
      "CAM_FRONT_RIGHT |    3428 |     2017 |   58.8 |    1411 |   41.2\n",
      "CAM_BACK_RIGHT  |    2467 |     1476 |   59.8 |     991 |   40.2\n",
      "CAM_BACK        |    5041 |     2529 |   50.2 |    2512 |   49.8\n",
      "CAM_BACK_LEFT   |     811 |      321 |   39.6 |     490 |   60.4\n",
      "CAM_FRONT_LEFT  |    1692 |      680 |   40.2 |    1012 |   59.8\n",
      "\n",
      "Note: 'Part.Vis' = v0-40 + v40-60 + v60-80 (proxy for truncated/occluded).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import BoxVisibility\n",
    "\n",
    "\n",
    "DST = Path(\"/data/Asad/NuScenesMiniNovel\")\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=str(DST), verbose=True)\n",
    "\n",
    "# 6 camera channels in nuScenes\n",
    "CAM_CHANNELS = [\n",
    "    \"CAM_FRONT\",\n",
    "    \"CAM_FRONT_RIGHT\",\n",
    "    \"CAM_BACK_RIGHT\",\n",
    "    \"CAM_BACK\",\n",
    "    \"CAM_BACK_LEFT\",\n",
    "    \"CAM_FRONT_LEFT\",\n",
    "]\n",
    "\n",
    "def is_road_object(category_name: str) -> bool:\n",
    "    return category_name.startswith(\"vehicle.\") or category_name.startswith(\"human.pedestrian.\")\n",
    "\n",
    "per_cam_total = Counter()          \n",
    "per_cam_visibility = {cam: Counter() for cam in CAM_CHANNELS} \n",
    "\n",
    "print(\"\\nCounting road objects and visibility per camera...\\n\")\n",
    "\n",
    "for sample in nusc.sample:\n",
    "    ann_tokens = sample[\"anns\"]\n",
    "\n",
    "    for cam in CAM_CHANNELS:\n",
    "        cam_token = sample[\"data\"][cam]\n",
    "\n",
    "        # Boxes visible in this camera\n",
    "        _, boxes, _ = nusc.get_sample_data(\n",
    "            cam_token,\n",
    "            box_vis_level=BoxVisibility.ANY,\n",
    "            selected_anntokens=ann_tokens\n",
    "        )\n",
    "\n",
    "        for box in boxes:\n",
    "            category = box.name\n",
    "            if not is_road_object(category):\n",
    "                continue\n",
    "\n",
    "            per_cam_total[cam] += 1\n",
    "\n",
    "            # Get annotation + visibility level\n",
    "            ann = nusc.get(\"sample_annotation\", box.token)\n",
    "            vis_token = ann[\"visibility_token\"]\n",
    "            vis_level = nusc.get(\"visibility\", vis_token)[\"level\"]   # e.g. 'v40-60', 'v80-100'\n",
    "            per_cam_visibility[cam][vis_level] += 1\n",
    "\n",
    "\n",
    "print(f\"{'Camera':15s} | {'Total':>7s} | {'Part.Vis':>8s} | {'Part.%':>6s} | {'Full':>7s} | {'Full%':>6s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cam in CAM_CHANNELS:\n",
    "    total = per_cam_total[cam]\n",
    "    if total == 0:\n",
    "        print(f\"{cam:15s} | {0:7d} | {0:8d} | {0:6.1f} | {0:7d} | {0:6.1f}\")\n",
    "        continue\n",
    "\n",
    "    full_visible = per_cam_visibility[cam].get('v80-100', 0)\n",
    "    partial = total - full_visible   # v0-40, v40-60, v60-80 grouped\n",
    "\n",
    "    partial_pct = 100.0 * partial / total\n",
    "    full_pct = 100.0 * full_visible / total\n",
    "\n",
    "    print(f\"{cam:15s} | {total:7d} | {partial:8d} | {partial_pct:6.1f} | {full_visible:7d} | {full_pct:6.1f}\")\n",
    "\n",
    "print(\"\\nNote: 'Part.Vis' = v0-40 + v40-60 + v60-80 (proxy for truncated/occluded).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45234a-1b27-4454-80aa-c586cd9cf250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe2225-0a05-4a5c-9e4a-9f7f96188f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "79a8edf4-0d61-4e7f-a570-f5252d015bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes in NuScenesMiniNovel:\n",
      "  - scene-0061\n",
      "  - scene-0103\n",
      "  - scene-0553\n",
      "  - scene-0655\n",
      "  - scene-0757\n",
      "  - scene-0796\n",
      "  - scene-0916\n",
      "  - scene-1077\n",
      "  - scene-1094\n",
      "  - scene-1100\n",
      "\n",
      "Rendering scene scene-0061...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-0061__multiview.mp4 with 39 frames\n",
      "\n",
      "Rendering scene scene-0103...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-0103__multiview.mp4 with 40 frames\n",
      "\n",
      "Rendering scene scene-0553...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-0553__multiview.mp4 with 41 frames\n",
      "\n",
      "Rendering scene scene-0655...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-0655__multiview.mp4 with 41 frames\n",
      "\n",
      "Rendering scene scene-0757...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-0757__multiview.mp4 with 41 frames\n",
      "\n",
      "Rendering scene scene-0796...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-0796__multiview.mp4 with 40 frames\n",
      "\n",
      "Rendering scene scene-0916...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-0916__multiview.mp4 with 41 frames\n",
      "\n",
      "Rendering scene scene-1077...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-1077__multiview.mp4 with 41 frames\n",
      "\n",
      "Rendering scene scene-1094...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-1094__multiview.mp4 with 40 frames\n",
      "\n",
      "Rendering scene scene-1100...\n",
      "  Saved /data/Asad/NuScenesMiniNovel/multiview_videos/scene-1100__multiview.mp4 with 40 frames\n",
      "\n",
      "Done. Videos are in: /data/Asad/NuScenesMiniNovel/multiview_videos\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "# Paths and setup\n",
    "DATAROOT = Path(\"/data/Asad/NuScenesMiniNovel\")\n",
    "VERSION = \"v1.0-mini\"\n",
    "\n",
    "OUTDIR = DATAROOT / \"multiview_videos\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The 6 camera channels in the order we want them in the grid\n",
    "ALL_CAMS = [\n",
    "    \"CAM_FRONT\",\n",
    "    \"CAM_FRONT_LEFT\",\n",
    "    \"CAM_FRONT_RIGHT\",\n",
    "    \"CAM_BACK\",\n",
    "    \"CAM_BACK_LEFT\",\n",
    "    \"CAM_BACK_RIGHT\",\n",
    "]\n",
    "\n",
    "# Init NuScenes over your *novel* dataset\n",
    "nusc_novel = NuScenes(version=VERSION, dataroot=str(DATAROOT), verbose=False)\n",
    "\n",
    "\n",
    "def make_scene_multiview_video(scene_name, fps=6, resize=(640, 360), max_frames=None):\n",
    "    \"\"\"\n",
    "    For a given scene name in the NuScenesMiniNovel dataset, create a multiview\n",
    "    video (3x2 grid of all 6 cameras) and save it to OUTDIR.\n",
    "    \"\"\"\n",
    "    scene_row = next((s for s in nusc_novel.scene if s[\"name\"] == scene_name), None)\n",
    "    assert scene_row is not None, f\"Scene {scene_name} not found in NuScenesMiniNovel\"\n",
    "\n",
    "    sample_token = scene_row[\"first_sample_token\"]\n",
    "    w, h = resize\n",
    "\n",
    "    grid_w, grid_h = 3 * w, 2 * h\n",
    "    out_path = OUTDIR / f\"{scene_name}__multiview.mp4\"\n",
    "\n",
    "    vw = cv2.VideoWriter(\n",
    "        str(out_path),\n",
    "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        fps,\n",
    "        (grid_w, grid_h),\n",
    "    )\n",
    "\n",
    "    frame_idx = 0\n",
    "    while sample_token:\n",
    "        sample = nusc_novel.get(\"sample\", sample_token)\n",
    "        tiles = []\n",
    "\n",
    "        for ch in ALL_CAMS:\n",
    "            if ch not in sample[\"data\"]:\n",
    "                # blank tile if channel missing\n",
    "                tile = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                sd_token = sample[\"data\"][ch]\n",
    "                sd = nusc_novel.get(\"sample_data\", sd_token)\n",
    "                img_path = DATAROOT / sd[\"filename\"]\n",
    "\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    tile = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "                else:\n",
    "                    tile = cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            tiles.append(tile)\n",
    "\n",
    "        # 3x2 grid: first row = FRONT, FRONT_LEFT, FRONT_RIGHT\n",
    "        #           second row = BACK, BACK_LEFT, BACK_RIGHT\n",
    "        row1 = np.hstack(tiles[:3])\n",
    "        row2 = np.hstack(tiles[3:])\n",
    "        grid = np.vstack([row1, row2])\n",
    "\n",
    "        # optional overlay: scene + frame idx\n",
    "        cv2.rectangle(grid, (10, grid_h - 40), (10 + 450, grid_h - 10), (0, 0, 0), -1)\n",
    "        cv2.putText(\n",
    "            grid,\n",
    "            f\"{scene_name} | frame {frame_idx}\",\n",
    "            (15, grid_h - 18),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        vw.write(grid)\n",
    "        frame_idx += 1\n",
    "\n",
    "        if max_frames is not None and frame_idx >= max_frames:\n",
    "            break\n",
    "\n",
    "        sample_token = sample[\"next\"]\n",
    "\n",
    "    vw.release()\n",
    "    return str(out_path), frame_idx\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# 1) show available scene names\n",
    "print(\"Scenes in NuScenesMiniNovel:\")\n",
    "scene_names = [s[\"name\"] for s in nusc_novel.scene]\n",
    "for n in scene_names:\n",
    "    print(\"  -\", n)\n",
    "\n",
    "# 2) Make multiview videos for all scenes (full length)\n",
    "video_paths = []\n",
    "for name in scene_names:\n",
    "    print(f\"\\nRendering scene {name}...\")\n",
    "    path, n_frames = make_scene_multiview_video(\n",
    "        scene_name=name,\n",
    "        fps=6,\n",
    "        resize=(640, 360),\n",
    "        max_frames=None,  # set e.g. 100 if you want to truncate\n",
    "    )\n",
    "    print(f\"  Saved {path} with {n_frames} frames\")\n",
    "    video_paths.append((name, path, n_frames))\n",
    "\n",
    "print(\"\\nDone. Videos are in:\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f965f0-f681-49fe-a87c-f637b2580fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a517e6-dbba-41e1-8003-510f6b7474a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37016d-55e0-480c-8121-30d7c1347380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bc3dd-bcfa-476a-b007-6b34411c57f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7d99e-41d4-4778-b669-3efd11f796fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968746a-dc9b-4ef6-a946-b38065983217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3dc3b-d366-445f-8820-11418a95a8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
